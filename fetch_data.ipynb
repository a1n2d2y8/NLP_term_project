{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af0aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse, unquote\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c719b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"./metadata.csv\", encoding=\"latin-1\")\n",
    "train = pd.read_csv(\"./train_QA.csv\", encoding=\"utf-8\")\n",
    "test = pd.read_csv(\"./test_Q.csv\", encoding=\"utf-8\")\n",
    "#metadata.head()\n",
    "#train.head()\n",
    "#test.head()\n",
    "FOLDER = \"./data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06bbd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_LIST = metadata['url']\n",
    "#HTTP headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "for index, url in enumerate(URL_LIST):\n",
    "\turl = url.strip()\n",
    "\ttry:\n",
    "\t\tfilename = f\"file_{index + 1}.pdf\"\n",
    "\t\tif not Path(FOLDER).exists():\n",
    "\t\t\tprint(f\"{FOLDER} not found\")\n",
    "\t\t\tbreak\n",
    "\t\tsave_path = Path(FOLDER) / filename\n",
    "\n",
    "\t\tresponse = requests.get(url, headers=headers, stream=True, timeout=30)\n",
    "\t\t# throw an exception\n",
    "\t\tresponse.raise_for_status()\n",
    "\n",
    "\t\twith open(save_path, 'wb') as f:\n",
    "\t\t\tfor chunk in response.iter_content(chunk_size=8192):\n",
    "\t\t\t\tf.write(chunk)\n",
    "\n",
    "\t\ttime.sleep(0.5)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"[fail] ({index+1}/{len(URL_LIST)}) URL: {url}\")\n",
    "\t\tprint(f\"       reason: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d0b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain_classic.docstore.document import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, UnstructuredPDFLoader\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112a19f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cengy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\cengy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad888fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"...\"\n",
    "login(token=hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4085c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1muser: \u001b[0m a1n2d2y8\n"
     ]
    }
   ],
   "source": [
    "!hf auth whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcf5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Taiwan is Taipei.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2:3b\" \n",
    "# Initialize an instance of the Ollama model\n",
    "llm = OllamaLLM(model=MODEL)\n",
    "# Invoke the model to generate responses\n",
    "response = llm.invoke(\"What is the capital of Taiwan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30762898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PDF...\n",
      "splitting...\n"
     ]
    }
   ],
   "source": [
    "DB_FOLDER = \"./chroma_db_store\"\n",
    "device = \"cuda\"\n",
    "\n",
    "if Path(DB_FOLDER).exists():\n",
    "\tshutil.rmtree(Path(DB_FOLDER))\n",
    "Path(DB_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "\tFOLDER, \n",
    "\tglob=\"*.pdf\", \n",
    "\tloader_cls=PyPDFLoader,\n",
    ")\n",
    "print(\"loading PDF...\")\n",
    "documents = loader.load()\n",
    "if not documents:\n",
    "\tprint(\"no file\")\n",
    "\t\n",
    "# Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\tchunk_size=1000,\n",
    "\tchunk_overlap=200,\n",
    "\tadd_start_index=True\n",
    ")\n",
    "print(\"splitting...\")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "model_kwargs = {\n",
    "\t'device': device, \n",
    "\t'trust_remote_code': True\n",
    "}\n",
    "encode_kwargs = {'normalize_embeddings': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7e6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Setup\n",
    "EMBED_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "\tmodel_name=EMBED_MODEL,\n",
    "\tmodel_kwargs=model_kwargs,\n",
    "\tencode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b16630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vector Store\n",
    "try:\n",
    "    vector_store.delete_collection()\n",
    "    print(\"old Collection clear\")\n",
    "except:\n",
    "    pass\n",
    "vector_store = Chroma.from_documents(\n",
    "\tdocuments=splits,\n",
    "\tembedding=embeddings_model,\n",
    "\tpersist_directory=DB_FOLDER\n",
    ")\n",
    "retriever = vector_store.as_retriever(\n",
    "\tsearch_type=\"similarity\",\n",
    "\tsearch_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10edb70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n",
      "\n",
      "Source1: data\\file_3.pdf\n",
      "Content: On July 2023, we launched the ML.ENERGY Leaderboard and Benchmark, the first inference energy\n",
      "leaderboard for modern generative AI models.6 Our goal was to measure and understand the energy\n",
      "consumption of generative AI models, and we provided a web-based leaderboard to allow everyone\n",
      "to browse the results. The leaderboard started with only LLM chat with tens of different LLMs, but\n",
      "gradually expanded to include more tasks, models, and datasets. Our benchmarking suite to supply\n",
      "data to the leaderboard is what we dub the ML.ENERGY Benchmark. This paper shares our design\n",
      "philosophy and principles we have acquired over time by gradually maintaining and upgrading the\n",
      "ML.ENERGY Benchmark and the Leaderboard, and highlights notable results we have obtained\n",
      "from the early 2025 iteration of the benchmark. Importantly, we plan to continuously update the\n",
      "benchmark and the leaderboard as long as resources allow, and what is presented in this paper is only...\n",
      "\n",
      "Source2: data\\file_3.pdf\n",
      "Content: services are deployed in the real world and thus their energy consumption (Section 2.2). Google\n",
      "disclosed the median energy consumption of their AI service [24]. It provides a comprehensive\n",
      "scope of measurement, even including the energy consumption of idle machines provisioned for\n",
      "stable service operation. However, measurements and reports are based on internal Google systems,\n",
      "workloads, hardware (TPUs), and model (Gemini) that are not publicly available, limiting the\n",
      "generalizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the\n",
      "first inference energy benchmark for modern generative AI models, and empowers users to not only\n",
      "measure but also optimize the energy consumption of their models. See Appendix D for more details.\n",
      "ML energy optimization.The ML.ENERGY Benchmark provides automated energy optimization\n",
      "recommendations based on energy measurements (Section 3.3). There are several other efforts that...\n",
      "\n",
      "Source3: data\\file_24.pdf\n",
      "Content: large models are called on to conduct inference in reality (e.g.,\n",
      "ChatGPT). As these state-of-the-art LLMs see increasing usage\n",
      "and deployment in various domains, a better understanding\n",
      "of their resource utilization is crucial for cost-savings, scaling\n",
      "performance, efficient hardware usage, and optimal inference\n",
      "strategies.\n",
      "In this paper, we describe experiments conducted to study the\n",
      "computational and energy utilization of inference with LLMs. We\n",
      "benchmark and conduct a preliminary analysis of the inference\n",
      "performance and inference energy costs of different sizes of\n",
      "LLaMA—a recent state-of-the-art LLM—developed by Meta AI\n",
      "on two generations of popular GPUs (NVIDIA V100 & A100)\n",
      "and two datasets (Alpaca and GSM8K) to reflect the diverse\n",
      "set of tasks/benchmarks for LLMs in research and practice.\n",
      "We present the results of multi-node, multi-GPU inference using\n",
      "model sharding across up to 32 GPUs. To our knowledge, our\n",
      "work is the one of the first to study LLM inference performance...\n"
     ]
    }
   ],
   "source": [
    "print(\"test...\")\n",
    "query = \"What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\" \n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "\tprint(f\"\\nSource{i+1}: {doc.metadata.get('source')}\")\n",
    "\tprint(f\"Content: {doc.page_content[:1000]}...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b92e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ID: q003] generating...\n",
      "\n",
      "[ID: q009] generating...\n",
      "\n",
      "[ID: q054] generating...\n",
      "\n",
      "[ID: q062] generating...\n",
      "\n",
      "[ID: q075] generating...\n",
      "\n",
      "[ID: q078] generating...\n",
      "   -> JSON decode error\n",
      "\n",
      "[ID: q091] generating...\n",
      "\n",
      "[ID: q102] generating...\n",
      "\n",
      "[ID: q105] generating...\n",
      "\n",
      "[ID: q106] generating...\n",
      "\n",
      "[ID: q124] generating...\n",
      "\n",
      "[ID: q135] generating...\n",
      "\n",
      "[ID: q139] generating...\n",
      "\n",
      "[ID: q146] generating...\n",
      "\n",
      "[ID: q153] generating...\n",
      "\n",
      "[ID: q158] generating...\n",
      "\n",
      "[ID: q164] generating...\n",
      "\n",
      "[ID: q166] generating...\n",
      "\n",
      "[ID: q170] generating...\n",
      "\n",
      "[ID: q200] generating...\n",
      "\n",
      "[ID: q202] generating...\n",
      "   -> JSON decode error\n",
      "\n",
      "[ID: q203] generating...\n",
      "\n",
      "[ID: q207] generating...\n",
      "\n",
      "[ID: q211] generating...\n",
      "\n",
      "[ID: q215] generating...\n",
      "\n",
      "[ID: q221] generating...\n",
      "\n",
      "[ID: q230] generating...\n",
      "\n",
      "[ID: q231] generating...\n",
      "\n",
      "[ID: q246] generating...\n",
      "\n",
      "[ID: q253] generating...\n",
      "\n",
      "[ID: q262] generating...\n",
      "\n",
      "[ID: q263] generating...\n",
      "\n",
      "[ID: q272] generating...\n",
      "\n",
      "[ID: q278] generating...\n",
      "\n",
      "[ID: q280] generating...\n",
      "\n",
      "[ID: q282] generating...\n",
      "\n",
      "[ID: q296] generating...\n",
      "\n",
      "[ID: q297] generating...\n",
      "\n",
      "[ID: q304] generating...\n",
      "\n",
      "[ID: q306] generating...\n",
      "\n",
      "[ID: q316] generating...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "CSV_PATH = \"./train_QA.csv\"      \n",
    "OUTPUT_CSV = \"./answer.csv\" \n",
    "def clean_json_string(json_str):\n",
    "    json_str = json_str.strip()\n",
    "    if json_str.startswith(\"```json\"):\n",
    "        json_str = json_str[7:]\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str[3:]\n",
    "    if json_str.endswith(\"```\"):\n",
    "        json_str = json_str[:-3]\n",
    "    return json_str.strip()\n",
    "template_str = \"\"\"\n",
    "    You are a data extraction expert. Please answer the questions based on the provided \"Reference Documents.\n",
    "    \n",
    "    Please be sure to output in **standard JSON format**, without including any other text or interpretations.\n",
    "    The JSON must contain the following fields:\n",
    "    - \"answer\": A clear natural-language response (e.g., 1438 lbs, Water consumption, TRUE).\n",
    "        * If no answer is possible, use exactly: \"Unable to answer with confidence based on the provided documents.\"\n",
    "    - \"answer_value\": The normalized numeric or categorical value (e.g., 1438, Water consumption, 1).\n",
    "        * If no answer is possible, use \"is_blank\".\n",
    "        * Ranges should be encoded as \"[low, high]\".\n",
    "        * Do not include symbols like <, >, ~ here. Those can be left in the \"answer\" field.\n",
    "    - \"answer_unit\": Unit of measurement (e.g., lbs, kWh, gCO2, projects, is_blank).\n",
    "    - \"supporting_materials\": Verbatim justification from the cited document (quote, table reference, figure reference, etc.).\n",
    "    - \"explanation\": Short reasoning describing why the cited material supports the answer.\n",
    "    \n",
    "    If you cannot find the answer in the reference file, please enter \"Unable to answer with confidence based on the provided documents.\" in the answer field.\n",
    "\n",
    "    === reference file ===\n",
    "    {context}\n",
    "    === question ===\n",
    "    {question}\n",
    "    \n",
    "    JSON output:\n",
    "    \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=template_str, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT} \n",
    ")\n",
    "\n",
    "def run_testing():\n",
    "\n",
    "    if not os.path.exists(DB_FOLDER):\n",
    "        print(\"no database\")\n",
    "        return\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        qid = row.get('id', index)\n",
    "        \n",
    "        print(f\"\\n[ID: {qid}] generating...\")\n",
    "\n",
    "        row_result = {\n",
    "            \"id\": qid,\n",
    "            \"question\": question,\n",
    "            \"answer\": \"Error\",\n",
    "            \"answer_value\": \"\",\n",
    "            \"answer_unit\": \"\",\n",
    "            \"supporting_materials\": \"\",\n",
    "            \"explanation\": \"\",\n",
    "            \"ref_url\": \"\",   \n",
    "            \"ref_id\": \"\"   \n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = qa_chain.invoke({\"query\": question})\n",
    "            raw_output = response['result']\n",
    "            source_docs = response['source_documents']\n",
    "\n",
    "            try:\n",
    "                cleaned_json = clean_json_string(raw_output)\n",
    "                parsed_data = json.loads(cleaned_json)\n",
    "                \n",
    "                row_result[\"answer\"] = parsed_data.get(\"answer\", \"\")\n",
    "                row_result[\"answer_value\"] = parsed_data.get(\"answer_value\", \"\")\n",
    "                row_result[\"answer_unit\"] = parsed_data.get(\"answer_unit\", \"\")\n",
    "                row_result[\"supporting_materials\"] = parsed_data.get(\"supporting_materials\", \"\")\n",
    "                row_result[\"explanation\"] = parsed_data.get(\"explanation\", \"\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"   -> JSON decode error\")\n",
    "                row_result[\"answer\"] = raw_output \n",
    "\n",
    "            if source_docs:\n",
    "                source_path = source_docs[0].metadata.get('source', '')\n",
    "                filename = os.path.basename(source_path) \n",
    "                \n",
    "\n",
    "                row_result[\"ref_url\"] = filename\n",
    "                \n",
    "                row_result[\"ref_id\"] = filename.replace(\".pdf\", \"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   -> RAG fail: {e}\")\n",
    "            row_result[\"explanation\"] = f\"System Error: {e}\"\n",
    "\n",
    "        results.append(row_result)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "    \n",
    "    cols_order = [\"id\", \"question\", \"answer\", \"answer_value\", \"answer_unit\", \"ref_id\", \"ref_url\", \"supporting_materials\", \"explanation\"]\n",
    "\n",
    "    final_cols = [c for c in cols_order if c in out_df.columns]\n",
    "    out_df = out_df[final_cols]\n",
    "\n",
    "    out_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "849ed38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.36585365853658536\n",
      "outcome: [False, True, False, True, True, False, False, True, False, False, False, True, False, True, True, False, True, False, False, False, False, True, True, False, False, True, False, False, False, False, False, True, False, True, False, True, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def calculate_accuracy(evaluation_csv_path, train_csv_path):\t\n",
    "    evaluation_report = pd.read_csv(evaluation_csv_path)\n",
    "    train = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    if len(evaluation_report) != len(train):\n",
    "        raise ValueError(\"length not match\")\n",
    "    \n",
    "    results = []\n",
    "    for eval_ans, true_ans in zip(evaluation_report['answer'], train['answer']):\n",
    "        if str(true_ans).strip().lower() in str(eval_ans).strip().lower():\n",
    "            results.append(True)\n",
    "        else:\n",
    "            results.append(False)\n",
    "    \n",
    "    accuracy = sum(results) / len(results)\n",
    "    return accuracy, results\n",
    "\n",
    "\n",
    "accuracy, results = calculate_accuracy(\"answer.csv\", \"train_QA.csv\")\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"outcome:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
