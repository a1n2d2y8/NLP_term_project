# Energy Considerations of Large Language Model Inference and Efficiency Optimizations

Jared Fernandez\*<sup>1</sup> , Clara Na\*<sup>1</sup> , Vashisth Tiwari\*<sup>1</sup> , Yonatan Bisk<sup>1</sup> , Sasha Luccioni<sup>2</sup> , Emma Strubell<sup>1</sup>

<sup>1</sup>Carnegie Mellon University, <sup>2</sup>Hugging Face,

Correspondence: [{jaredfern,](mailto:jaredfern@cmu.edu) [clarana,](mailto:clarana@cmu.edu) [vashisthtiwari}](mailto:vashisthtiwari@cmu.edu)@cmu.edu

# Abstract

As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is *highly sensitive to workload geometry, software stack, and hardware accelerators*, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.

# 1 Introduction

Improvements in task performance by large language models (LLMs) have prompted large-scale investments in computing hardware and energy infrastructure to support the development and deployment of LLM and related machine learning models [\(Isaac,](#page-10-0) [2025;](#page-10-0) [Smith,](#page-11-0) [2025;](#page-11-0) [Cai and Sophia,](#page-9-0)

![](_page_0_Figure_10.jpeg)

**Figure Description:**
The image is a graph with three axes labeled "Energy (kW)", "Theoretical", and "Azure Conv." Each axis has numerical values ranging from 0 to 10^3 kW. There are four colored stars at different points across the graph: two purple stars near the bottom left corner of the graph, one green star towards the top right corner, and another yellow star also near the top right corner but slightly closer to the center than the other yellow star.

Below the graph, there's a legend that associates each color with a specific term: "PyTorch vLLM" for the yellow stars, "BurstGPT" for the purple stars, and "Azure Code Task" for the green star. This suggests that the graph represents some form of performance or efficiency metric related to artificial intelligence models running on Azure infrastructure. The exact nature of the data represented by these stars isn't clear without additional context.



Figure 1: Proper application of efficiency methods with optimized vLLM (orange) approaches the ideal energy consumption (green) as compared with an unoptimized baseline PyTorch (purple) implementation.

[2025\)](#page-9-0). However, the growing prevalence of LLMs yields commensurate increases in the energy demand, water use, and carbon emissions associated with their development and deployment [\(Morrison](#page-10-1) [et al.,](#page-10-1) [2025;](#page-10-1) [Li et al.,](#page-10-2) [2025;](#page-10-2) [Strubell et al.,](#page-11-1) [2020;](#page-11-1) [Luccioni et al.,](#page-10-3) [2024b\)](#page-10-3). Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 [\(Aljbour et al.,](#page-9-1) [2024;](#page-9-1) [Shehabi et al.,](#page-11-2) [2024;](#page-11-2) [Green et al.,](#page-10-4) [2024\)](#page-10-4). However, such projections of energy use primarily rely upon sector-wide estimates of demand or substantial simplifications of the of the energy demands of individual models.

In order to develop effective energy policy for this growing demand, it is necessary to characterize the underlying computational workloads of development (i.e. model training) and deployment (i.e. inference). In particular, the cost and efficiency of inference is especially crucial due to the scale and increased frequency at which models are served for repeated use. Concretely, Meta reports that inference workloads constitute up to 70% of their AI power consumption [\(Wu et al.,](#page-11-3) [2022\)](#page-11-3) while Google attributes 60% of their ML energy [\(Patterson et al.,](#page-11-4) [2022\)](#page-11-4) and between 80 to 90% of ML AWS cloud computing demand [\(Barr,](#page-9-2) [2019;](#page-9-2) [Leopold,](#page-10-5) [2019\)](#page-10-5).

<sup>\*</sup>Equal contribution

To address the problem of inference efficiency, the NLP and machine learning research communities have developed various optimizations spanning: algorithms, software frameworks, and hardware accelerators. Such optimizations have primarily targeted improvements in model speed (e.g. latency and throughput; [\(Leviathan et al.,](#page-10-6) [2023;](#page-10-6) [Kwon et al.,](#page-10-7) [2023\)](#page-10-7)). Moreover, these methods are frequently assessed in constrained settings or on simplified datasets that fail to capture the broad diversity of real-world tasks. These tasks range from traditional NLP applications like sequence tagging and summarization to more computationally demanding workloads such as synthetic data generation and chain-of-thought reasoning. *There remains a critical gap in understanding of the energy costs of language model inference, especially when efficiency interventions are applied jointly in real-world settings.*

In this work, we examine the energy costs of LLM inference and present a comprehensive analysis of the impact of: *data dimensionality, decoding strategies, serving frameworks, compilation techniques, GPU hardware platforms, model parallelism, and architectural variants* on total energy use during inference. Based on our energy profiling across these optimizations, we approximate offline inference with LLMs based on real-world workload with variable sequence lengths and batching, considering both an upper bound of naive unoptimized inference and a lower bound of theoretical optimized inference. Our analysis reveals that while idealized estimations of hardware utilization substantially underestimate the energy use of language model inference, proper application of inference efficiency optimizations can substantially reduce the energy requirements of inference by up to 73% from unoptimized baselines with vanilla PyTorch and Huggingface Transformers and to within 26.6% of theoretical ideal performance on simulated offline workloads (see Table [4\)](#page-8-0).

## 2 Methodology

In the following section, we describe our experimental setup for evaluating inference efficiency.

Model Architectures. We focus our experiments on language models ranging from 1B to 32B parameters, primarily evaluating Llama-3.1-8B-Ba se and Llama-3.1-8B-Instruct models as representative decoder-only transformers [\(Dubey et al.,](#page-9-3) [2024\)](#page-9-3). To investigate effects of scaling model archi-

tecture, we include the Qwen-1.5-32B model [\(Bai](#page-9-4) [et al.,](#page-9-4) [2023\)](#page-9-4). For architectural comparisons, we analyze the sparse OLMoE mixture-of-expert (MoE) model alongside its dense counterparts – the 1B and 7B OLMo architectures – which maintain comparable active and total parameter counts, respectively [\(Muennighoff et al.,](#page-10-8) [2024;](#page-10-8) [Groeneveld et al.,](#page-10-9) [2024\)](#page-10-9).

Data Dimensionality We investigate the impact of data dimensionality across three key dimensions: input sequence lengths, output generation lengths, and batch sizes.

Inference with large language models is commonly decomposed into two stages: prefilling and token generation, each with a different energy profile [\(Patel et al.,](#page-11-5) [2024\)](#page-11-5). The prefill stage processes prompts in parallel and is typically compute-bound, achieving high GPU utilization. In contrast, the autoregressive decoding stage is typically memorybound and leads to GPU under-utilization. These bottlenecks and their resulting energy profiles shift with input and output lengths.

To address GPU under-utilization during generation, serving systems employ batching strategies. However, the effectiveness of batching varies with input-output characteristics [\(Agrawal et al.,](#page-9-5) [2024;](#page-9-5) [Li et al.,](#page-10-10) [2024\)](#page-10-10). Long input sequences limit maximum batch sizes due to memory constraints, while variable output lengths can lead to inefficient batch utilization as some sequences complete before others.

Our analysis spans batch sizes from 1 (singleexample inference) to task-dependent maximums (up to 1024), ensuring coverage of a broad range of maximally realistic settings.

We ground analysis in NLP workloads spanning text classification, summarization, translation, and open-ended text generation. Different tasks exhibit different data dimensionalities: classification involves minimal generation (often a single token), summarization pairs long contexts with mediumlength outputs, and translation typically assumes balanced input-output lengths. Input length statistics in considered datasets are shown in Table [3.](#page-7-0)

In a controlled sweep, we explore scenarios with up to 32k input tokens and 4k output tokens, varying sequence lengths by powers of two. We fix generation to 64 or 8 tokens when varying context lengths, and assume 512 or 64 token input context when varying output lengths. Input context length is enforced via truncation of longer sequences from PG19 [\(Rae et al.,](#page-11-6) [2019\)](#page-11-6).

<span id="page-2-1"></span>![](_page_2_Figure_0.jpeg)

**Figure Description:**
The image is a graphical representation of two different energy models: Prefill Energy vs. Input Sequence Length (on the left) and Decode Energy vs. Output Sequence Length (on the right). Both graphs are plotted with x-axis representing input or output sequence length in tokens and y-axis representing energy levels in kilojoules per token (kJ/token).

On the left side, there are three lines corresponding to B5=1, BS5=8, and BS5=4. Each line represents a different model for prefill energy calculation. The numerical values along these lines indicate the amount of energy required at each sequence length. For instance, at an input sequence length of 10^24 tokens, the prefill energy requirement according to the B5=1 model is approximately 1 kJ/token, while it's about 9 kJ/token for the B5=4 model.

The right side shows another set of data points for decode energy versus output sequence length. Again, we see three lines corresponding to B5=1, BS5=8, and BS5=4. These lines also show varying amounts of energy as the output sequence length increases. At an output sequence length of 10^24 tokens, the decode energy requirement according to the B5=1 model is around 3 kJ/token, whereas it's just over 7 kJ/token for the B5=4 model.

Both graphs have a title "Prefill Energy vs. Input sequence length" above the left one and "Decode Energy vs. Output sequence length" above the right one. They include labels indicating that 'tokens' refer to units used within the context of the models being compared. Additionally, there are legends explaining the meaning of the colored dots on the graphs. The colors correspond to different types of sequences, such as 'BS5=1', 'BS5=8', and 'BS5=4'.



Figure 2: Controlled sweeps of input and output sequence lengths on A6000 GPUs, on vLLM backend, described in [§3.1.](#page-3-0) We decompose inference costs into prefill and decode energy. At small batch sizes and input sequence lengths, energy intensity of a workload scales sub-linearly with increasing sequence length input sequence lengths. Decoding is more energy intensive per token than prefill, but energy intensity begins scaling linearly even for short generations and small batch sizes with the vLLM framework.

Decoding Strategies. Different decoding strategies used for generation have different computational profiles and can have a substantial impact on the generation efficiency [\(Kwon et al.,](#page-10-7) [2023\)](#page-10-7). In order to study the impact of sampling methods and auto-regressive decoding strategies, we investigate *greedy decoding, beam search decoding, temperature sampling, top-*p *decoding* affect the energy requirements and end-to-end latency [\(Holtzman](#page-10-11) [et al.,](#page-10-11) [2020\)](#page-10-11).

In addition to auto-regressive decoding, we study the impact of speculative decoding. *Speculative decoding* is commonly used as a latency minimization inference optimization [\(Kwon et al.,](#page-10-7) [2023\)](#page-10-7). In speculative decoding, a lightweight draft model is used to predict multiple tokens (γ) which are then verified by the target model in parallel [\(Leviathan et al.,](#page-10-6) [2023;](#page-10-6) [Chen et al.,](#page-9-6) [2023\)](#page-9-6). Speculative decoding provides latency improvement by better utilizing GPUs over autoregressive decoding.

In our experiments, we use the following targetdraft model pairs with a look-ahead value γ = 4 across various batch sizes: DeepSeek-R1-Distil l-Qwen-32B with mobiuslabsgmbh/DeepSeek-R 1-ReDistill-Qwen-1.5B-v1.1 [\(Guo et al.,](#page-10-12) [2025;](#page-10-12) [Yang et al.,](#page-11-7) [2024\)](#page-11-7); Llama-3.1-8B-Base with Lla ma-3.2-1B [\(Dubey et al.,](#page-9-3) [2024\)](#page-9-3).

Software Optimizations. Choice in the software frameworks used for inference significantly impacts both latency and energy efficiency through optimized kernel implementations and computational graph management [\(Georgiou et al.,](#page-9-7) [2022;](#page-9-7) [Fernan](#page-9-8)[dez et al.,](#page-9-8) [2023\)](#page-9-8). We evaluate two widely-adopted libraries used in LLM inference: native PyTorch with HuggingFace transformers [\(Wolf et al.,](#page-11-8) [2020\)](#page-11-8), and vLLM, an optimized framework for LLM inference that achieves improved compute and memory utilization [\(Paszke et al.,](#page-11-9) [2019;](#page-11-9) [Kwon et al.,](#page-10-7) [2023\)](#page-10-7); experiments are conducted in bfloat16 precision.

Within these frameworks, we compare with a native PyTorch baselines with Just-in-Time compilation via TorchInductor (i.e. torch.compile) and CUDA Graphs kernel serialization. Furthermore, for vLLM, we evaluate continuous batching which efficiently handles variable output lengths in batch processing by overlaying sequences [\(Yu](#page-12-0) [et al.,](#page-12-0) [2022\)](#page-12-0).

Hardware Platforms. Our experiments are conducted using an on-premise heterogeneous server with multiple GPU types and node configurations. Specifically, we conduct experiments on multiple generations of consumer workstation and datacenter GPU accelerators from the Ampere (A6000, A100 80GB PCIe), and Ada Lovelace (A6000 Ada) microarchitecture.

All experiments run on 8-GPU nodes with standardized node- and job-level CPU and RAM configurations for each GPU type. For multi-GPU experiments, we utilize up to 4 GPUs simultaneously, investigating tensor parallel inference with group sizes of 2 and 4 devices. [1](#page-2-0) . We examine both standard and speculative decoding approaches using the Llama-3.1-8B and Qwen-32B models. Additional details on computing hardware are provided in Appendix [A.](#page-12-1)

<span id="page-2-0"></span><sup>1</sup>This configuration leaves 4-7 GPUs available for other users. While the Slurm scheduler does not enforce complete isolation in network, memory, and CPU infrastructure across jobs, concurrent workloads in practice were not CPU- or memory-intensive enough to impact ours significantly – for example, in the vast majority of cases (98%), an ambient measurement of the RAM utilization in a node our jobs were running on was less than 20% of the total available

<span id="page-3-4"></span>![](_page_3_Figure_0.jpeg)

**Figure Description:**
The image is a scatter plot with a grid background that displays various data points related to energy efficiency for different building sizes. Each point represents an individual sample of energy efficiency (measured in kilowatt-hours per square foot) for a specific batch size. There are five categories labeled at the top right corner: "Speculative Decoding," "Greedy Decoding," "Beam Search," "Temperature Sampling," and "Top-p Sampling." These labels suggest that each category corresponds to a different method or approach used for decoding, which could be part of machine learning or optimization processes.

The x-axis of the graph indicates the batch size, ranging from 2^0 to 2^7, where '0' likely means no batching and '7' implies seven levels of batching. This suggests that as the batch size increases, more samples may have been collected or analyzed.

On the y-axis, there are numerical values representing the energy efficiency measurements in kilowatt-hours per square foot. However, without additional context, it's difficult to determine the exact meaning of these numbers. They appear to be standardized units commonly used in energy efficiency studies.

Each point on the graph has a color code corresponding to one of the methods mentioned above. For example, blue dots might represent Speculative Decoding, while red squares could indicate Greedy Decoding. The colors help distinguish between the different methods being compared across the range of batch sizes.

Overall, the image seems to compare the effectiveness of different algorithms or techniques in optimizing energy efficiency within buildings, possibly in the context of artificial intelligence or computational modeling applications. It provides a visual representation of how these methods perform under varying conditions represented by the batch size variable.



Figure 3: At small batch sizes, speculative decoding provides reduced latency and energy savings. At larger batch size speculative decoding increases energy.

**Performance Measures.** We evaluate the efficiency of inference by measuring the latency, throughput, GPU energy, and GPU power required for the inference of 1,024 examples <sup>2</sup>. Total energy use and GPU power metrics are measured using Nvidia Management Library (NVML) via the CodeCarbon library (Courty et al., 2024). Prior to evaluation, we conduct a warmup on up to 20 batches to allow for memory allocation, required CUDA graph capture, and JiT compilation <sup>3</sup>. Results are reported as the mean values energy use, latency, or power usage of three runs.

### 3 Results

In the following section, we examine the effects of variations of data dimensionality, model architecture, decoding strategies, and software optimizations on inference energy use.

### <span id="page-3-0"></span>3.1 Effects of Dataset and Sequence Length

We present results from our controlled sweep of sequence lengths and batch sizes in Figure 2. Prefill costs increase as a function of input sequence length, at the same rate regardless of batch sizes when scaling sequences larger than 128 tokens. At shorter sequence lengths and smaller batch sizes, the energy costs of prefill are constant regardless of the computational workload due to significant undersaturation of the accelerator. Although we fix output generation tokens to 64, we verify that at this convergence in rate of energy intensity increase occurs at the same point when instead fixing generation length to 8 tokens; see Figure 11 in Appendix E.

![](_page_3_Figure_9.jpeg)

**Figure Description:**
The image displays a scatter plot with two axes: Energy (kW·h) on the x-axis and Batch Size on the y-axis. There are three sets of data points represented by different colored dots. Each set is labeled with an abbreviation for the type of data it represents: OLMo_1B, OLMMo_7B, and OLMoe_18B. These labels suggest that each point corresponds to a batch size experiment involving either one billion (OLMmo_1B), seven billion (OLMmo_7B), or eighteen billion (OLMoe_18B).

The energy consumption is measured in kilowatt hours per batch, as indicated by "Energy (kW·h)" above the horizontal axis. The vertical axis indicates the batch size, ranging from 2^4 to 2^9 batches. This suggests that the experiments were conducted at various scales, possibly increasing computational power or complexity over time.

There's also a legend box located in the top right corner of the graph. It contains four color codes corresponding to the colors used in the plot: blue, green, orange, and red. However, without additional context, it's unclear which color code corresponds to which dataset label.

Overall, the image appears to be a scientific or technical chart comparing energy efficiency across different batch sizes within machine learning models, specifically those using the Open Language Model (OLM). The purpose of such a study could be to understand how scaling up model training impacts both computational resources and the accuracy or performance of the model.



Figure 4: Mixture-of-Experts LLMs require more energy than dense models with comparable active parameters; differences are pronounced at larger batch sizes.

In Figure 2, the energy intensity of the decode likewise scales with input context length only at larger input sequence lengths.

However, the energy intensity of decoding scales linearly with sequence length regardless of sequence length or batch sizes due to the autoregressive, sequential nature of decoding.

Generally, decoding energy dominates the overall workload in all settings but those with the shortest generation lengths, such as those seen in classification workloads and short form summarization. Note the log-log scale and the parallel linear trends, where the differences in intercepts are proportionate with the differences in batch size <sup>4</sup>. In the following sections, we discuss a variety of algorithmic and software interventions that are appropriate for different types of workload geometries.

### 3.2 Effects of Algorithmic Optimizations

Speculative Decoding Only Reduces Energy at Low Batch Sizes. Speculative decoding is commonly used to achieve inference speedups in low-batch inference in which autoregressive decoding fails to achieve high GPU VRAM utilization. However, for large batch sizes where GPU is already saturated, draft model speculation and excess verifications introduce additional overhead. In the large batch case, for short to medium contexts, LLM inference is typically compute bound, making speculative decoding slower than autoregressive decoding with the target model (Chen et al., 2025; Liu et al., 2024).

Compared to variations in energy use from alternate decoding strategies and sampling methods, speculative decoding has the greatest effect on the

<span id="page-3-1"></span><sup>&</sup>lt;sup>2</sup>For experiments with batch sizes larger than 256, metrics are computed over 4096 examples and then normalized.

<span id="page-3-2"></span><sup>&</sup>lt;sup>3</sup>Due to size, warmup is limited to 4 batches for inference with the Qwen-32B.

<span id="page-3-3"></span><sup>&</sup>lt;sup>4</sup>See Fig 10 in Appendix E for additional results on vanilla PyTorch backend, and Figure 12 for comparison with real energy intensity measurements for a sample of classical NLP tasks

<span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)

**Figure Description:**
The image displays a series of three bar graphs with numerical data points plotted against them. Each graph is labeled as "(a) A100 8GB PCIe," indicating that these are performance benchmarks for an 8-gigabyte (GB) solid-state drive (SSD) using the PCI Express (PCIe) interface.

The first graph at the top shows energy consumption measured in watts (W), with each bar representing a different configuration or setting: "PyTorch (compile=False)" has two bars, one purple and one orange; "PyTorch (compile=True)" has one blue bar; "Pytorch (compiled=False)" has one pink bar; and "Pytorch (compiled=True)" has one green bar. These labels suggest that the graphs compare the energy efficiency of various configurations within PyTorch, which is a popular open-source machine learning library developed by Facebook's AI Research lab.

Below the energy consumption graph, there are two more graphs showing batch size measurements. The leftmost graph compares batches sizes ranging from 2 to 32, while the rightmost graph compares batch sizes ranging from 4 to 64. Both graphs have similar color schemes but include additional details such as the number of operations per second (OPS/sec). For example, the label "VLLM (age=True)" indicates that the operation being timed involves Vision & Language Modeling with True age flag enabled. Similarly, other labels like "VLM (ager=False)" and "VLM (age=False)" indicate variations in the model architecture or training process.

Each graph includes a legend explaining the meaning of the colors used to represent the different settings or models. Additionally, there is a note stating "(c) A600 Adapter," suggesting that the adapter used during testing may also affect the results shown in the graphs.

Overall, the image provides technical information about the computational requirements and energy usage of different PyTorch configurations when running specific tasks involving vision and language processing. It appears to be part of a research paper or presentation comparing the performance characteristics of various deep learning models under different conditions.



Figure 5: Energy consumption comparison across different GPUs for inference with PyTorch and vLLM backends of 1024 samples for 64 output tokens. For each GPU, we compare PyTorch with and without compilation, and vLLM with and without CUDA Graph serialization. The line in black represents the maximum allowable batch size for PyTorch. Relative savings are most apparent in the low batch size regime and that vLLM due to its optimizations can serve a larger batch size.

energy use and latency of language model inference. At smaller batch sizes ( $\leq 16$ ) speculative decoding is effective in reducing the total energy cost of inference with up to +29.14% compared to single-example inference (Figure 3). However, autoregressive decoding methods are more efficient at larger batch sizes, with speculative decoding requiring 25.65% more energy when performing inference at a batch size of 128.

### Mixture of Experts Incurs Higher Inference En-

ergy Costs. Sparse mixture-of-experts are often utilized as an alternative architecture due to their increased sample efficiency during training and increased performance relative to dense neural networks with the same number of active parameters. Although both dense OLMo-1B and the OLMoE1B-7 B mixture-of-experts models use substantially less energy than the dense OLMo-7B model, the OLMoE architecture utilizes up to 54.24% more energy than the base OLMo 1B model, despite having a similar number of active parameters.

We identify that the increased energy and latency of MoE's can be attributed to the fused kernel used in the expert layers which is substantially slower than the corresponding GEMM operation in linear layers in the dense model; 19.70% slower at batch size 1 and 63% slower at batch size 8. Notably, we observe that the additional routing operations in the MoE model introduce minimal latency; and that the increased overhead of more CUDA graph and kernel launch operations are largely mitigated through kernel serialization and graph compilation optimizations (i.e. vLLM with CUDA Graphs).

### 3.3 Effects of Software Optimizations

### PagedAttention with vLLM Improves Efficiency.

Compared to native PyTorch, the vLLM inference serving engine improves both the throughput and the energy efficiency. The vLLM framework uses PagedAttention to implement non-contiguous KV cache blocks which reduces memory fragmentation and allocation of redundant memory in the case of sparse sequences (Kwon et al., 2023).

These optimizations allow for improved memory efficiency and the vLLM framework to support larger batch sizes on fixed memory GPUs.

### **Compilation and Kernel Serialization Improves**

Efficiency. The graph compilation and kernel serialization increase hardware utilization by removing redundant operations in the computational graph and reducing the kernel launch overhead (Fernandez et al., 2023), respectively. We observe that both torch.compile and CUDA graph serialization (eager=False) improve throughput at no additional energy cost in Figure 5. However, we note that the benefits of CUDA graphs are more apparent at lower batch sizes, as the relative cost of kernel launch is larger for smaller computational workloads.

#### Continuous Batching Reduces Energy Use.

LLM inference is inherently autoregressive, requiring many sequential operations. Static batching maintains a fixed batch size throughout inference, which leads to GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. *Continuous batching* mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022). This ap-

<span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)

**Figure Description:**
The image is a graph with two plots: one for energy (kW) on the x-axis and another for batch size on the y-axis. Both axes are labeled as "Energy (kW)" and "Batch Size," respectively. There are three data points represented by different colored dots: blue, green, and orange. Each dot represents an individual measurement of kW at various batch sizes.

At the top right corner of the graph, there's a legend that explains the symbols used to denote each type of measurement or condition. It lists "Single GPU", "Tensor Parallel: 2", and "Tensor Parallel: 4". These labels likely correspond to the types of computations being performed during the measurements.

The title of the graph reads "Llama 3.1 8B" which suggests it relates to a model named Llama with a batch size of 8 billion parameters (8B). This indicates that the graph is comparing the energy consumption of different parallelization strategies within the context of training a large language model like GPT-3.5.

In the bottom left corner of the graph, additional text provides information about the specific metrics being measured: "DeepSoot Distill Oven 32bR." This could be referring to the hardware setup or software tool used for these experiments.

Overall, the graph appears to be from a technical paper or presentation related to machine learning research, specifically focused on the efficiency of training large models using distributed computing resources.



![](_page_5_Figure_1.jpeg)

**Figure Description:**
The image is a graph titled "DeepSeek Distill Qwen 32B". It appears to be a scatter plot with two axes: one labeled 'Energy (kW)' ranging from 0 to 4, and another labeled 'Batch Size' ranging from 1 to 50. There are three sets of data points represented by different colored circles: blue for TENSOR PARALLEL: 2, orange for TENSOR PARALLEL: 4, and red for ENERGY. Each set of data points represents a batch size, as indicated by the labels next to each color code.

The x-axis shows the energy consumption measured in kilowatts per hour (kWh), while the y-axis indicates the number of batches processed. The scale on both axes suggests that the graph compares the computational efficiency of processing batches at various sizes across different levels of energy usage.

There are also annotations indicating specific conditions or settings. For example, there is a note about "TENSOR PARALLEL: 2" and "TENSOR PARALLEL: 4", which likely refer to parallelization techniques used during training. These notes suggest that the graph is comparing the performance of these techniques under varying energy consumption rates. However, without additional context, it's not clear exactly what the terms "TENSOR PARALLEL: 2" and "TENSOR PARALLEL: 4" mean within the context of this graph.



Figure 6: Energy Use of Llama-3.1 8B and Qwen 32B with varying degrees of Tensor Parallelism.

proach is particularly effective when generation lengths have high variance, yielding significant speedups at larger batch sizes.

We observe that at smaller batch sizes the overhead of online scheduling outweighs its benefits but at larger batch sizes, online serving with continuous batching requires less energy; details in Appendix D. We note that the numbers under-represent the impact of continuous batching given the samples are drawn from the same dataset, thereby reducing the variance in input and output lengths.

### 3.4 Effects of Hardware Design Choices

Multi-GPU Tensor Parallelism Reduces Latency for Increased Power Use Model parallelism techniques such as tensor and pipeline parallelism are frequently used to alleviate the memory pressure of large sets of model parameters and batch sizes, as well as to leverage multiple hardware accelerators in order to speed up workload execution (Narayanan et al., 2021). Additionally, for fixed workloads, tensor parallelism reduces both the per-device computational intensity and perdevice power utilization as the workload is sharded across accelerator. However, the speedups from additional accelerators are insufficient to offset the energy cost of utilizing more devices (i.e. utilizing twice the GPUs fails to yield a two-fold speedup).

In Figure 6, we observe that utilizing tensor parallelism to scale from inference with a single GPU to four GPUs reduces latency and per-device power utilization for the Llama-3.1 8B model. However, increasing parallelism yields higher total energy use due to the larger number of accelerators. Concretely, parallelizing a fixed workload over two and four GPUs decreases latency by 40.16% and 61.34% but increases total energy use by 29.3% and 55.23% at single batch inference due to the introduction of additional devices.

**Effects of Hardware Speed** The effectiveness of optimization techniques varies significantly across hardware platforms, with faster accelerators showing greater benefits from optimizations that target computational efficiency. Our results demonstrate that graph compilation, kernel serialization, and speculative decoding achieve their maximum impact on the A100 GPU.

Specifically, PyTorch compilation yields a 29.90% improvement on the A100, which drops to 13.28% on the RTX 6000 Ada and further to 1.96% on the A6000. Similarly, vLLM's eager mode optimization shows a 25.47% improvement on the A100 versus 2.97% on the A6000. This pattern suggests that as hardware computational capabilities increase, the relative impact of software optimizations targeting kernel efficiency becomes more pronounced.

# <span id="page-5-1"></span>4 The Impact of Optimizations on Inference Energy Use

In this section, we outline our approach to modeling the energy consumption of an LLM under both synthetic and realistic workload distributions. We leverage classical NLP tasks and datasets of inference requests to estimate energy usage across different execution environments, including PyTorchnative and vLLM backends with software optimizations on a single A6000 GPU.

# <span id="page-5-2"></span>4.1 Modeling Energy Requirements Using Offline Serving

We consider the energy required to process a dataset  $\mathcal{D} = \{R_1, R_2, \dots, R_N\}$  in an offline setting in which all requests can be batch processed freely, and where each request  $R_k$  consists of a tuple  $(i_k, o_k)$ , representing the input token length  $i_k$  and the output generation length  $o_k$ :

$$R_k = (i_k, o_k), \quad \forall k \in \{1, \dots, N\}.$$

![](_page_6_Figure_0.jpeg)

**Figure Description:**
The image is a graph titled "Azure Conversation Input Tokens - CDF". It represents the cumulative distribution function (CDF) of tokens for Azure conversations. On the x-axis, there are numerical values ranging from 0 to 23, which likely correspond to different levels or stages of token count. These numbers could be representing the number of tokens at each stage.

On the y-axis, two lines are plotted with labels: "Real Distribution" and "Binned Approximation". Both lines show an upward trend as the value on the x-axis increases, indicating that more tokens are being used as the conversation progresses. However, the line labeled "Binned Approximation" has a noticeable dip around the value of 14, while the "Real Distribution" line maintains a steady increase without such a dip. This suggests that the binned approximation may not accurately capture the actual distribution of tokens in real scenarios.

The title "Azure Conversation Input Tokens - CDF" indicates that the graph is specifically related to the input tokens within Azure conversational systems, using a cumulative distribution function to visualize their frequency or usage over time or across different stages of the conversation.



![](_page_6_Figure_1.jpeg)

**Figure Description:**
The image is a graph titled "Azure Conversation Output Tokens - CDF". It represents the cumulative distribution function (CDF) of tokens outputted by Azure conversations. There are two lines on the graph: one for real distribution and another for binned approximation. Both lines start at zero and increase as the token count increases from left to right along the x-axis.

The y-axis indicates probability, ranging from 0 to 1. At the top left corner of the graph, there's an explanation that reads "Real Distribution" with a blue line representing it, and "Binned Approximation" with an orange line representing it. Below these labels, there's a legend explaining the symbols used in the graph.

On the bottom left side of the graph, there's a note indicating "Token Count", which likely refers to the number of tokens being considered or analyzed. This suggests that the graph is comparing how often certain numbers of tokens occur naturally versus through some form of binning approximation method. The exact nature of the 'real distribution' and 'binned approximation' methods isn't explained within the visible part of the image.



Figure 7: Comparison of the real token length distributions (blue) with the binned approximation (orange) for Azure conversation input (left) and output (right) token lengths. The CDF plots illustrate how our binning strategy approximates the empirical distribution while ensuring computational efficiency for energy estimation.

Since  $i_k$  and  $o_k$  vary significantly across requests, we utilize dataset statistics—including the median and 99th percentile of input and output lengths (discussed in §4.3) to inform our binning strategy.

Binning Strategy. To effectively handle the broad range of  $(i_k, o_k)$  values, we define discrete bin sets for input and output lengths:

$$\begin{split} I_{\text{bins}} &= \{2^m \mid m \in \mathbb{N}, 4 \leq m \leq 13\} \\ &= \{32, 128, 256, 512, 1024, 2048, 4096, 8192\}, \quad \widehat{E}_{\text{total}} = \sum_{(I^*, O^*)} \left(\frac{N^{\text{real}}(I^*, O^*)}{B(I^*, O^*)}\right) E_{\text{batch}}^{\text{real}}(I^*, O^*), \\ O_{\text{bins}} &= \{2^n \mid n \in \mathbb{N}, 3 \leq m \leq 9\} \\ &= \{8, 16, 32, 64, 128, 256, 512\}. \end{split}$$

These bin choices ensure sufficient coverage across realistic request distributions. Notably, we exclude extremely long input requests (> 8k tokens) and generation outputs beyond 512 tokens.

Mapping Requests to Bins. Given a request R = (i, o), we map it to the closest ceiling bin:

$$I^* = \min\{I \in I_{\text{bins}} \mid I \ge i\},$$
  
$$O^* = \min\{O \in O_{\text{bins}} \mid O \ge o\}.$$

We group requests within the same  $(I^*, O^*)$  bin into batches of size  $B(I^*, O^*)$ , the maximum allowable batch size for the given hardware and backend configuration. Each batch processes  $B(I^*, O^*)$ requests in parallel, allowing for more efficient energy utilization, which is more representative of real-world inference setups.

Given our hardware configuration and backend, we collect the estimates of  $E_{\text{batch}}(I^*, O^*)$ , which corresponds to the energy used to serve a request of batch size B with input prompts of length I\*and output lengths O\*.

We collect real energy measurements  $\mathbf{E}_{\text{batch}}^{\text{real}}(\mathbf{I}^*, \mathbf{O}^*)$ , representing the observed energy usage when processing a full batch of size  $B(I^*, O^*)$  with input lengths  $I^*$  and output lengths  $O^*$ . Thus, the total estimated energy consumption across the workload to serve N requests that fall in the bin is given by:

$$\widehat{E}_{\text{total}} = \sum_{(I^*, O^*)} \left( \frac{N^{\text{real}}(I^*, O^*)}{B(I^*, O^*)} \right) E_{\text{batch}}^{\text{real}}(I^*, O^*).$$

where  $N^{\text{real}}(I^*, O^*)$  is the total number of observed requests mapped to bin  $(I^*, O^*)$ , and  $\frac{N^{\text{real}}(I^*, O^*)}{B(I^*, O^*)}$  represents the number of batches required to process them.

### **Idealized Baseline**

As a naive baseline, we estimate an upper bound of the energy efficiency of these workloads with a baseline derived from the manufacturer-rated hardware speeds  $(FLOPS_{HW})$ , power draw (TDP) ,and floating point operations (FLOPs) required for inference FLOPs 5. This approximation assumes hardware is being utilized as maximum efficiency both in through idealized floating point operation throughput and maximum power draw.

$$\begin{split} \widehat{E}_{\text{Optimal}} &= \left(\frac{\text{TDP}}{FLOPS_{HW}}\right) \\ &\times \sum_{(I^*,O^*)} N^{real}(I^*,O^*) \times FLOPs(I^*,O^*) \end{split}$$

<span id="page-7-3"></span>

| Dataset    | Mean $\pm$ Std        | Median | 99th |
|------------|-----------------------|--------|------|
| BurstGPT   | $256.80 \pm 242.27$   | 215    | 1038 |
| Azure Chat | $1631.58 \pm 1529.64$ | 928    | 6683 |
| Azure Code | $2511.28 \pm 2133.54$ | 1930   | 7685 |

Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads

<span id="page-7-4"></span>

| Dataset    | $\mathbf{Mean} \pm \mathbf{Std}$ | Median | 99th |
|------------|----------------------------------|--------|------|
| BurstGPT   | $35.10 \pm 108.59$               | 7      | 478  |
| Azure Chat | $105.51 \pm 158.25$              | 41     | 694  |
| Azure Code | $22.69 \pm 74.78$                | 8      | 271  |

Table 2: Output Sequence Length Statistics Across Real-World LLM Workloads

#### <span id="page-7-1"></span>4.3 Evaluations

We examine a suite of classical NLP tasks and LLM inference workloads, each characterized by a range of different input context and output generation sequences; with dataset statistics provided in Tables 3, 1, 2. We simulate a large-scale offline processing setting on the RTX A6000 GPUs, in which examples are binned by sequence lengths (as described in §4 and processed in parallel in the largest possible batches that fit in GPU memory.

Utilizing the simulated workloads described in Sec 4.1, we estimate the effectiveness of the inference efficiency optimizations evaluated in Section 4.1. Based on these results, we select an inference framework with efficiency optimizations targeting large batch inference. Concretely, we consider inference with a dense model utilizing vLLM with CUDA graph serialization (eager mode off) on a single GPU and compare it to unoptimized inference native PyTorch as a lower bound on energy efficiency. In addition, we also model the idealized energy baseline based on the model and hardware configurations.

Classical NLP Tasks. We benchmark the energy use in a set of classical natural language processing tasks in the English language: text classification (IMDB, Maas et al., 2011), machine translation (WMT-14, Bojar et al., 2014), summarization (CNN-DailyMail, Nallapati et al., 2016), and text generation (Wikitext-2 (Merity et al., 2016)).

For each of these tasks, we sample a subset of 1024 examples with statistics of each dataset for the input and the output tokens provided in Table 3.

<span id="page-7-0"></span>

| Task           | $\mathbf{Mean} \pm \mathbf{Std}$ | Max  | Output |
|----------------|----------------------------------|------|--------|
| Translation    | $49.96 \pm 39.39$                | 550  | 64     |
| Generation     | $136.89 \pm 93.13$               | 547  | 64     |
| Classification | $292.48 \pm 239.94$              | 3112 | 1      |
| Summarization  | $838.49 \pm 400.70$              | 2386 | 64     |

Table 3: Tokenized Input and Output Length Statistics Across NLP Tasks used for Energy Benchmarking

<span id="page-7-5"></span>![](_page_7_Figure_12.jpeg)

**Figure Description:**
The image is a scatter plot with three axes labeled "wiki," "imdb," and "task." Each axis has numerical values ranging from -0.1 to 0.5, indicating some form of measurement or scale for each dimension. There are four colored dots representing data points: one blue dot at (0.02, 0.46), two orange dots at (-0.03, 0.78) and (0.09, 0.78), and one green dot at (0.09, 0.78). These dots likely correspond to different categories or variables being plotted against these dimensions.

In addition to the dots, there are labels above each axis that provide context about the nature of the data represented by the dots. Above the wiki axis, there's an icon with text reading "PyTorch vLLM" which suggests that the data might be related to performance metrics for PyTorch version LLM when used with Wikipedia content. Similarly, above the imdb axis, there's another icon with text "Task" followed by "CNN" and "wmt," implying that the data could relate to tasks performed using Convolutional Neural Networks (CNNs) and possibly a metric called "wmt" as well. Lastly, above the task axis, there's an icon with text "Energy" followed by "0.02," which may indicate energy consumption levels associated with the tasks performed.

The overall layout of the graph seems to compare the results of certain tasks across different models or configurations, potentially involving natural language processing (NLP) given the reference to PyTorch and LLMs. However, without additional information, it's difficult to determine the exact meaning of the data points and their relationship to the various axes.



Figure 8: Energy Comparison in doing inference over 1024 samples between PyTorch with Compilation off and vLLM with eager model off.

We note that the input sequences were padded to the maximum sequence length. The energy profiles for the best run, characterized by the least energy are summarized in Figure 8, with consistent reductions in energy use provided by inference efficiency optimizations.

**Real-World LLM Workloads** Additionally, we estimate the energy intensity and effectiveness of efficiency optimizations on real-world LLM workloads. We simulate the offline processing of LLM inference requests as used in applications for shortform conversations with the Burst-GPT dataset (Wang et al., 2024) and long context conversations and code completion with the Azure LLM Inference chat and code traces (Stojkovic et al., 2024b). Each dataset provides a traces of LLM inference requests with their corresponding input context and output generation lengths. As compared with the classical NLP tasks, modern LLM workloads tend to be longer in both input context and output generation token lengths, with code-assist applications having longer contexts, whereas conversational settings resulting in longer generations.

Due to the larger number of requests and increased sequence lengths, we observe that these workloads require substantially larger amounts of energy. However, we find that proper applications of inference efficiency optimizations can substantially reduce energy costs with savings of 73.00%,

<span id="page-7-2"></span> $<sup>^5</sup>$ Based on the Nvidia datasheet for the RTX A6000 GPU, we utilize consider  $FLOPS_{HW}$  of 309.7 TFLOPS and a 300W TDP power draw; and estimate theoretical inference FLOPs with the DeepSpeed profiler (Rasley et al., 2020).

<span id="page-8-0"></span>

| Dataset            | PyTorch %∆ | vLLM % ∆ |
|--------------------|------------|----------|
| BurstGPT           | 506.52%    | 63.75%   |
| Azure Code         | 102.79%    | 26.59%   |
| Azure Conversation | 490.23%    | 64.22%   |

Table 4: Percentage differences of energy consumption relative to theoretical values for Various Tasks with Offline Inference.

37.58%, and 72.18% on BurstGPT, Azure Code and Conversation, respectively.

## 5 Related Work

Efficient Methods for LLM Inference To meet the service-level-objective (SLO) serving requirements of real deployment settings, efficiency optimizations for LLM inference are often designed to optimize model serving speed, as measured by latency and time-to-first-token. A variety of methods have been developed to meet these latency constraints, including: continuous batching [\(Yu et al.,](#page-12-0) [2022\)](#page-12-0), model parallelism [\(Narayanan et al.,](#page-11-10) [2021;](#page-11-10) [Huang et al.,](#page-10-17) [2019;](#page-10-17) [Li et al.,](#page-10-18) [2020\)](#page-10-18), speculative decoding [\(Liu et al.,](#page-10-13) [2024;](#page-10-13) [Leviathan et al.,](#page-10-6) [2023;](#page-10-6) [Chen et al.,](#page-9-6) [2023,](#page-9-6) [2025\)](#page-9-10), and disaggregated serving [\(Zhong et al.,](#page-12-4) [2024\)](#page-12-4).

Solely optimizing system performance for speed is insufficient in characterizing and does not provide insight into the model energy use and resulting carbon emissions of LLM inference; as such methods may require additional computation or exhibit low correlation between efficiency cost indicators [\(Dehghani et al.,](#page-9-12) [2022\)](#page-9-12). Recent work has explored methods for explicitly reducing energy requirements and carbon emissions for LLM serving via disaggregated serving over heterogeneous hardware [\(Shi et al.,](#page-11-14) [2024\)](#page-11-14), system-wide scheduling and request routing to energy-optimized instances [\(Stojkovic et al.,](#page-11-13) [2024b\)](#page-11-13), and prompt directives to induce shorter sequence generations [\(Li et al.,](#page-10-10) [2024\)](#page-10-10). However, the exact impact or improvements in energy requirements for latency-optimized methods remains not fully characterized.

Estimations and Measurement of of Energy Use in NLP The energy and carbon emissions of machine learning models have been a growing concern in the research community and industry as the scale of models and prevalence of deployment has increased [\(Schwartz et al.,](#page-11-15) [2020;](#page-11-15) [Wu et al.,](#page-11-3) [2022\)](#page-11-3). Estimations of the energy requirements and environmental impact of LLMs has largely focused on estimation of costs for pretraining and finetuning

due to the large singular costs of model developments [\(Strubell et al.,](#page-11-1) [2020;](#page-11-1) [Wang et al.,](#page-11-16) [2023;](#page-11-16) [Luccioni et al.,](#page-10-19) [2023;](#page-10-19) [Faiz et al.,](#page-9-13) [2023\)](#page-9-13); with large industrial developers similarly reporting the energy required for pretraining [\(OLMo et al.,](#page-11-17) [2024;](#page-11-17) [Morri](#page-10-1)[son et al.,](#page-10-1) [2025;](#page-10-1) [Dubey et al.,](#page-9-3) [2024\)](#page-9-3).

In contrast to training, inference workloads are higher in variability with variation in request frequencies, batching, input and output sequence lengths executed over diverse hardware platforms at scale; and more complex energy use profiles due to variations in power draw during prefill and decoding stages of generation [\(Patel et al.,](#page-11-5) [2024\)](#page-11-5). Previous work has investigated the comparative energy cost of machine learning models across various tasks [\(Luccioni et al.,](#page-10-3) [2024b,](#page-10-3)[a\)](#page-10-20), the energy costs of LMs of various sizes [\(Samsi et al.,](#page-11-18) [2023;](#page-11-18) [Wu et al.,](#page-11-19) [2025\)](#page-11-19), the effects of hardware configurations (i.e. GPU power capping and frequency scaling; [\(Samsi et al.,](#page-11-18) [2023\)](#page-11-18)), and the impact of sequence length variability and batching strategies [\(Patel et al.,](#page-11-5) [2024;](#page-11-5) [Stojkovic et al.,](#page-11-20) [2024a;](#page-11-20) [Wilkins](#page-11-21) [et al.,](#page-11-21) [2024\)](#page-11-21). However, such evaluations of inference energy use often rely on simplified deployment settings with limited sets of model architectures and serving frameworks.

## 6 Conclusion

In this work, we evaluate the impact of common inference efficiency optimizations on the energy requirements of large language model serving. We examine a variety of optimization techniques and evaluate on representative data corresponding to classical NLP tasks as well as modern LLM deployment settings. We conclude that the effectiveness of latency optimizations in reducing energy use is highly sensitive to the shape of the input data, underlying hardware architecture, and software framework implementations; and that optimizations cannot be applied uniformly.

Additionally, we conduct a case study of classical NLP tasks and real-world LLM inference workloads and find that proper application of the studied inference optimizations can reduce total energy use by up to 73% on the BurstGPT chat dataset.

# Limitations and Risks

In this work, we evaluate the energy efficiency and carbon emissions of LLM inference as approximated by total GPU power usage. Although GPUs the majority of arithmetic operations required for

inference and operate at a higher TDP than other components, we do not account for the energy use by other other components of the hardware system such as power use from CPU, memory, or disk storage [\(McAllister et al.,](#page-10-21) [2024;](#page-10-21) [Patel et al.,](#page-11-5) [2024\)](#page-11-5); or estimate the energy requirements of other hardware accelerator architectures (e.g. TPUs, NPUs, etc.). Likewise, we conduct an investigation of commonly used inference software frameworks and standard efficiency optimizations. However, there remain other settings and computational optimizations that can be applied to LLM inference, such as utilizing: reduced or mixed precision, adaptive adjustment of GPU frequency, additional forms of model parallelism, or other forms of load management and workload scheduling; which remain out of the scope of this work [\(Stojkovic et al.,](#page-11-13) [2024b\)](#page-11-13).

In this work, we primarily focus on the operation energy use of machine learning inference. Estimation of the embodied costs of inference; and the costs of machine learning training remain out of the scope of this work.

Although improved characterization of the energy use of LLM inference can be used to design more efficient serving settings and reduce the energy needs of inference, it is possible that reductions in the cost of pretraining may then lead more individuals and organizations to pursue large model pretraining (i.e. Jevons Paradox).

## References

- <span id="page-9-5"></span>Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming throughput-latency tradeoff in llm inference with sarathi-serve. *Proceedings of 18th USENIX Symposium on Operating Systems Design and Implementation, 2024, Santa Clara*.
- <span id="page-9-1"></span>Jordan Aljbour, Tom Wilson, and P Patel. 2024. Powering intelligence: Analyzing artificial intelligence and data center energy consumption. *EPRI White Paper no. 3002028905*.
- <span id="page-9-4"></span>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. *arXiv preprint arXiv:2309.16609*.
- <span id="page-9-2"></span>Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance costeffective inferencing.
- <span id="page-9-11"></span>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling,

- Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ales Tamchyna. 2014. [Findings of the 2014 workshop on](http://www.aclweb.org/anthology/W/W14/W14-3302) [statistical machine translation.](http://www.aclweb.org/anthology/W/W14/W14-3302) In *Proceedings of the Ninth Workshop on Statistical Machine Translation*, pages 12–58, Baltimore, Maryland, USA. Association for Computational Linguistics.
- <span id="page-9-0"></span>Kendrick Cai and Deborah Mary Sophia. 2025. Alphabet plans massive capex hike, reports cloud revenue growth slowed. *Reuters*.
- <span id="page-9-6"></span>Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. *arXiv preprint arXiv:2302.01318*.
- <span id="page-9-10"></span>Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, and Beidi Chen. 2025. Magicdec: Breaking the latencythroughput tradeoff for long context generation with speculative decoding. In *The Thirteenth International Conference on Learning Representations*.
- <span id="page-9-9"></span>Benoit Courty, Victor Schmidt, Sasha Luccioni, Goyal-Kamal, MarionCoutarel, Boris Feld, Jérémy Lecourt, LiamConnell, Amine Saboni, Inimaz, supatomic, Mathilde Léval, Luis Blanche, Alexis Cruveiller, ouminasara, Franklin Zhao, Aditya Joshi, Alexis Bogroff, Hugues de Lavoreille, Niko Laskaris, Edoardo Abati, Douglas Blank, Ziyao Wang, Armin Catovic, Marc Alencon, Michał St˛echły, Christian Bauer, Lucas Otávio N. de Araújo, JPW, and MinervaBooks. 2024. [mlco2/codecarbon: v2.4.1.](https://doi.org/10.5281/zenodo.11171501)
- <span id="page-9-12"></span>Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, and Ashish Vaswani. 2022. [The efficiency misnomer.](https://openreview.net/forum?id=iulEMLYh1uR) In *International Conference on Learning Representations*.
- <span id="page-9-3"></span>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*.
- <span id="page-9-13"></span>Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan Chen, and Lei Jiang. 2023. Llmcarbon: Modeling the end-to-end carbon footprint of large language models. *arXiv preprint arXiv:2309.14393*.
- <span id="page-9-8"></span>Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, and Emma Strubell. 2023. The framework tax: Disparities between inference efficiency in nlp research and deployment. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 1588–1600.
- <span id="page-9-7"></span>Stefanos Georgiou, Maria Kechagia, Tushar Sharma, Federica Sarro, and Ying Zou. 2022. Green ai: Do deep learning frameworks have different costs? In *Proceedings of the 44th International Conference on Software Engineering*, pages 1082–1094.

- <span id="page-10-4"></span>Alistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. 2024. How data centers and the energy sector can sate ai's hunger for power. *McKinsey and Company*.
- <span id="page-10-9"></span>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. *arXiv preprint arXiv:2402.00838*.
- <span id="page-10-12"></span>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. *arXiv preprint arXiv:2501.12948*.
- <span id="page-10-11"></span>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. [The curious case of neural text de](https://openreview.net/forum?id=rygGQyrFvH)[generation.](https://openreview.net/forum?id=rygGQyrFvH) In *International Conference on Learning Representations*.
- <span id="page-10-17"></span>Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. *Advances in neural information processing systems*, 32.
- <span id="page-10-0"></span>Mike Isaac. 2025. Meta to increase spending to \$65 billion this year in a.i. push. *New York Times*.
- <span id="page-10-7"></span>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In *Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles*.
- <span id="page-10-5"></span>George Leopold. 2019. Aws to offer nvidia's t4 gpus for ai inferencing. *URL: https://web. archive. org/web/20220309000921/https://www. hpcwire. com/2019/03/19/aws-upgrades-its-gpu-backed-aiinference-platform/(visited on 2022-04-19)*.
- <span id="page-10-6"></span>Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In *International Conference on Machine Learning*, pages 19274–19286. PMLR.
- <span id="page-10-10"></span>Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. 2024. Sprout: Green generative ai with carbon-efficient llm inference. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing*, pages 21799–21813.
- <span id="page-10-2"></span>Pengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. 2025. [Making AI Less "Thirsty": Un](https://doi.org/10.48550/arXiv.2304.03271)[covering and Addressing the Secret Water Footprint](https://doi.org/10.48550/arXiv.2304.03271) [of AI Models.](https://doi.org/10.48550/arXiv.2304.03271) *arXiv preprint*. ArXiv:2304.03271 [cs].
- <span id="page-10-18"></span>Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch

- distributed: Experiences on accelerating data parallel training. *arXiv preprint arXiv:2006.15704*.
- <span id="page-10-13"></span>Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. 2024. Optimizing speculative decoding for serving large language models using goodput. *arXiv preprint arXiv:2406.14066*.
- <span id="page-10-19"></span>Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Estimating the carbon footprint of bloom, a 176b parameter language model. *Journal of Machine Learning Research*, 24(253):1–15.
- <span id="page-10-20"></span>Sasha Luccioni, Boris Gamazaychikov, Sara Hooker, Régis Pierrard, Emma Strubell, Yacine Jernite, and Carole-Jean Wu. 2024a. Light bulbs have energy ratings—so why can't ai chatbots? *Nature*, 632(8026):736–738.
- <span id="page-10-3"></span>Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2024b. [Power hungry processing: Watts driving the](https://doi.org/10.1145/3630106.3658542) [cost of ai deployment?](https://doi.org/10.1145/3630106.3658542) In *Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency*, FAccT '24, page 85–99, New York, NY, USA. Association for Computing Machinery.
- <span id="page-10-14"></span>Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In *Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies*, pages 142–150.
- <span id="page-10-21"></span>Sara McAllister, Fiodar Kazhamiaka, Daniel S Berger, Rodrigo Fonseca, Kali Frost, Aaron Ogus, Maneesh Sah, Ricardo Bianchini, George Amvrosiadis, Nathan Beckmann, et al. 2024. A call for research on storage emissions. In *Proceedings of the 3rd Workshop on Sustainable Computer Systems (HotCarbon)*.
- <span id="page-10-16"></span>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. [Pointer sentinel mixture mod](https://arxiv.org/abs/1609.07843)[els.](https://arxiv.org/abs/1609.07843) *Preprint*, arXiv:1609.07843.
- <span id="page-10-1"></span>Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, and Jesse Dodge. 2025. [Holistically evaluating the environmental impact of](https://openreview.net/forum?id=04qx93Viwj) [creating language models.](https://openreview.net/forum?id=04qx93Viwj) In *The Thirteenth International Conference on Learning Representations*.
- <span id="page-10-8"></span>Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. 2024. Olmoe: Open mixture-of-experts language models. *arXiv preprint arXiv:2409.02060*.
- <span id="page-10-15"></span>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çaglar Gu ˘ ˙lçehre, and Bing Xiang. 2016. [Abstrac](https://doi.org/10.18653/v1/K16-1028)[tive text summarization using sequence-to-sequence](https://doi.org/10.18653/v1/K16-1028) [RNNs and beyond.](https://doi.org/10.18653/v1/K16-1028) In *Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning*, pages 280–290, Berlin, Germany. Association for Computational Linguistics.

- <span id="page-11-10"></span>Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In *Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis*, pages 1– 15.
- <span id="page-11-17"></span>Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo 2 furious. *arXiv preprint arXiv:2501.00656*.
- <span id="page-11-9"></span>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. *Advances in neural information processing systems*, 32.
- <span id="page-11-5"></span>Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, and Ricardo Bianchini. 2024. Characterizing power management opportunities for llms in the cloud. In *Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3*, pages 207–222.
- <span id="page-11-4"></span>David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. [The carbon footprint of machine learn](https://arxiv.org/abs/2204.05149)[ing training will plateau, then shrink.](https://arxiv.org/abs/2204.05149) *Preprint*, arXiv:2204.05149.
- <span id="page-11-6"></span>Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. [Com](https://arxiv.org/abs/1911.05507)[pressive transformers for long-range sequence mod](https://arxiv.org/abs/1911.05507)[elling.](https://arxiv.org/abs/1911.05507) *arXiv preprint*.
- <span id="page-11-11"></span>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.
- <span id="page-11-18"></span>Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs of large language model inference. In *2023 IEEE High Performance Extreme Computing Conference (HPEC)*, pages 1–9. IEEE.
- <span id="page-11-15"></span>Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai. *Communications of the ACM*, 63(12):54–63.
- <span id="page-11-2"></span>Arman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024. 2024 united states data center energy usage report.

- <span id="page-11-14"></span>Tianyao Shi, Yanran Wu, Sihang Liu, and Yi Ding. 2024. Greenllm: Disaggregating large language model serving on heterogeneous gpus for lower carbon emissions. *arXiv preprint arXiv:2412.20322*.
- <span id="page-11-0"></span>Brad Smith. 2025. The golden opportunity for american ai.
- <span id="page-11-20"></span>Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas. 2024a. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference. *arXiv preprint arXiv:2403.20306*.
- <span id="page-11-13"></span>Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Torrellas, and Esha Choukse. 2024b. Dynamollm: Designing llm inference clusters for performance and energy efficiency. *arXiv preprint arXiv:2408.00741*.
- <span id="page-11-1"></span>Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2020. Energy and policy considerations for modern deep learning research. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pages 13693–13696.
- <span id="page-11-16"></span>Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, and Sasha Luccioni. 2023. [Energy and car](https://doi.org/10.18653/v1/2023.findings-emnlp.607)[bon considerations of fine-tuning BERT.](https://doi.org/10.18653/v1/2023.findings-emnlp.607) In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 9058–9069, Singapore. Association for Computational Linguistics.
- <span id="page-11-12"></span>Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhenheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, and Xiaowen Chu. 2024. [Burst](https://arxiv.org/abs/2401.17644)[gpt: A real-world workload dataset to optimize llm](https://arxiv.org/abs/2401.17644) [serving systems.](https://arxiv.org/abs/2401.17644) *Preprint*, arXiv:2401.17644.
- <span id="page-11-21"></span>Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Offline energy-optimal llm serving: Workloadbased energy models for llm inference on heterogeneous systems. *ACM SigEnergy newletter*.
- <span id="page-11-8"></span>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In *Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations*, pages 38–45.
- <span id="page-11-3"></span>Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. *Proceedings of Machine Learning and Systems*, 4:795–813.
- <span id="page-11-19"></span>Yanran Wu, Inez Hua, and Yi Ding. 2025. Unveiling environmental impacts of large language model serving: A functional unit view. *arXiv preprint arXiv:2502.11256*.
- <span id="page-11-7"></span>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. *arXiv preprint arXiv:2412.15115*.

<span id="page-12-0"></span>Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A distributed serving system for {Transformer-Based} generative models. In *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)*, pages 521–538.

<span id="page-12-4"></span>Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In *18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)*, pages 193–210.

## <span id="page-12-1"></span>A Hardware Details

In Table [5,](#page-13-1) we provide additional details on the hardware configurations of the nodes used in our benchmarking experiments.

## B Dataset Licenses

The CNN-DailyMail dataset used for summarization is released under the Apache-2.0 License. The dataset Wikitext-2 dataset for text generation is available under the Creative Commons Attribution-ShareAlike License. The WMT-14 translation datasets are released for non-commercial use. The BurstGPT and Azure trace datasets are released under CC-BY-4.0 licenses.

## C Acknowledgment of AI Assistance

Artificial intelligence assistance was used to assist in literature review and for code completion assistance, specifically during the creation of visualizations.

# <span id="page-12-3"></span>D Additional Optimzations: Continuous Batching

In Figure [9,](#page-13-2) we present additional results on the impact of vLLM's continuous batching for online inference in which we observe that at large batch sizes continuous batching yields reductions in energy use.

## <span id="page-12-2"></span>E Additional Sequence Length Results

In Figure [10,](#page-13-0) we present additional results on the effects of scaling input and output sequence lengths with the PyTorch framework.

<span id="page-13-1"></span>

| CPU               | RAM   | GPU                   | GPU TDP | FP32 TFLOPS | Bfloat16 TFLOPS |
|-------------------|-------|-----------------------|---------|-------------|-----------------|
| 256xAMD EPYC 7763 | 1TB   | Nvidia RTX A6000      | 300W    | 38.7        | _               |
| 128xAMD EPYC 7513 | 500GB | Nvidia RTX A6000 Ada  | 300W    | 91.1        | _               |
| 128xAMD EPYC 7763 | 1TB   | Nvidia RTX A100-80 GB | 300W    | 156         | 312             |

Table 5: Node Hardware Specifications

<span id="page-13-2"></span>![](_page_13_Figure_2.jpeg)

**Figure Description:**
The image displays three separate graphs with a similar structure, each representing energy reduction over time for different batches of an A100 8GB PCle device. Each graph is labeled as (a) A100 8GB PCIe, indicating that these are performance metrics related to a specific type of memory card used by the device.

The x-axis represents "Batch size," which likely refers to the number of iterations or computational steps performed during some process within the device. This could be referring to training cycles for machine learning models, for example.

The y-axis indicates "Energy Reduction (J)," suggesting that the vertical axis shows the amount of energy saved or consumed at various points along the batch size scale. The units of measurement are Joules (J).

Each graph has two lines: one in purple and another in orange. These lines seem to represent different data sets or experiments, possibly comparing the energy efficiency between different versions or configurations of the device's software or hardware.

In all three graphs, there are several dots marked in red. These dots appear to indicate specific milestones or significant changes in the energy consumption rate. For instance, in the first graph, there are four such red dots, while the second graph has five, and the third graph has six. It seems like these red dots might highlight particular events or conditions where the energy saving rate increases significantly.

At the bottom right corner of each graph, there is text providing additional information about the data being presented. In the first graph, it reads "(a) A100 8GB PCIe." Similarly, the other two graphs also have labels "(b) A600 8GB AdA" and "(c) A6000 8GB AdA." These labels suggest that the graphs compare different types of devices or components, specifically mentioning "AdA," which may refer to a specific technology or feature associated with the device's operation.

Overall, the image provides a detailed comparison of energy efficiency across multiple batches of a device using different configurations or settings.



Figure 9: Energy reduction comparison between online and offline serving modes across different GPUs  $(E_{offline}-E_{online})*100/E_{offline})$ . The optimizations employed for online serving save up to 5% energy at larger batch sizes

<span id="page-13-0"></span>![](_page_13_Figure_4.jpeg)

**Figure Description:**
The image is a graphical representation of three plots comparing energy scaling for input sequence length across different token types: BPE (Byte Pair Encoding), BS-64 (Byte Subsequence with 64 tokens), and BS-512 (Byte Subsequence with 512 tokens). Each plot shows two lines representing two different models or methods: one labeled "BS=64" and another labeled "BS=512". These labels suggest that the data points are related to byte subseqences with varying lengths.

The x-axis represents the input sequence length in tokens, ranging from 0 to approximately 10^3 tokens. This axis indicates that as the number of tokens increases, so does the complexity of the input sequences.

On the y-axis, there are three sets of numerical values corresponding to each model's performance at various input sequence lengths. For example, the first set of numbers under the "BS=64" label corresponds to the energy scaling for input sequence length when using BPE tokenization. Similarly, the second set of numbers under the "BS=512" label corresponds to the energy scaling for input sequence length when using BS-512 tokenization.

Each line has several peaks and valleys, which likely correspond to specific patterns within the input sequences. The peaky nature of these graphs suggests that certain combinations of tokens may be more energetically favorable than others. However, without additional context, it is difficult to determine the exact meaning of 'energy', 'input', or 'output'.

Overall, the image appears to be a scientific figure used to compare the efficiency or effectiveness of different tokenization schemes in handling complex input sequences. It provides a visual comparison between the performance of the two models across various input sequence lengths.



Figure 10: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vanilla PyTorch backend.

<span id="page-14-0"></span>![](_page_14_Figure_0.jpeg)

**Figure Description:**
The image is a graphical representation of four different plots comparing energy efficiency (in Watts per square meter) between two types of sequences: prefill energy vs input sequence length for both decode energy and generation duration vs output sequence length. Each plot has three subplots labeled as B5-1, B5-8, and BS6-4, which likely correspond to different models or algorithms being compared.

In each subplot, there are three lines representing different conditions or parameters:

1. Blue line with blue triangles at the top right corner represents "BS1" with an average energy consumption of approximately 0.27 W/m^2.
2. Orange line with orange squares at the top right corner represents "BS8" with an average energy consumption of about 0.39 W/m^2.
3. Green line with green diamonds at the top right corner represents "BS6-4" with an average energy consumption of around 0.51 W/m^2.

The x-axis of all graphs indicates the output sequence length in tokens, ranging from 10^0 to 10^6 tokens. The y-axis shows the energy consumption in Watts per square meter, ranging from 10^-10 to 10^10 W/m^2.

Each subplot includes error bars indicating the variability within the data points. Additionally, there's a legend in the bottom left corner that explains the symbols used to denote the different models or conditions.

Overall, the image provides a detailed comparison of energy efficiency across various model configurations and sequence lengths. It appears to be related to machine learning or computational modeling, where such comparisons might help researchers understand how different approaches affect computational resources.



Figure 11: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vLLM offline inference. Here, we display multiple fixed sequence length sizes for comparison as we sweep across batch size and the other dimension of sequence length.

<span id="page-15-0"></span>![](_page_15_Figure_0.jpeg)

**Figure Description:**
The image appears to be a graphical representation of data from an experiment or study involving textgen summarization. It is divided into three panels labeled "classification," "summarization," and "translation." Each panel contains two sub-panels: one for classification (left) and one for translation (right).

In each sub-panel, there are four bars representing different conditions or variables. These bars have numerical labels ranging from 0.02 to 1.00, which likely correspond to some form of performance metric such as accuracy, error rate, or another evaluation measure.

The leftmost bar in each sub-panel has a label that reads "energy" with a value of 0.05. This could indicate the energy consumption associated with the process being measured.

On the right side of each sub-panel, there are additional bars corresponding to the categories "textgen" and "translation." These bars also range from 0.02 to 1.00 but do not contain any visible labels indicating their specific meaning within the context of the study.

Overall, the image seems to compare the effectiveness of various methods related to natural language processing tasks, including classification, summarization, and translation. However, without more information about the study's methodology and objectives, it is difficult to provide a detailed interpretation of the results presented.



Figure 12: Classical NLP tasks and their energy intensities with vLLM backends. From top to bottom, the batch size varies from 1, 8, to 128