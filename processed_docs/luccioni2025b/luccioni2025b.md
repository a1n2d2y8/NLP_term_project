# Bridging the Gap:

# Integrating Ethics and Environmental Sustainability in AI Research and Practice

ALEXANDRA SASHA LUCCIONI and GIADA PISTILLI, Hugging Face, Canada/France RAESETJE SEFALA and NYALLENG MOOROSI, Distributed AI Research Institute, Canada/Lesotho

As the possibilities for Artificial Intelligence (AI) have grown, so have concerns regarding its impacts on society and the environment. However, these issues are often raised separately; i.e. carbon footprint analyses of AI models typically do not consider how the pursuit of scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately harmful to the environment. On the other hand, model audits that aim to evaluate model performance and disparate impacts mostly fail to engage with the environmental ramifications of AI models and how these fit into their auditing approaches. In this separation, both research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential solutions for making informed choices that can be developed at their convergence. In this essay, we build upon work carried out in AI and in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such as generalizability, transparency, evaluation and equity across AI research and practice. We argue that the efforts aiming to study AI's ethical ramifications should be made in tandem with those evaluating its impacts on the environment, and we conclude with a proposal of best practices to better integrate AI ethics and sustainability in AI research and practice.

#### 1 INTRODUCTION

In recent years, AI systems have become pervasive, presented as a key tool in the fight against climate change [\[168\]](#page-20-0) and in societally-beneficial domains such as health and education [\[64](#page-17-0), [71,](#page-17-1) [87\]](#page-17-2). However, the training and deployment of AI systems also comes with a cost in terms of energy and natural resources [\[52,](#page-16-0) [117,](#page-18-0) [192\]](#page-21-0) and can inadvertently result in the amplification of inequalities [\[183](#page-21-1)] and proliferation of biases [\[6\]](#page-15-0) when systems are put into practice. Historically, the societal and environmental impacts of AI systems have been addressed separately, in two distinct areas of study – i.e. scholarship that aims to address the ethics of AI models focuses on aspects such as bias evaluation or auditing [\[33,](#page-16-1) [201\]](#page-21-2), typically overlooking models' impacts on natural resources and ecosystems [\[127](#page-19-0)]. Conversely, work that aims to estimate the growing carbon footprint and energy consumption of AI models [\[121](#page-19-1), [192\]](#page-21-0) does not typically address the contribution this has towards shifting the balance of power or amplifying inequalities [\[1](#page-15-1), [2\]](#page-15-2).

Some recent scholarship has started to establish explicit links between ethics and sustainability often hones in on specific applications, such as the emblematic "Stochastic Parrots" paper, which addresses both sustainability and ethics as issues in the context of large language models (LLMs) [\[16](#page-15-3)]. Apart from this exception and a precious few others, conversations around AI ethics and sustainability have taken place separately, in different venues and by different sets of stakeholders. However, since in both AI sustainability and AI ethics research, the aim is to think through how we might develop technologies which are useful and inclusive while we limit harm to people and environments, similar themes emerge, often structured via concepts such as functionality and efficiency just as much as justice, fairness and equity. In both of these disciplines.

In the present article, we highlight these transversal themes and argue that both the societal and environmental impacts of AI systems should be considered in parallel in all aspects of AI theory, practice and governance. Drawing upon work from within the AI community as well as related fields, we explore a series of use cases illustrating that

Authors' addresses: Alexandra Sasha Luccioni; Giada Pistilli, sasha.luccioni@huggingface.co, Hugging Face, Canada/France; Raesetje Sefala; Nyalleng Moorosi, Distributed AI Research Institute, Canada/Lesotho.

ethics and sustainability go hand-in-glove when it comes to the development and the deployment of AI systems. By doing so, we hope to shed light on the importance of pursuing research that blends together different perspectives to allow for a better understanding of the societal impacts of AI systems.

Given that our goal is to allow our work to be read and understood by a variety of audiences, we start, in Section [§2,](#page-1-0) by defining the concepts and terms that are core to our subsequent analysis. We continue by examining the current state of AI ethics and sustainability from 3 different but complementary perspectives: theoretical principles and frameworks ([§3.1\)](#page-2-0), AI research and practice([§3.2\)](#page-3-0), and AI regulation and governance ([§3.3\)](#page-4-0). Next, in [§4,](#page-5-0) we identify four transversal themes that we have found to be particularly central to both AI ethics and sustainability: generalizability, evaluation, transparency, and power. Finally, in [§5,](#page-9-0) we propose ways forward at the intersection of these themes and three directions of AI research and practice. We wrap up the article with ideas for future work that can be pursued at the nexus of AI ethics and sustainability and a brief conclusion.

#### <span id="page-1-0"></span>2 KEY CONCEPTS AND DEFINITIONS

## Sustainability

When it comes to the concept of sustainability, one of its first and most widely-accepted definitions originates from the 1987 Brundtland Report, which defines sustainable development as "development that meets the needs of the present without compromising the ability of future generations to meet their own needs" [\[1987\]](#page-17-3)[p.41]. This definition remains central to the field of sustainability write large, informing frameworks such as the UN Sustainable Development Goals (SDGs), which were developed in 2015 as a blueprint for achieving peace and prosperity for people and the planet [\[199\]](#page-21-3). However, also in 1987, environmental economist Edward Barbier proposed an alternative definition to sustainability, structuring it around three pillars: environmental, societal and economic, arguing that sustainable development can only be truly achieved when both environmental stewardship, social equity and economic viability coexist and are inter-connected [\[1987](#page-15-4)].

In the context of AI, the term 'sustainability' is most often used to refer solely to environmental sustainability [\[41,](#page-16-2) [61\]](#page-17-4). The umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [\[203\]](#page-21-4). This proposal would then encompass the vast variety of work being done at the nexus of machine learning and fields such as biodiversity monitoring, agriculture, transportation, etc. (for a review, see [\[206\]](#page-21-5) and [\[168\]](#page-20-0)). AI and sustainability has also been central to workshops such as SustaiNLP and the International Sustainable AI Workshop, that have put the emphasis on efficient methods and the application of AI to sustainability-related problems, as well as the Tackling Climate Change with Machine Learning workshop, which aims to demonstrate that AI can be an invaluable tool in helping society adapt to and mitigate the effects of climate change.

# Ethics

Inherently characterized by ongoing perplexity, ethics aspires for certainty and consensus, yet also remains dynamic and evolving. It has its origins in the work of philosophers such as Aristotle, who posited that ethics connects theory with praxis, with the goal of guiding human actions towards eudaimonia (i.e. the highest human good) [\[8](#page-15-5)]. In the millennia since Aristotle, the philosophical sub-domain of applied ethics has sought to establish normative principles for a variety of human activities and domains, which inexorably depend on the context of application and the individuals

involved, leading to much debate regarding which norms should be applicable in which contexts, as well as the definition of key ethical concepts such as fairness [\[89,](#page-17-5) [175\]](#page-20-1), transparency [\[60,](#page-17-6) [147](#page-19-2)] and, indeed, the very definition of ethics itself [\[21](#page-15-6), [195\]](#page-21-6).

Lacking consensus, the field of AI ethics often applies Western moral theories ranging from utilitarianism [\[18](#page-15-7), [134\]](#page-19-3) to egalitarianism [\[209\]](#page-22-0) and virtue ethics [\[7,](#page-15-8) [8\]](#page-15-5) to propose ways of assessing the ethicality of AI systems. However, the application of these moral theories faces challenges given the difficulty of, e.g. quantifying the concept of utility in utilitarianism, assessing and comparing who is worse off in egalitarianism, or evaluating cultural variability in defining virtues in virtue ethics. This is also the case in terms of the application of these theories in modern-day contexts involving new types of AI-driven technologies such as robots or autonomous vehicles, which can be limited without a comprehensive understanding of both AI's technical capabilities (e.g. the limitations of the underlying models) as well as the diversity of life experiences of the people using these tools, who can interact with them in ways that are hard to predict or design for [\[102](#page-18-1)]. Given that these concepts encompass work from a multitude of domains that espouse different objectives, values and methods [\[23](#page-15-9)], their definition can have major consequences on the way in which these concepts are operationalized in AI models and systems [\[10,](#page-15-10) [65](#page-17-7), [183](#page-21-1)]. Work that addresses the ethical aspects of AI systems is discussed and published in conferences such as the ACM Conference on Fairness, Accountability, and Transparency (FAccT), as well as the AAAI/ACM Conference on AI, Ethics, and Society (AIES), which both have a cross-disciplinary focus and cover a multitude of topics in terms of the societal and ethical aspects of AI.

#### 3 EXISTING SCHOLARSHIP IN AI ETHICS AND SUSTAINABILITY

In the sections below, we explore existing scholarship in order to critically analyze how both ethics and sustainability are defined in theory and operationalized in practice within the diverse communities that pursue AI. First, we describe the principles and frameworks that have been proposed to guide AI from both an ethical and environmental perspective; next, we examine how these principles are applied, implicitly and explicitly, in AI research and practice. Finally, we look at several recent approaches for regulating and governing AI and the different roles adopted by stakeholders and organizations.

#### <span id="page-2-0"></span>3.1 Principles and Frameworks

A common starting point to ensure the ethical development and deployment of AI systems is the definition of a structure for guiding this process, which can be operationalized via sets of principles or a framework. On the one hand, ethical principles are often defined at a high level, describing values and concepts outside of any specific context of deployment. As such, multiple sets of guiding principles for 'ethical AI' have been proposed by different organizations ranging from research institutes to nonprofit and for-profit entities. However, given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines. To this point, a 2019 analysis by Jobin et al. reviewed 84 sets of guidelines mentioning a variety of principles, finding very limited convergence between them but identifying the values of transparency, fairness, non-maleficence, privacy and responsibility as being most common [\[2019\]](#page-18-2). Similarly, a multitude of ethical frameworks have been proposed, with most of them espousing specific visions of ethics by putting an emphasis on aspects ranging from human empowerment [\[64\]](#page-17-0) to virtue ethics [\[79](#page-17-8)]. In comparison to principles, ethical frameworks often aim to frame ethics from the perspective of implementation, identifying how challenges can be addressed and how to build consensus around ethical values, often anchored to specific contexts of deployment of AI systems, such as medicine [\[125\]](#page-19-4) or autonomous vehicles [\[109](#page-18-3)]. However, the technological mechanisms proposed to operationalize

these frameworks are often defined in abstract terms, which have been found to be difficult to implement in practice from an engineering perspective, making them difficult to operationalize in practice [\[155](#page-20-2)]. In terms of environmental sustainability, the UN SDGs are most commonly used to inform AI frameworks [\[72](#page-17-9)], with inspiration from fields such as ecology to guide the definition of methods based on metrics and evaluation methods that enable a more holistic assessment of AI's environmental impacts. [\[112](#page-18-4)]. However, similarly to AI principles, there is an equal multiplicity of AI frameworks, and multiple analyses have been carried out with the goal of establishing overlap and transversal connections, which were found to be lacking [\[14](#page-15-11), [155,](#page-20-2) [205](#page-21-7)].

Conversely, analyses of ethical AI principles have also observed a general lack of recognition of AI's environmental impacts within the different sets of principles that have been defined. Different reasons have been proposed for this lack of connection, from the reliance of most AI principles on traditional Western ethical perspectives, which are humancentered and assign intrinsic value to human beings above other living things [\[28\]](#page-15-12), to the paucity of research on AI's environmental impacts, which hinders the development of coherent principles [\[25\]](#page-15-13). When sustainability is addressed in ethical AI frameworks, it is once again only limited to environmental sustainability, notably carbon footprint estimation. For instance, several industry-led sets of principles specifically addressing the environmental sustainability of AI have been proposed in recent years by organizations such as Salesforce [\[2024\]](#page-20-3) and the Green Software Foundation [\[2023\]](#page-17-10). While these propositions touch upon metrics such as energy efficiency, they do not address topics such as rebound effects [\[120](#page-18-5)] [1](#page-3-1) , transparency and access to compute, which we consider to be core to these discussions and which we discuss in Section [4.](#page-5-0) Nonetheless, certain frameworks, such as the UNESCO recommendations on the ethics of artificial intelligence [\[2021\]](#page-21-8) do emphasize the importance of sustainability and of evaluating technologies based on their environmental impacts via the UN SDGs, which, as discussed in the introduction, have sustainable development at their core – which we explore in more depth in subsequent sections.

## <span id="page-3-0"></span>3.2 Research and Practice

Given that AI is a distributed field consisting of a multitude of practitioners and organizations, the practical application of the principles and frameworks described in the previous section can differ immensely. In a 2022 study of papers submitted to conferences such as ICML and NeurIPS, Birhane et al. analyzed the values that were highlighted by their authors – i.e. the positive attributes of their project that they emphasized and the negative impacts they considered explicitly [\[2022a\]](#page-15-14). From the 59 values they identified, the most emphasis was put on aspects such as technical progress, quantitative evidence, and novelty, whereas ethical considerations around values such as beneficence, interpretability and respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a fraction of papers. Also, not a single one of the values Birhane et al. identified was explicitly linked to environmental sustainability, highlighting once again the lack of connection in the research community with sustainability writ large.

Progressing from this observation, while sustainability-oriented research has not been prominently featured in venues that directly address AI ethics, it remains a topic of research that has been gathering momentum in recent years. The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of 퐶푂<sup>2</sup> emissions [\[192\]](#page-21-0). Follow-up work by other researchers has shed more light on this issue, revealing different aspects of model architecture [\[153\]](#page-20-4) and training procedure [\[53](#page-16-3)] that can impact its ensuing

<span id="page-3-1"></span><sup>1</sup>The relationship between efficiency and sustainability is far from straightforward, given phenomena such as rebound effects, in which improved efficiency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resources – see [\[27,](#page-15-15) [120\]](#page-18-5) for a more in-depth review.

carbon footprint. The first proposal for "Green AI", i.e. AI research that takes environmental impacts into consideration when training AI models, was made by Schwartz et al. in 2020 [\[181\]](#page-20-5) - it was subsequently broadened to include aspects such as hardware and scaling [\[215\]](#page-22-1) and model deployment [\[121](#page-19-1)] more recently. However, this field of research, while increasingly prolific, has failed to take ethical considerations into account in its analyses, focusing solely on aspects such as carbon intensity and energy efficiency, and not on issues such as the environmental impacts of increased consumption due to the use of AI in targeted advertising [\[96\]](#page-18-6), or the application of AI in the oil and gas sector [\[77\]](#page-17-11), which are liable to counter-balance any actual efficiency gains [2](#page-4-1) .

#### <span id="page-4-0"></span>3.3 Governance and Regulation

Governance and regulation aim to establish mechanisms for decision-making, guiding the development and deployment of AI systems, and outlining the roles and responsibilities of each party involved [\[219\]](#page-22-2). There are many distributed efforts for governance in AI whose aim is to develop ethical guardrails, with some highlighting the importance of international institutions [\[85](#page-17-12)], and others focusing on the public sector [\[101](#page-18-7)] – reflecting that both bottom-up and top-down endeavors are useful to establish functional mechanisms for governing AI. If we take a look at recent community endeavors for AI governance, the 2022 Big Science workshop proposed a bottom-up approach that established mechanisms for various ethical aspects of the project such as data governance, quality metrics, and fostering stakeholder collaboration and transparency [\[92\]](#page-18-8), as well as drafting a consensus-driven ethical framework for governing the resulting artifacts that encompasses both legal and technical dimensions [\[154](#page-20-6)]. Interestingly, Big Science was one of the few projects that also considered and documented the carbon footprint of model training, evaluation and deployment, proposing a holistic, life cycle approach to estimating emissions [\[121](#page-19-1)]. There have also been proposals arguing for putting sustainability in the center of AI development and deployment [\[203\]](#page-21-4), as well as frameworks for certifying the sustainability of AI systems [\[25\]](#page-15-13), across all the different pillars of sustainability (i.e. social, environmental and economic) [\[70](#page-17-13)]. However, we are still at the beginning of building governance mechanisms that offer a more comprehensive analysis of the impacts of AI systems from the perspective of ethics and sustainability.

A pivotal example of top-down governance is the European Union's AI Act, which draws from broadly defined ethical principles to inform its regulations [\[39\]](#page-16-4). In fact, the text of the Act demonstrates considerable progress following the recent European dialogues, seeking to regulate AI applications that may infringe on human rights, adhering, among others, to the ethical principles of human oversight, human agency and transparency. Moreover, the AI Act's foundation on the premise that risk equates to potential human rights harms [\[152\]](#page-19-5) echoes one of the longstanding traditions in AI ethics of prioritizing human rights [\[9,](#page-15-16) [216](#page-22-3)]. This approach embodies the ethical commitment to safeguard fundamental freedoms and human rights in the digital era, ensuring that AI technologies do not infringe upon these important principles. However, while both EU AI Act [\[2022\]](#page-16-4), as well as similar regulatory initiatives in China [\[2023](#page-16-5)] and Canada [\[2023\]](#page-18-9) point to the need to protect both fundamental human rights as well as to limit damage to environment, there are no official provisions regarding sustainability in any of their texts, and it remains to be seen how existing standards for environmental impacts in all of these jurisdictions will apply to AI systems. Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [\[20](#page-15-17)], which did not mention AI's greenhouse gas emissions nor energy usage, as well as multi-nation declarations such as the Bletchley Declaration [\[2023\]](#page-15-18), illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.

<span id="page-4-1"></span><sup>2</sup>Recent media coverage of Microsoft's sustainability promises has estimated that a single contract to use AI to expand oil production "could enable carbon emissions adding up to 640 percent of the company's carbon removal targets" [\[191](#page-21-9)].

#### <span id="page-5-0"></span>4 TRANSVERSAL ISSUES IN AI ETHICS AND SUSTAINABILITY

"Is it fair . . . that the residents of the Maldives (likely to be underwater by 2100) or the 800,000 people in Sudan affected by drastic floods pay the environmental price of training and deploying ever larger English LMs, when similar large-scale models aren't being produced for Dhivehi or Sudanese Arabic?"

Bender, Gebru et al. [\[2021\]](#page-15-3)

In the current section, we define four recurring issues that we have found to be particularly salient to discussions around both AI ethics and sustainability. These issues are inspired by previous carried out by critical scholars such as Dobbe and Whittaker [\[2019](#page-16-0)], Birhane [\[2022b\]](#page-15-9), van Wynsberghe [\[2021](#page-21-4)] as well as Bender and Gebru [\[2021\]](#page-15-3), as cited above. We start, in Section [4.1](#page-5-1) with a discussion of the perils of assumptions of the generalizability of data and models in both ethics and sustainability. We follow, in Section [4.2](#page-6-0) with a reflection upon the current state of evaluation of AI systems, what is measured, what is missing, and why that matters. Next, we remark upon the current lack of transparency with regards to information relevant to both ethics and sustainabilityin Section [4.3.](#page-7-0) Finally, in Section [4.4,](#page-8-0) we discuss the balance of power and the allocation of justice, and how existing inequalities can be further amplified by AI systems.

#### <span id="page-5-1"></span>4.1 Generalizability

AI technologies function based on assumptions of generalizability and representativeness – i.e. that given sufficient data, an AI model can learn to accurately represent (any) given process and even adapt to previously unseen data [\[74\]](#page-17-14). For instance, the concept of pre-training AI models on large datasets such as ImageNet [\[47\]](#page-16-6) dates back to the early 1990s [\[178\]](#page-20-7) and has since become the dominant training paradigm in both computer vision [\[162,](#page-20-8) [193\]](#page-21-10) and natural language processing [\[50](#page-16-7), [114](#page-18-10)]. In fact, pre-training is heavily dependent upon the assumption of representativeness - i.e. that the huge amounts of training data used for pre-training represent the world as a whole, or at least a sufficient part of it to be useful for any kind of downstream application (i.e. fine-tuning, transfer learning, etc). While the limitations of such claims for both AI models and datasets have been previously shown (see Chasalow and Levy [\[36\]](#page-16-8), Koch et al. [\[98\]](#page-18-11), Raji et al. [\[157\]](#page-20-9), Smith et al. [\[188\]](#page-21-11)), the theory of generalizability, and the perception of certain types of AI models, ie. LLMs, as "general purpose technologies" continues to persist [\[57\]](#page-16-9). This can come with both ethical and environmental ramifications when systems trained under the pretense of generalizability are applied in contexts that differ from the ones represented in their training data – we discuss some of these below.

Given the data that fuels AI models is produced by humans, it is intrinsically laden with subjective judgments [\[132\]](#page-19-6) and representative of specific worldviews [\[46](#page-16-10)]. Numerous studies have shown that both the data used for training AI models [\[54](#page-16-11), [157,](#page-20-9) [166\]](#page-20-10) and the models themselves [\[16,](#page-15-3) [73](#page-17-15), [213\]](#page-22-4) are not, in fact, representative of the world at large and that the biases contained in training data persist even if further fine-tuning is carried out [\[103\]](#page-18-12), which can have 'cascade' effects when models are deployed in production [\[173\]](#page-20-11), which can contribute to perpetuating negative biases [\[68\]](#page-17-16). Similarly, off-the-shelf, proprietary technologies that are marketed as generic can fail at the tasks when applied in differing contexts, e.g. in applications such as facial recognition (when they fail to recognize people from populations under-represented in training datasets [\[33\]](#page-16-1)) and crime prediction (where they have dismal accuracy rates across different locations [\[174\]](#page-20-12)) – and yet, out-of-the-box AI systems for these tasks and many others continue to be built and deployed under the assumptions that they will work no matter the context of their application. This can have devastating effects on already marginalized communities when applied for tasks such as criminal sentencing [\[6\]](#page-15-0), and facial recognition [\[33](#page-16-1)].

Similarly to data concerning human beings, environmental and ecological data can also contain biases, for instance in its temporal coverage and geographical spread, which can be as damaging to biomes [3](#page-6-1) of plants and animals as they are to communities of humans [\[95](#page-18-13), [187\]](#page-21-12). From a modeling perspective, AI models trained to carry out biodiversity monitoring on one ecosystem often fail to perform accurately on others, no matter the species [\[99,](#page-18-14) [184](#page-21-13)], and yet ecological bias assessments are not often carried out before model training. In fact, White et al. refer to geographical differences in ecological datasets as the main factor limiting our capacity to predict how biodiversity will be impacted by future changes in the climate, notably due to missing data from regions such as Africa and South America, making data-driven approaches such as AI unrepresentative of entire continents [\[210](#page-22-5)].

An example of the dire consequences this can have can be found in the field of short-term climate modeling and disaster detection, which relies on data from sensors and weather radar stations that allow for events such as floods and wildfires to be tracked in real-time across borders, oceans and continents. While geospatial data is inherently global (given that satellites circle the planet as a whole), it relies upon assumptions of generalizability that often fail to hold when applied in contexts that differ from the training data, or the fact that many regions are missing these data sources for reasons ranging from insufficient funding and aging technological infrastructure to lack of connectivity [\[37,](#page-16-12) [198\]](#page-21-14), which results in data gaps [\[182,](#page-21-15) [197\]](#page-21-16). This can come at a cost to living beings, both human and animal, in the regions and contexts where these technologies are applied – for instance in early warning systems for extreme weather events [\[150\]](#page-19-7).and deforestation detection [\[97](#page-18-15)]. Of course, any assessment of representativity (or lack thereof) hinges upon an appropriate evaluation of this assumption – which we address in the following section.

### <span id="page-6-0"></span>4.2 Evaluation

A popular adage states that "you can't improve what you don't measure" [4](#page-6-2) ; in the context of AI systems, this can be translated into the fact that the criteria that we use to evaluate AI systems and the way in which this evaluation are carried out are important – i.e. the metrics we choose help us embed our values as communities about outcomes we wish to see and those that we put less emphasis on [\[136](#page-19-8)]. While leaderboards such as Papers With Code tend to only measure performance-based metrics such as accuracy or precision, factoring in other metrics can make comparisons between different models more meaningful and actionable. This is due to the fact that real-world constraints on model deployment often result in trade-offs being made between different factors that include accuracy and efficiency [\[31\]](#page-15-19), but also robustness [\[217](#page-22-6)], inclusion [\[88](#page-17-17)] and data quality [\[11\]](#page-15-20). This means that in order to meaningfully assess the utility of AI systems in different practical contexts, other measures must be considered in parallel to performance and accuracy [\[122\]](#page-19-9); and often the best people to do the assessments for the trade-offs are the populations who will use these tools. For example, when it comes to the evaluation of generative technologies such as large language models, these do not have a single well-established evaluation approach [\[139](#page-19-10), [159](#page-20-13)]. Approaches that are used for evaluating their ethical limitations include red-teaming [\[66](#page-17-18)], external audits [\[139,](#page-19-10) [158\]](#page-20-14) as well as more holistic model evaluations that reflect different aspects of model performance [\[26,](#page-15-21) [67\]](#page-17-19). However, critiques of the approaches described above also include their non-inclusivity of marginalized communities [\[48\]](#page-16-13) as well as a lack of formalized approaches and standards for model evaluation, making apples-to-apples comparisons between models difficult [\[40](#page-16-14)]. Also, as many of the most widely deployed AI systems are currently proprietary and direct access to models is not possible, it is hard to exhaustively evaluate many popular commercial models for any meaningful evaluation to take place [\[118,](#page-18-16) [190](#page-21-17)].

<sup>3</sup>A biome is a bio-geographical unit that corresponds to a community of plants and animals that share a physical environment and a climate.

<span id="page-6-2"></span><span id="page-6-1"></span><sup>4</sup>The quote is often attributed to Peter Drucker, although its exact origins are unclear.

Similarly, evaluating the environmental impacts of AI systems is far from straightforward, and we are still missing many pieces of the puzzle needed in order to meaningfully estimate these impacts. For instance, most of the carbon footprint assessments only focus on the training stage of AI models, which is easier to quantify and report [\[153,](#page-20-4) [192](#page-21-0)], but which only represents a portion of models' total environmental impacts. In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model's overall emissions [\[121\]](#page-19-1), meaning that similar studies that only took training into account were potentially underestimating their emissions by half. Also, while commendable in terms of its granularity, this kind of carbon accounting fails to recognize the societal and economic aspects of sustainability, such as the contribution of LLMs such as BLOOM towards amplifying the existing inequalities in the field of AI due to the increased amount of computing resources that they require, which are unattainable to many members of the AI community, as well as the propagation of biases via their usage. Furthermore, the authors themselves note that there is currently no information available about the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the overall carbon footprint this represents. This highlights that the emphasis on environmental sustainability often fails to account for other aspects of AI's global impacts – and any kind of evaluation hinges upon transparency, which is sorely lacking in the field of AI – we discuss this in more length in the following section.

#### <span id="page-7-0"></span>4.3 Transparency

Transparency is widely recognized as a fundamental principle in science in general and AI in particular [\[63,](#page-17-20) [107,](#page-18-17) [208,](#page-22-7) [212\]](#page-22-8) but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based systems are not inherently transparent or interpretable, given the complexity of the neural network architectures they espouse and the number of parameters they contain [\[105,](#page-18-18) [113](#page-18-19), [140\]](#page-19-11). Efforts such as interpretability approaches are useful and can help interpret the predictions of models posthoc [\[104,](#page-18-20) [164](#page-20-15)], whereas artifacts such as data sheets and model cards [\[69,](#page-17-21) [137\]](#page-19-12) can contribute towards making AI systems more understandable for users, providing essential information about AI models in a user-friendly format. These artifacts allow users to understand not just how an AI system functions, but also its limitations, potential biases, implications, and environmental impacts. However, even though model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face) and provide important information about models, they are not sufficient to guarantee, for instance, the reproducibility of reported results, which is a core tenet of scientific practice [\[146](#page-19-13)].

Indeed, several studies of transparency found that an overwhelming amount of results published at technical AI conferences do not document all of the variables necessary to reproduce the results they report [\[78](#page-17-22), [156\]](#page-20-16). This situation highlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility of AI models and their findings. Such an approach would acknowledge the connection between transparency and reproducibility: transparent research practices enable reproducibility, which in turn facilitates independent scrutiny, validation, further development of research findings by other scientists [\[80\]](#page-17-23). The absence of transparency, especially in sharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing AI models' scientific impact and impeding their adoption within the wider scientific community [\[80](#page-17-23)].

In terms of sustainability, the AI community has historically been even less transparent regarding the environmental impacts of AI models and systems, with most work in this field being done post-hoc by researchers who did not do the initial model training and deployment (e.g. [\[116](#page-18-21), [192\]](#page-21-0)). The most common environmental sustainability metric for AI models, their carbon footprint, is rarely, if ever, disclosed. While model cards of recent models such as BLOOM [\[214\]](#page-22-9) and Stable Diffusion [\[169](#page-20-17)] have included carbon footprint information, it remains far from common information communicated by model creators – recent work has found that the overwhelming majority of models shared publicly do not include this information [\[34](#page-16-15)]. In fact, most carbon footprint analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [\[2023\]](#page-18-21). In fact, until recent years, the general emphasis in the AI community was put on the efficiency and 'greenness' of AI as opposed to its environmental costs, which are now slowly starting to gain traction as an important consideration for AI systems [\[86,](#page-17-24) [141](#page-19-14)]. In fact, given the increasing size and computational requirements of models being productionized in recent years (especially LLMs), training them is only accessible to a small fraction of the AI community, which means that the organizations who have the necessary resources for this have a disproportionate influence on the field as a whole— we discuss this in the following section.

# <span id="page-8-0"></span>4.4 Power and Equity

Modern AI research and practice are not equitable by design: their cost in terms of computer hardware as well as human skills means that only a small percentage of both academic and industrial organizations can contribute to many aspects of model development. With the recent advent of AI models of ever-increasing scale and complexity, the digital divide in AI is only increasing, as it takes more compute and human skill to train and deploy AI models and systems [\[1,](#page-15-1) [19,](#page-15-22) [35,](#page-16-16) [118\]](#page-18-16). This means that the future wide-sweeping benefits that AI technologies are promised to have for humanity as a whole [\[149\]](#page-19-15) are contingent upon access to technologies that are fundamentally unequally distributed. Despite explicit proposals to pursue more equitable and explicitly de-colonialist approaches [\[124,](#page-19-16) [138\]](#page-19-17), the 'bigger-is-better' paradigm continues to be central to AI research and practice [\[204](#page-21-18)]. In a similar fashion, the places where AI research is being carried out are also skewed towards institutions from a handful of countries mostly located in the Global North [\[2\]](#page-15-2), which inexorably impacts the choices made during the AI development and deployment process, introducing many biases (which we have already addressed in previous sections). In fact, recent work has proposed that the very pursuit of sustainable AI has the opposite effect, contributing towards maintaining the status quo and "securing the dominant socio-economic interests of neo-liberal capitalism" [\[180](#page-20-18)]. For instance, major technological corporations have dedicated significant resources towards solutions such as improving the efficiency of their data centers, proposing numerous initiatives towards technological sustainability [\[4,](#page-15-23) [75](#page-17-25)], including research at the nexus of AI and the climate [\[44](#page-16-17), [128\]](#page-19-18). However, both Microsoft and Google announced that they would miss their 2024 sustainability targets, due in part to the energy demands of the AI tools that they have been developing and deploying [\[130](#page-19-19), [161\]](#page-20-19). The impacts of these computation-intensive data centers can further be expanded to include the mining of rare metals and the disposal of e-waste, both of which predominantly affect countries from the Global South which profiting technology companies from the Global North [\[86,](#page-17-24) [194\]](#page-21-19).

Similarly to AI, issues of power, equity and justice are also central in the field of sustainability, since climate change is an inherently inequitable phenomenon – with a handful of countries and regions in North America, East Asia and Europe responsible for a disproportionate portion of global emissions, while the impacts of sea level rise and extreme weather events being felt most strongly in countries with very minimal carbon footprints, raising questions of equity and justice and how to address them [\[49,](#page-16-18) [126,](#page-19-20) [151](#page-19-21), [177\]](#page-20-20). Similarly, the majority of climate-focused AI solutions overlook issues of justice and power, focusing predominantly on the climate-positive aspects of technologies and not who

stands to benefit from them, or whether these issues can be solved with technology in the first place [\[30\]](#page-15-24).For instance, when AI systems that carry out precision agriculture are developed, the emphasis is made on efficiency or increased crop yields [\[185\]](#page-21-20), and not the fact that these systems are liable to replace already underprivileged communities such as migrant workers that traditionally harvested crops by hand, or the disparate impacts of proposed solutions on different communities [\[55,](#page-16-19) [207,](#page-21-21) [218\]](#page-22-10). In addition, AI technologies can be seen as further exacerbating the existing inequalities in terms of the distribution of power and profits [\[83](#page-17-26)] and perpetuating the existing extractionist approaches in terms of labor by exploiting already under-paid and marginalized workers for data collection and labeling tasks [\[165\]](#page-20-21) as well as creating a new class of precarious crowd-sourced workers [\[211](#page-22-11)], which are often located in the Global South or in already marginalized communities, where the impacts of climate change have reduced the viability of traditional professions such as farming [\[81,](#page-17-27) [163](#page-20-22)]. This is an additional example for why we advocate for establishing more concerted efforts for integrating AI ethics and sustainability – we describe these in the following section.

# <span id="page-9-0"></span>5 ESTABLISHING BEST PRACTICES FOR AI ETHICS AND SUSTAINABILITY

Having established a multitude of transversal topics that span AI ethics and sustainability, we now focus on proposing best practices that would integrate the two in AI research and practice. We draw upon existing work to show how we can build upon it both within the AI community and in tandem with members of other communities ranging from sustainable development to policy-making and engineering.

|                  | Principles and Frameworks                                                                                          | Research and Practice                                                                                                  | Governance and Regulation                                                                                                               |
|------------------|--------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Generalizability | Shifting from the dominant<br>Western moral philosophies<br>to include perspectives from<br>non-Western traditions | Studying how well AI models<br>generalize to different populations<br>of human and non-human living<br>beings          | Espousing bottom-up governance<br>approaches based on the cultural,<br>societal and geographical constraints<br>of AI system deployment |
| Evaluation       | Developing frameworks that<br>accommodate both ethical and<br>environmental responsibility                         | Carrying out more holistic<br>evaluation of models and systems,<br>spanning both ethical and<br>environmental criteria | Integrating both social and<br>environmental assessments<br>into existing and in-progress<br>AI regulation                              |
| Transparency     | Broadening the scope of<br>transparency to include its<br>social and environmental aspects                         | Communicate the costs<br>and impacts of AI systems<br>on both the environment<br>and society                           | Requiring deployed AI systems<br>to carry out audits and report<br>a minimum of metrics spanning<br>both bias and ethics                |
| Power            | Developing principles and<br>frameworks that address<br>both human and ecological needs                            | Make equity-informed<br>trade-offs when developing<br>and deploying AI                                                 | Involving multiple stakeholders,<br>especially ones from the concerned<br>communities and areas, in the<br>governance process           |

Table 1. A summary of the proposed best practices for different axes that we cover in our article

#### 5.1 Principles

In the context of AI, there are multiple facets of AI technologies that have to be taken into consideration, given the intersection between AI and the broader societal context in which it operates. Importantly, integrating broader sustainability into AI guidelines ensures that justice and fairness are not just about social dimensions but also include respecting and protecting the environment – i.e. expanding the definition of sustainable AI to include the social and economic pillars proposed by Barbier [\[12](#page-15-4)]. This integration also acknowledges that true justice in AI cannot be achieved without considering its environmental and societal implications, which helps build the bridge between complementary approaches in both AI ethics and (environmental) sustainability research.

Generalizability. Given the observed disconnect between ethics and sustainability in the context of AI principles and frameworks, we find that, while these offer a valuable starting point, they often fall short in addressing AI's complex ethical issues due to their lack of contextual sensitivity [\[142\]](#page-19-22). We also believe that improving upon this necessitates a more nuanced and context-specific approach to AI ethics, one that embraces the varied ethical dimensions presented by AI, including its environmental implications. Current ethical charters in AI, often detached from this perplexity, represent preliminary thoughts on the ethical landscape but lack the depth required for many practical applications [\[5\]](#page-15-25). By recognizing these limitations, we intend not to discard these definitions and principles but to improve upon them. In this context, environmental and sustainability challenges related to AI development and deployment are integral to the broader ethical reflections within the field. This integration between ethics and sustainability in AI calls for a holistic approach, where ethical considerations are not viewed in isolation but are intrinsically linked with environmental and sustainability oversight. For instance, shifting from the dominant Western moral philosophies to include perspectives from non-Western traditions such as relational ethics [\[131\]](#page-19-23), Ubuntu ethics [\[145](#page-19-24)], and Confucian ethics [\[111\]](#page-18-22) can offer valuable insights into community, social harmony, and the interconnectedness of beings, emphasizing the impact of AI on society and interpersonal relationships.

Evaluation. There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable. Recent work has begun bridging the gap; for instance, work by Lynch et al. is inspired by the concept of urgent governance in environmental studies, which distinguishes system reliability and societal harm and advocates for the consideration of both when auditing infrastructure and technologies [\[123](#page-19-25)]. Raji et al. use a similar approach for their proposed end-to-end framework for internal algorithmic auditing of AI models, which takes into account both technical and ethical assessments [\[159](#page-20-13)]. In a similar vein, a recent model evaluation framework by Rakova et al., proposes an environmental justice-oriented lens to carry out algorithmic audits [\[160\]](#page-20-23), that of Genovesi and Mönig places sustainability at the center of Ethical AI certification [\[70\]](#page-17-13), while that of Metcalf et al. uses environmental impact assessments as an example of a formal mechanism that can be used to inspire the assessment of AI technologies [\[129\]](#page-19-26). All of these approaches acknowledge that ethical decisions in AI have environmental consequences and vice versa, thus necessitating a framing that accommodates both ethical and environmental responsibility.

Transparency. When viewed as a means to foster greater accountability, transparency takes on a central role: it becomes a principle that enhances ethical compliance and promotes environmental responsibility, contributing to sustainability. For instance, integrating social transparency and sustainability can be exemplified by an AI system designed for urban planning, such as an AI tool developed to optimize city layouts for efficiency. In this example, to include sustainability approaches, developers would also provide information on the environmental footprint of running the AI system, such as energy consumption during data processing and potential environmental benefits of the proposed urban layouts, like reduced carbon emissions from optimized traffic flows or green spaces. In this way, by deepening the concept of transparency to include social and environmental aspects, we would move towards creating AI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts they have and making more informed decisions based on the information at our disposal [\[51\]](#page-16-20).

Equity and Power. Equity is about ensuring fair access and participation in the benefits and governance of technology across different communities, especially those historically marginalized. Philosophical perspectives on equity, drawing

from theories of distributive justice [\[106,](#page-18-23) [135](#page-19-27)], emphasize the necessity of equitable distribution of resources and responsibilities and risks associated with AI technologies. This principle is especially relevant in environmental justice, where the disproportionate impact of environmental harms on specific populations demands a reevaluation of AI technologies are deployed at scale – for instance, Schlosberg's theory of recognitional justice highlights the importance of recognizing and respecting diverse community needs and values in environmental policies [\[176\]](#page-20-24). Additionally, equity requires that AI development actively includes diverse voices in its creation and implementation phases, ensuring that AI systems do not perpetuate existing disparities but rather contribute to rectifying them. This approach draws upon environmental justice literature for developing principles and frameworks that addresses both human and ecological needs, thus framing equity as a matter of distribution, procedural and interactional fairness [\[32](#page-16-21)].

### 5.2 Research

Despite a lack of common terminology, similar issues arise both in terms of considerations of AI ethics and sustainability and considering the inter-connectedness of the two when designing and deploying AI systems is paramount given their socio-technicality and the consequences this can have on the human and non-human species residing in these regions.

Generalizability. At the nexus of AI ethics and sustainability, existing scholarship has already established that the most disadvantaged and marginalized members of our societies tend to be the least well-represented in the 'Big Data' used in many AI models and systems [\[33](#page-16-1), [43](#page-16-22), [68](#page-17-16), [186](#page-21-22)]; the same applies at the level of countries and regions, with 'global' datasets reflecting things like wealth and economic development only being tested in a select few countries [\[24,](#page-15-26) [84](#page-17-28)] and the most extensive biodiversity datasets consisting of data collected in a subset of regions from the Global North, as well as regions close to cities and roads [\[15](#page-15-27), [196\]](#page-21-23). Studying the limits of application of AI systems both in terms of ethics and sustainability and how well they generalize to different populations of human and non-human subjects is important to question existing assumptions. For instance, studies of emblematic datasets such as ImageNet found that it to misrepresent both humans [\[2021\]](#page-16-23) and other living beings such as insects and fish [\[2023\]](#page-18-24). Developing new datasets that are more representative of diverse populations and contexts - such as the Dollar Street dataset [\[167\]](#page-20-25) and CropHarvest [\[197\]](#page-21-16) - and using this dataset in research and practice and help improve the applicability of AI systems and their ability to represent more diverse populations from both a societal and environmental perspective.

Evaluation. As AI is increasingly used in the fight against climate change, holistic evaluation of models and systems becomes ever more relevant. For instance, in the high-stakes domain of solar geoengineering, which aims to develop new ways for modifying the Earth's climate to reduce the global warming effect (i.e. by enhancing the reflexivity of clouds so that they reflect more of the sun's rays), AI is often used to help model the potential far-reaching effects of even minor interventions and understand how they will impact local and global climate patterns [\[62,](#page-17-29) [148](#page-19-28), [179\]](#page-20-26). This is because regional changes to the climate may trigger increased fragility of some regions and not others, which is hard to quantify and therefore, to be optimized for in AI models that aim to predict the consequences of climate interventions [\[83](#page-17-26)]. For instance, different thresholds of solar reflexivity are optimal for different regions, and optimizing results based on a given region (e.g. the continental United States) would make things worse for others (e.g. Western Africa), which can suffer droughts and other forms of damage [\[126\]](#page-19-20). Considerations around the wider rebound effects

of proposed AI solutions are also important: for instance, in the case of AI systems that improve aircraft efficiency [\[108\]](#page-18-25), more efficient aircraft can result in cheaper airfare and therefore, more travel overall[5](#page-12-0) .

Transparency. Falk and van Wynsberghe argue that transparency is a crucial factor for establishing "whether the net environmental impact of AI for Sustainability is positive or how the positive impact on the respective sustainability goal can outweigh the very different negative impact of the models' development on sustainability" [\[61\]](#page-17-4) – we would expand this to include AI systems in general, which should use a variety of mechanisms to communicate the costs and potential impacts of their systems on both the environment and society. In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass "social transparency", which involves integrating socio-technical aspects in the description and understanding of AI systems [\[56\]](#page-16-24). Social transparency involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint. By doing so, it provides a more complete picture, making the AI system transparent but also understandable in a broader societal context. This augmented view of transparency, which would integrate both social and environmental dimensions, resonates with the increasing awareness that AI is not simply a technological tool but a socio-technical system with extensive repercussions, spanning both people and the environment [\[202](#page-21-24)].

Equity. Recognizing that Green AI (or sustainable AI), i.e. the development and deployment of AI systems that puts the emphasis solely on efficiency or the reduction of greenhouse gas emissions, is not necessarily inherently more equitable or just – for instance, if the more efficient models are not widely shared, or entail an increased usage of compute due to their efficiency – is an important first step towards improving the current direction towards bigger models. Recent research in both AI ethics and sustainability has shed light on the extent to which AI systems enable the amplification of existing social inequities [\[17,](#page-15-28) [58](#page-16-25), [115\]](#page-18-26) as well as contributing to the preservation of the ecological status quo vis-a-vis to climate change [\[194\]](#page-21-19). Given these findings, it is important to make equity-informed trade-offs when developing and deploying AI – for instance by carrying out energy prediction in resource-constrained energy grids, which are common in low- and middle-income countries [\[13\]](#page-15-29), or by using AI to predict the impacts of changes in climate on societal aspects such as disease propagation and health [\[100\]](#page-18-27).

# 5.3 Governance

As the field of AI ethics increasingly intersects with regulation, including law and policy, it showcases its interdisciplinary nature, meaning that in order to be successful, governance initiatives must incorporate efforts from different domains, depending on the context of the application.

Generalizability. While there is no single solution to complex questions involving governance over AI systems, various bottom-up governance approaches have been proposed based on the cultural, societal and geographical constraints of AI system deployment. Some of these follow the tenets of the Maori culture, which is based on principles ¯ that consider both impacts on nature as well as on fellow human beings, bridging the gap between ethics and sustainability [\[82](#page-17-30), [143\]](#page-19-29); others espouse those established by the Indigenous AI community, which is based on both values and practices of social and environmental sustainability, both core to many Indigenous epistemologies [\[110\]](#page-18-28). Also, regulating the deployment of out-of-the-box AI solutions that operate on the premise of generalizability without taking

<span id="page-12-0"></span><sup>5</sup>This is often referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use [\[93](#page-18-29)].

context into account can help ensure that systems that are meant to be widely applicable are truly representative of the context of application – where evaluations, as explained below, will play a crucial role.

Evaluation. As noted by Metcalf et al. [\[2021](#page-19-26)], there is a parallel between environmental impact assessments and AI ethics audits, which can be extended beyond ethical compliance to include assessments of environmental impacts, such as energy consumption and carbon emissions. Requiring audits of commercial AI systems before their deployment in practice, both in contexts such as education and healthcare that come with high stakes in terms of societal impacts, but also in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental impacts, will require the development of new governance approaches. For instance, attempting to evaluate the wider rebound effects of AI tools and their impacts on consumption and human behavior is important to represent their broader impacts on both society and the environment [\[86,](#page-17-24) [96](#page-18-6), [194\]](#page-21-19). Finally, integrating both social and environmental assessments into existing and in-progress regulation and developing new approaches to evaluate these impacts can help ensure that the deployment of AI systems is carried out in a way that is ethically sound and sustainable across multiple dimensions.

Transparency. Recent years have seen less transparency in AI research and practice, especially in terms of generative AI models [\[189](#page-21-25)]. However, as these systems are increasingly being deployed in society, having more information regarding how these systems were created and deployed remains paramount. Ensuring that enough details are provided both regarding the energy consumed and greenhouse gasses emitted during model training and deployment can help track how the environmental impacts of AI are evolving over time. Mandating transparency for already deployed AI systems can help establish audits, red teaming efforts and AI energy score ratings to raise users' awareness around the impacts of the systems they use [\[29\]](#page-15-30), contributing to what can be termed "usable transparency" [\[144](#page-19-30)]. By ensuring that the processes and results of these practices are well-documented and publicly accessible, stakeholders would be better equipped to understand and evaluate the ethical and sustainable aspects of AI systems. Such policies would promote a culture of openness in the AI industry, encouraging developers to prioritize ethical considerations and sustainability alongside technical advancements.

Equity. Involving multiple stakeholders, especially ones from the concerned communities and domains, in this process is important to ensure that different perspectives and lived experiences are reflected in the development and deployment of AI systems [\[170](#page-20-27), [171](#page-20-28)] as well as the different communities and can influence existing and future practices [\[45](#page-16-26), [126\]](#page-19-20). From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [\[133\]](#page-19-31), whereas the White House Office of Management and Budget (OMB)'s first government-wide policy around the usage of AI includes, inter alia, clauses that stipulate that government agencies should take both environmental impacts and bias and fairness into account when procuring AI-enabled services [\[59\]](#page-16-27) - exhibiting thought leadership that will hopefully have wider repercussions.

# 6 CONCLUSION

We recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies like AI, it can be difficult to define what progress looks like and how it can be achieved. We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) – but by adopting a multitude of endeavors such as the ones described in the paragraphs above can help involve different actors and hopefully build momentum across the AI community. The beauty of making transversal connections that go above and beyond the silos in which many AI technologists tend to operate is that we can also be inspired by the multitude of rich and relevant work that has been done in other domains – from ecology to philosophy, as well as governance and climate science, to propose ways forward that would allow the AI community to improve systems from the perspective of both ethics and sustainability. Looking forward, the field of AI ethics is rapidly evolving, with insights racing to keep up with the rapid pace of technological advancements in AI. This dynamic landscape presents an ongoing challenge: to develop AI in a way that is inclusive, just, and cognizant of its environmental and societal impacts. Furthermore, it is becoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences. Therefore, integrating AI ethics with sustainability is not just beneficial but necessary, ensuring that AI advancements are not only technologically innovative and ethically sound but also maximizing their potential to engender sustainable advancement.

#### REFERENCES

- <span id="page-15-1"></span>[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.
- <span id="page-15-2"></span>[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névéol, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. The elephant in the room: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).
- <span id="page-15-18"></span>[3] AI Safety Summit. 2023. The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023. (2023). [https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)
- <span id="page-15-23"></span>[4] Amazon Web Services. 2021. Sustainability in the Cloud. [https://sustainability.aboutamazon.com/environment/the-cloud.](https://sustainability.aboutamazon.com/environment/the-cloud)
- <span id="page-15-25"></span>[5] Daniel Andler. 2023. Intelligence artificielle, intelligence humaine: le double énigme. Gallimard, Paris.
- <span id="page-15-0"></span>[6] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. Machine bias. In Ethics of data and analytics. Auerbach Publications, 254–264.
- <span id="page-15-8"></span>[7] Thomas Aquinas. 1702. Summa theologica. J. Mentelin.
- <span id="page-15-5"></span>[8] Aristotle. 350. Nicomachean Ethics. Original work published in 350 B.C.E..
- <span id="page-15-16"></span>[9] Hutan Ashrafian. 2015. Intelligent robots must uphold human rights. Nature 519 (2015), 391.<https://doi.org/10.1038/519391a>
- <span id="page-15-10"></span>[10] Carolyn Ashurst, Solon Barocas, Rosie Campbell, and Deborah Raji. 2022. Disentangling the Components of Ethical Research in Machine Learning. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 2057–2068.<https://doi.org/10.1145/3531146.3533781>
- <span id="page-15-20"></span>[11] Ricardo Baeza-Yates and Zeinab Liaghat. 2017. Quality-efficiency trade-offs in machine learning for text processing. In 2017 IEEE international conference on big data (big data). IEEE, 897–904.
- <span id="page-15-4"></span>[12] Edward B Barbier. 1987. The concept of sustainable economic development. Environmental conservation 14, 2 (1987), 101–110.
- <span id="page-15-29"></span>[13] Mohini Bariya, Genevieve Flaspohler, Ngoran Clare-Joyce, and Margaret Odero. 2023. Topology Estimation from Voltage Edge Sensing for Resource-Constrained Grids. Tackling Climate Change with Machine Learning Workshop - ICLR 2023 (2023).
- <span id="page-15-11"></span>[14] Vita Santa Barletta, Danilo Caivano, Domenico Gigante, and Azzurra Ragone. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering. 358–367.
- <span id="page-15-27"></span>[15] Jan Beck, Marianne Böller, Andreas Erhardt, and Wolfgang Schwanghart. 2014. Spatial bias in the GBIF database and its effect on modeling species' geographic distributions. Ecological Informatics 19 (2014), 10–15.
- <span id="page-15-3"></span>[16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 610–623.<https://doi.org/10.1145/3442188.3445922>
- <span id="page-15-28"></span>[17] Ruha Benjamin. 2023. Race after technology. In Social Theory Re-Wired. Routledge, 405–415.
- <span id="page-15-7"></span>[18] Jeremy Bentham. 1789. An Introduction to the Principles of Morals and Legislation. T. Payne and Son.
- <span id="page-15-22"></span>[19] Tamay Besiroglu, Sage Andrus Bergerson, Amelia Michael, Lennart Heim, Xueyun Luo, and Neil Thompson. 2024. The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny? arXiv preprint arXiv:2401.02452 (2024).
- <span id="page-15-17"></span>[20] Joseph R Biden. 2023. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence. (2023).
- <span id="page-15-6"></span>[21] Elettra Bietti. 2020. From Ethics Washing to Ethics Bashing: A View on Tech Ethics from within Moral Philosophy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT\* '20). Association for Computing Machinery, New York, NY, USA, 210–219.<https://doi.org/10.1145/3351095.3372860>
- <span id="page-15-14"></span>[22] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2022. The Values Encoded in Machine Learning Research. arXiv[:2106.15590](https://arxiv.org/abs/2106.15590) [cs.LG]
- <span id="page-15-9"></span>[23] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S. Brown, Johnathan Flowers, Anthony Ventresque, and Christopher L. Dancy. 2022. The forgotten margins of AI ethics. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 948–958.
- <span id="page-15-26"></span>[24] Joshua Blumenstock. 2018. Don't forget people in the use of big data for development.
- <span id="page-15-13"></span>[25] Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van Wynsberghe. 2022. From an Ethics of Carefulness to an Ethics of Desirability: Going Beyond Current Ethics Approaches to Sustainable AI. Sustainability 14, 8 (2022).<https://doi.org/10.3390/su14084472>
- <span id="page-15-21"></span>[26] Rishi Bommasani, Percy Liang, and Tony Lee. 2023. Holistic evaluation of language models. Annals of the New York Academy of Sciences 1525, 1 (2023), 140–146.
- <span id="page-15-15"></span>[27] Mioara Borza. 2014. The connection between efficiency and sustainability–a theoretical approach. Procedia Economics and Finance 15 (2014), 1355–1363.
- <span id="page-15-12"></span>[28] Andrew Brennan and Norva Y. S. Lo. 2022. Environmental Ethics. In The Stanford Encyclopedia of Philosophy (Summer 2022 ed.), Edward N. Zalta (Ed.). Metaphysics Research Lab, Stanford University.
- <span id="page-15-30"></span>[29] Benedetta Brevini. 2020. Black boxes, not green: Mythologizing artificial intelligence and omitting the environment. Big Data & Society 7, 2 (2020), 2053951720935141.<https://doi.org/10.1177/2053951720935141>
- <span id="page-15-24"></span>[30] Benedetta Brevini. 2023. Myths, techno solutionism and artificial intelligence: reclaiming AI materiality and its massive environmental costs. In Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing, 869–877.
- <span id="page-15-19"></span>[31] Alexander EI Brownlee, Jason Adair, Saemundur O Haraldsson, and John Jabbo. 2021. Exploring the accuracy–energy trade-off in machine learning. In 2021 IEEE/ACM International Workshop on Genetic Improvement (GI). IEEE, 11–18.

- <span id="page-16-21"></span>[32] Robert D. Bullard, Glenn S. Johnson, and Beverly H. Wright. 1997. Confronting Environmental Injustice: It's the Right Thing to Do. Race, Gender & Class 5, 1 (1997), 63–79.
- <span id="page-16-1"></span>[33] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency. PMLR, 77–91.
- <span id="page-16-15"></span>[34] Joel Castaño, Silverio Martínez-Fernández, Xavier Franch, and Justus Bogner. 2023. Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study. arXiv preprint arXiv:2305.11164 (2023).
- <span id="page-16-16"></span>[35] Alan Chan, Chinasa T Okolo, Zachary Terner, and Angelina Wang. 2021. The limits of global inclusion in AI development. arXiv preprint arXiv:2102.01265 (2021).
- <span id="page-16-8"></span>[36] Kyla Chasalow and Karen Levy. 2021. Representativeness in statistics, politics, and machine learning. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 77–89.
- <span id="page-16-12"></span>[37] Rachel Chason and Rael Ombuor. 2021. A lack of weather data in Africa is thwarting critical climate research. <https://www.washingtonpost.com/world/2021/09/24/africa-climate-weather-data/>
- <span id="page-16-5"></span>[38] Chinese Data Law Alliance. 2023. Chinese Artificial Intelligence Law, v. 1.0.<https://mp.weixin.qq.com/s/85D8TjMkN9Tl-oWjq15JiQ>
- <span id="page-16-4"></span>[39] Concil of EU. 2022. Artificial Intelligence Act: Council calls for promoting safe AI that respects fundamental rights. <https://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artificial-intelligence-act-council-calls-for-promoting-safe-ai-that-respects-fundamental-rights/>
- <span id="page-16-14"></span>[40] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. 2022. Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1571–1583.
- <span id="page-16-2"></span>[41] Josh Cowls, Andreas Tsamados, MariarosariaTaddeo, and Luciano Floridi. 2023. The AI gambit: leveraging artificial intelligence to combat climate change—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.
- <span id="page-16-23"></span>[42] Kate Crawford and Trevor Paglen. 2021. Excavating AI: The politics of images in machine learning training sets. Ai & Society 36, 4 (2021), 1105–1116.
- <span id="page-16-22"></span>[43] Rowena Cullen. 2001. Addressing the digital divide. Online information review 25, 5 (2001), 311–320.
- <span id="page-16-17"></span>[44] Amane Dannouni, Stefan A. Deutscher, Ghita Dezzaz, Adam Elman, Antonia Gawel, Marsden Hanna, Andrew Hyland, Amjad Kharij, Hamid Maher, David Patterson, Edmond Rhys Jones, Juliet Rothenberg, Hamza Tber, Maud Texier, and Ali Ziat. 2023. Accelerating Climate Action with AI.<https://www.gstatic.com/gumdrop/sustainability/accelerating-climate-action-ai.pdf>
- <span id="page-16-26"></span>[45] Rozita Dara, Seyed Mehdi Hazrati Fard, and Jasmin Kaur. 2022. Recommendations for ethical and responsible use of artificial intelligence in digital agriculture. Frontiers in Artificial Intelligence 5 (2022), 884192.
- <span id="page-16-10"></span>[46] Hannah Davis. 2020. A Dataset is a Worldview. Towards Data Science (2020).
- <span id="page-16-6"></span>[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. IEEE, 248–255.
- <span id="page-16-13"></span>[48] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Soldaini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Ghosh, Kyra Yee, Irene Font Peradejordi, et al. 2023. Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society. 375–386.
- <span id="page-16-18"></span>[49] Fatma Denton. 2002. Climate change vulnerability, impacts, and adaptation: Why does gender matter? Gender & Development 10, 2 (2002), 10–20.
- <span id="page-16-7"></span>[50] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
- <span id="page-16-20"></span>[51] Nicholas Diakopoulos. 2020. Accountability, transparency, and algorithms. The Oxford handbook of ethics of AI 17, 4 (2020), 197.
- <span id="page-16-0"></span>[52] Roel Dobbe and Meredith Whittaker. 2019. AI and climate change: how they're connected, and what we can do about it. AI Now Institute 17 (2019).
- <span id="page-16-3"></span>[53] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.
- <span id="page-16-11"></span>[54] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758 (2021).
- <span id="page-16-19"></span>[55] Anuoluwapo Abosede Durokifa and Edwin Chikata Ijeoma. 2018. Neo-colonialism and Millennium Development Goals (MDGs) in Africa: A blend of an old wine in a new bottle. African Journal of Science, Technology, Innovation and Development 10, 3 (2018), 355–366.
- <span id="page-16-24"></span>[56] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towards Social Transparency in AI Systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 82, 19 pages.<https://doi.org/10.1145/3411764.3445188>
- <span id="page-16-9"></span>[57] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 (2023).
- <span id="page-16-25"></span>[58] Virginia Eubanks. 2018. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin's Press.
- <span id="page-16-27"></span>[59] Executive Office of the President, Office of Management and Budget. 2024. Memorandum for the heads of executive departments and agencies on Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence. <https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf>

- <span id="page-17-6"></span>[60] Florian Eyert and Paola Lopez. 2023. Rethinking Transparency as a Communicative Constellation. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT '23). Association for Computing Machinery, New York, NY, USA, 444–454. <https://doi.org/10.1145/3593013.3594010>
- <span id="page-17-4"></span>[61] Sophia Falk and Aimee van Wynsberghe. 2023. Challenging AI for Sustainability: what ought it mean? AI and Ethics (2023), 1–11.
- <span id="page-17-29"></span>[62] Alec Feinberg. 2022. Solar Geoengineering Modeling and Applications for Mitigating Global Warming: Assessing Key Parameters and the Urban Heat Island Influence. Frontiers in Climate 4 (2022), 870071.
- <span id="page-17-20"></span>[63] H. Felzmann, E. Fosch-Villaronga, C. Lutz, et al. 2020. Towards Transparency by Design for Artificial Intelligence. Science and Engineering Ethics 26 (2020), 3333–3361.<https://doi.org/10.1007/s11948-020-00276-4>
- <span id="page-17-0"></span>[64] Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, et al. 2018. AI4People—an ethical framework for a good AI society: opportunities, risks, principles, and recommendations. Minds and machines 28 (2018), 689–707.
- <span id="page-17-7"></span>[65] Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2021. The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making. Commun. ACM 64, 4 (2021), 136–143.
- <span id="page-17-18"></span>[66] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).
- <span id="page-17-19"></span>[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. <https://doi.org/10.5281/zenodo.10256836>
- <span id="page-17-16"></span>[68] Timnit Gebru. 2019. Oxford handbook on AI ethics book chapter on race and gender. arXiv preprint arXiv:1908.06165 (2019).
- <span id="page-17-21"></span>[69] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.
- <span id="page-17-13"></span>[70] Sergio Genovesi and Julia Maria Mönig. 2022. Acknowledging Sustainability in the Framework of Ethical Certification for AI. Sustainability 14, 7 (2022).<https://doi.org/10.3390/su14074157>
- <span id="page-17-1"></span>[71] Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, Irene Y Chen, and Rajesh Ranganath. 2020. A review of challenges and opportunities in machine learning for health. AMIA Summits on Translational Science Proceedings 2020 (2020), 191.
- <span id="page-17-9"></span>[72] Amandeep S Gill and Stefan Germann. 2022. Conceptual and normative approaches to AI governance for a global digital ecosystem supportive of the UN Sustainable Development Goals (SDGs). AI and Ethics 2, 2 (2022), 293–301.
- <span id="page-17-15"></span>[73] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862 (2019).
- <span id="page-17-14"></span>[74] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT press.
- <span id="page-17-25"></span>[75] Google. 2022. Carbon free energy for Google Cloud regions. [https://cloud.google.com/sustainability/region-carbon.](https://cloud.google.com/sustainability/region-carbon)
- <span id="page-17-10"></span>[76] Green Software Foundation. 2023. Can AI be Truly Green? <https://greensoftware.foundation/articles/can-ai-truly-be-green>
- <span id="page-17-11"></span>[77] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil Profit from Climate Destructions. <https://www.greenpeace.org/usa/reports/oil-in-the-cloud/>
- <span id="page-17-22"></span>[78] Odd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the art: Reproducibility in artificial intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.
- <span id="page-17-8"></span>[79] Thilo Hagendorff. 2022. A virtue-based framework to support putting AI ethics into practice. Philosophy & Technology 35, 3 (2022), 55.
- <span id="page-17-23"></span>[80] Benjamin Haibe-Kains et al. 2020. Transparency and reproducibility in artificial intelligence. Nature 586, 7829 (2020), E14–E16.
- <span id="page-17-27"></span>[81] Karen Hao. 2022. Artificial intelligence is creating a new colonial world order. MIT Technology Review (2022).
- <span id="page-17-30"></span>[82] Karen Hao. 2022. A new vision of artificial intelligence for the people. MIT Technology Review (2022).
- <span id="page-17-26"></span>[83] Jan-Christoph Heilinger, Hendrik Kempt, and Saskia Nagel. 2023. Beware of sustainable AI! Uses and abuses of a worthy goal. AI and Ethics (2023), 1–12.
- <span id="page-17-28"></span>[84] Martin Hilbert. 2016. Big data for development: A review of promises and challenges. Development Policy Review 34, 1 (2016), 135–174.
- <span id="page-17-12"></span>[85] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi, et al. 2023. International institutions for advanced AI. arXiv preprint arXiv:2307.04699 (2023).
- <span id="page-17-24"></span>[86] Mél Hogan. 2018. Big data ecologies. Ephemera 18, 3 (2018), 631.
- <span id="page-17-2"></span>[87] Wayne Holmes and Ilkka Tuomi. 2022. State of the art and practice in AI in education. European Journal of Education 57, 4 (2022), 542–570.
- <span id="page-17-17"></span>[88] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).
- <span id="page-17-5"></span>[89] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un)Fairness: Lessons for Machine Learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT\* '19). Association for Computing Machinery, New York, NY, USA, 49–58. <https://doi.org/10.1145/3287560.3287600>
- <span id="page-17-3"></span>[90] Strategic Imperatives. 1987. Report of the World Commission on Environment and Development: Our common future. Accessed Feb 10 (1987), 1–300.

- <span id="page-18-9"></span>[91] Innovation, Science and Economic Development Canada. 2023. The Artificial Intelligence and Data Act (AIDA) – Companion document. <https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document>
- <span id="page-18-8"></span>[92] Yacine Jernite et al. 2022. Data governance in the age of large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM.
- <span id="page-18-29"></span>[93] W Stanley Jevons. 1866. The coal question. In The Economics of Population. Routledge, 193–204.
- <span id="page-18-2"></span>[94] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of AI ethics guidelines. Nature machine intelligence 1, 9 (2019), 389–399.
- <span id="page-18-13"></span>[95] LN Joppa, Brian O'Connor, Piero Visconti, Cathy Smith, Jonas Geldmann, Michael Hoffmann, James EM Watson, Stuart HM Butchart, Malika Virah-Sawmy, Benjamin S Halpern, et al. 2016. Filling in biodiversity threat gaps. Science 352, 6284 (2016), 416–418.
- <span id="page-18-6"></span>[96] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527.
- <span id="page-18-15"></span>[97] Eva Kinnebrew, Jose I Ochoa-Brito, Matthew French, Megan Mills-Novoa, Elizabeth Shoffner, and Katherine Siegel. 2022. Biases and limitations of Global Forest Change and author-generated land cover maps in detecting deforestation in the Amazon. PLoS One 17, 7 (2022), e0268970.
- <span id="page-18-11"></span>[98] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Foster. 2021. Reduced, reused and recycled: The life of a dataset in machine learning research. arXiv preprint arXiv:2112.01716 (2021).
- <span id="page-18-14"></span>[99] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. 2021. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning. PMLR, 5637–5664.
- <span id="page-18-27"></span>[100] Julian Kuehnert, Deborah McGlynn, Sekou L. Remy, Aisha Walcott-Bryant, and Anne Jones. 2022. Surrogate Ensemble Forecasting for Dynamic Climate Impact Models. arXiv[:2204.05795](https://arxiv.org/abs/2204.05795) [cs.LG]
- <span id="page-18-7"></span>[101] Maciej Kuziemski and Gianluca Misuraca. 2020. AI governance in the public sector: Three tales from the frontiers of automated decision-making in democratic settings. Telecommunications policy 44, 6 (2020), 101976.
- <span id="page-18-1"></span>[102] Travis LaCroix and Alexandra Sasha Luccioni. 2022. Metaethical perspectives on'Benchmarking'AI ethics. arXiv preprint arXiv:2204.05151 (2022).
- <span id="page-18-12"></span>[103] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto. 2023. When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 3206–3219.<https://doi.org/10.18653/v1/2023.eacl-main.234>
- <span id="page-18-20"></span>[104] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675–1684.
- <span id="page-18-18"></span>[105] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2019. Faithful and customizable explanations of black box models. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 131–138.
- <span id="page-18-23"></span>[106] Julian Lamont (Ed.). 2017. Distributive Justice. Routledge.
- <span id="page-18-17"></span>[107] S. Larsson and F. Heintz. 2020. Transparency in artificial intelligence. Internet Policy Review 9, 2 (2020).<https://doi.org/10.14763/2020.2.1469>
- <span id="page-18-25"></span>[108] Soledad Le Clainche, Esteban Ferrer, Sam Gibson, Elisabeth Cross, Alessandro Parente, and Ricardo Vinuesa. 2023. Improving aircraft performance using machine learning: a review. Aerospace Science and Technology (2023), 108354.
- <span id="page-18-3"></span>[109] Jaana Leikas, Raija Koivisto, and Nadezhda Gotcheva. 2019. Ethical framework for designing autonomous intelligent systems. Journal of Open Innovation: Technology, Market, and Complexity 5, 1 (2019), 18.
- <span id="page-18-28"></span>[110] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Kaipulaumakaniolono Baker, Scott Benesiinaabandan, Michelle Brown, Melanie Cheung, Meredith Coleman, Ashley Cordes, Joel Davison, et al. 2020. Indigenous protocol and artificial intelligence position paper. (2020).
- <span id="page-18-22"></span>[111] Chenyang Li. 2013. The Confucian philosophy of harmony. Vol. 10. Routledge.
- <span id="page-18-4"></span>[112] Mochen Liao, Kai Lan, and Yuan Yao. 2022. Sustainability implications of artificial intelligence in the chemical industry: A conceptual framework. Journal of industrial ecology 26, 1 (2022), 164–182.
- <span id="page-18-19"></span>[113] Zachary C Lipton. 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 3 (2018), 31–57.
- <span id="page-18-10"></span>[114] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
- <span id="page-18-26"></span>[115] Kirsten Lloyd. 2018. Bias amplification in artificial intelligence systems. arXiv preprint arXiv:1809.07842 (2018).
- <span id="page-18-21"></span>[116] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023).
- <span id="page-18-0"></span>[117] Alexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2023. Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv[:2311.16863](https://arxiv.org/abs/2311.16863) [cs.LG]
- <span id="page-18-16"></span>[118] Alexandra Sasha Luccioni and Anna Rogers. 2023. Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice. arXiv preprint arXiv:2308.07120 (2023).
- <span id="page-18-24"></span>[119] Alexandra Sasha Luccioni and David Rolnick. 2023. Bugs in the data: How ImageNet misrepresents biodiversity. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 14382–14390.
- <span id="page-18-5"></span>[120] Alexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025).

- <span id="page-19-1"></span>[121] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language model. arXiv preprint arXiv:2211.02001 (2022).
- <span id="page-19-9"></span>[122] Federica Lucivero. 2020. Big data, big waste? A reflection on the environmental sustainability of big data initiatives. Science and engineering ethics 26, 2 (2020), 1009–1030.
- <span id="page-19-25"></span>[123] Amanda H Lynch and Siri Veland. 2018. Urgency in the Anthropocene. MIT Press.
- <span id="page-19-16"></span>[124] Mirca Madianou. 2021. Nonhuman humanitarianism: when'AI for good'can be harmful. Information, Communication & Society 24, 6 (2021), 850–868.
- <span id="page-19-4"></span>[125] Melissa Mccradden, Oluwadara Odusi, Shalmali Joshi, Ismail Akrout, Kagiso Ndlovu, Ben Glocker, Gabriel Maicas, Xiaoxuan Liu, Mjaye Mazwi, Tee Garnett, et al. 2023. What's fair is. . . fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 1505–1519.
- <span id="page-19-20"></span>[126] Duncan P McLaren. 2018. Whose climate and whose ethics? Conceptions of justice in solar geoengineering modelling. Energy research & social science 44 (2018), 209–221.
- <span id="page-19-0"></span>[127] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR) 54, 6 (2021), 1–35.
- <span id="page-19-18"></span>[128] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature 624, 7990 (2023), 80–85.
- <span id="page-19-26"></span>[129] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish. 2021. Algorithmic Impact Assessments and Accountability: The Co-Construction of Impacts. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 735–746.<https://doi.org/10.1145/3442188.3445935>
- <span id="page-19-19"></span>[130] Rachel Metz. 2024. Google's Emissions Shot Up 48% Over Five Years Due to AI. Bloomberg (2024).
- <span id="page-19-23"></span>[131] Thaddeus Metz and Scott C. Miller. 2016. Relational ethics. In The international encyclopedia of ethics. 1–10.
- <span id="page-19-6"></span>[132] Milagros Miceli, Julian Posada, and Tianling Yang. 2022. Studying up machine learning data: Why talk about bias when we mean power? Proceedings of the ACM on Human-Computer Interaction 6, GROUP (2022), 1–14.
- <span id="page-19-31"></span>[133] Nieminen Mika, Gotcheva Nadezhda, Leikas Jaana, and K Raija. 2019. Ethical AI for the Governance of the Society: Challenges and Opportunities. In CEUR Workshop Proceedings, Vol. 2505. 20–26.
- <span id="page-19-3"></span>[134] John Stuart Mill. 1863. Utilitarianism. Parker, Son, and Bourn.
- <span id="page-19-27"></span>[135] David Miller and Michael Walzer (Eds.). 1995. Pluralism, Justice, and Equality. Oxford University Press.
- <span id="page-19-8"></span>[136] Margaret Mitchell, Dylan Baker, Nyalleng Moorosi, Emily Denton, Ben Hutchinson, Alex Hanna, Timnit Gebru, and Jamie Morgenstern. 2020. Diversity and inclusion metrics in subset selection. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 117–123.
- <span id="page-19-12"></span>[137] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229.
- <span id="page-19-17"></span>[138] Shakir Mohamed, Marie-Therese Png, and William Isaac. 2020. Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. Philosophy & Technology 33 (2020), 659–684.
- <span id="page-19-10"></span>[139] Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. 2023. Auditing large language models: a three-layered approach. AI and Ethics (2023), 1–31.
- <span id="page-19-11"></span>[140] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.
- <span id="page-19-14"></span>[141] Steven Gonzalez Monserrate. 2022. The cloud is material: On the environmental impacts of computation and data storage. (2022).
- <span id="page-19-22"></span>[142] L. Munn. 2022. The Uselessness of AI Ethics. AI Ethics (2022).<https://doi.org/10.1007/s43681-022-00209-w>
- <span id="page-19-29"></span>[143] Luke Munn. 2023. The five tests: designing and evaluating AI according to indigenous Maori principles. ¯ AI & SOCIETY (2023), 1–9.
- <span id="page-19-30"></span>[144] Patrick Murmann and Simone Fischer-Hübner. 2017. Usable transparency enhancing tools: A literature review. (2017).
- <span id="page-19-24"></span>[145] Mechthild Nagel. 2022. Ludic ubuntu ethics: Decolonizing justice. Taylor & Francis.
- <span id="page-19-13"></span>[146] National Academies of Sciences, Engineering and Medicine. 2019. Reproducibility and replicability in science. (2019).
- <span id="page-19-2"></span>[147] Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh. 2022. Disclosure by Design: Designing Information Disclosures to Support Meaningful Transparency and Accountability. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 679–690.<https://doi.org/10.1145/3531146.3533133>
- <span id="page-19-28"></span>[148] Peer Nowack, Peter Braesicke, Joanna Haigh, Nathan Luke Abraham, John Pyle, and Apostolos Voulgarakis. 2018. Using machine learning to build temperature-based ozone parameterizations for climate sensitivity simulations. Environmental Research Letters 13, 10 (2018), 104016.
- <span id="page-19-15"></span>[149] Victor Ordonez, Taylor Dunn, and Eric Noll. 2023. OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks:'A little bit scared of this'. (2023).<https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshapesociety-acknowledges/story>
- <span id="page-19-7"></span>[150] Friederike Otto. 2023. Without Warning: A Lack of Weather Stations Is Costing African Lives. <https://e360.yale.edu/features/africa-weather-stations-climate-change>
- <span id="page-19-21"></span>[151] Edward A Page. 2008. Distributing the burdens of climate change. Environmental Politics 17, 4 (2008), 556–575.
- <span id="page-19-5"></span>[152] European Parliament. 2023. Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI. (2023). <https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai>

- <span id="page-20-4"></span>[153] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).
- <span id="page-20-6"></span>[154] Giada Pistilli, Carlos Muñoz Ferrandis, Yacine Jernite, and Margaret Mitchell. 2023. Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (<confloc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>) (FAccT '23). Association for Computing Machinery, New York, NY, USA, 343–354.<https://doi.org/10.1145/3593013.3594002>
- <span id="page-20-2"></span>[155] Erich Prem. 2023. From ethical AI frameworks to tools: a review of approaches. AI and Ethics 3, 3 (2023), 699–716.
- <span id="page-20-16"></span>[156] Edward Raff. 2019. A Step Toward Quantifying Independently Reproducible Machine Learning Research. arXiv[:1909.06674](https://arxiv.org/abs/1909.06674) [cs.LG]
- <span id="page-20-9"></span>[157] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. 2021. AI and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366 (2021).
- <span id="page-20-14"></span>[158] Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buolamwini. 2023. Change from the outside: Towards credible third-party audits of ai systems. Missing links in AI governance (2023), 5.
- <span id="page-20-13"></span>[159] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, et al. 2020. Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM, 33–44.
- <span id="page-20-23"></span>[160] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Social-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic Audits. In 2023 ACM Conference on Fairness, Accountability, and Transparency. ACM.<https://doi.org/10.1145/3593013.3594014>
- <span id="page-20-19"></span>[161] Akshat Rathi and Dina Bass. 2024. Microsoft's AI Push Imperils Climate Goal as Carbon Emissions Jump 30%. Bloomberg (2024).
- <span id="page-20-8"></span>[162] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. arXiv[:1506.02640](https://arxiv.org/abs/1506.02640) [cs.CV]
- <span id="page-20-22"></span>[163] Jennifer Rhee. 2018. The robotic imaginary: The human and the price of dehumanized labor. U of Minnesota Press.
- <span id="page-20-15"></span>[164] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386 (2016).
- <span id="page-20-21"></span>[165] Paola Ricaurte. 2022. Ethics for the majority world: AI and the question of violence at scale. Media, Culture & Society 44, 4 (2022), 726–745.
- <span id="page-20-10"></span>[166] Anna Rogers. 2021. Changing the world by changing the data. arXiv preprint arXiv:2105.13947 (2021).
- <span id="page-20-25"></span>[167] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022. The Dollar Street dataset: Images representing the geographic and socioeconomic diversity of the world. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.
- <span id="page-20-0"></span>[168] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.
- <span id="page-20-17"></span>[169] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684–10695.
- <span id="page-20-27"></span>[170] Sarah Rotz, Evan Gravely, Ian Mosby, Emily Duncan, Elizabeth Finnis, Mervyn Horgan, Joseph LeBlanc, Ralph Martin, Hannah Tait Neufeld, Andrew Nixon, et al. 2019. Automated pastures and the digital divide: How agricultural technologies are shaping labour and rural communities. Journal of Rural Studies 68 (2019), 112–122.
- <span id="page-20-28"></span>[171] Mark Ryan. 2022. The social and ethical impacts of artificial intelligence in agriculture: mapping the agricultural AI literature. AI & SOCIETY (2022), 1–13.
- <span id="page-20-3"></span>[172] Salesforce. 2024. Sustainable AI Policy Principles. [https://www.salesforce.com/content/dam/web/en\\_us/www/documents/company/sustainability/salesforce-sustainable-ai-policy-principles.pdf](https://www.salesforce.com/content/dam/web/en_us/www/documents/company/sustainability/salesforce-sustainable-ai-policy-principles.pdf)
- <span id="page-20-11"></span>[173] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. "Everyone wants to do the model work, not the data work": Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1–15.
- <span id="page-20-12"></span>[174] Aaron Sankin and Surya Mattu. 2023. Predictive Policing Software Terrible At Predicting Crimes. [https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes.](https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes)
- <span id="page-20-1"></span>[175] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C. Parkes, and Yang Liu. 2019. How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness.In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, HI, USA) (AIES '19). Association for Computing Machinery, New York, NY, USA, 99–106.<https://doi.org/10.1145/3306618.3314248>
- <span id="page-20-24"></span>[176] David Schlosberg. 2012. Climate Justice and Capabilities: A Framework for Adaptation Policy. Ethics & International Affairs 26, 4 (2012), 445–461.
- <span id="page-20-20"></span>[177] David Schlosberg and Lisette B Collins. 2014. From environmental to climate justice: climate change and the discourse of environmental justice. Wiley Interdisciplinary Reviews: Climate Change 5, 3 (2014), 359–374.
- <span id="page-20-7"></span>[178] Jurgen Schmidhuber. 1991. Neural Sequence Chunkers. Technical Report FKI-148-91 (1991).
- <span id="page-20-26"></span>[179] Christian Schroeder de Witt and Thomas Hornigold. 2019. Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem. arXiv e-prints (2019), arXiv–1905.
- <span id="page-20-18"></span>[180] Paul Schütze. 2024. The Problem of Sustainable AI: A Critical Assessment of an Emerging Phenomenon. Weizenbaum Journal of the Digital Society 4, 1 (2024).
- <span id="page-20-5"></span>[181] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.

- <span id="page-21-15"></span>[182] Raesetje Sefala, Timnit Gebru, Luzango Mfupe, Nyalleng Moorosi, and Richard Klein. 2021. Constructing a visual dataset to study the effects of spatial apartheid in South Africa. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2).
- <span id="page-21-1"></span>[183] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency. 59–68.
- <span id="page-21-13"></span>[184] Sarab S Sethi, Avery Bick, Robert M Ewers, Holger Klinck, Vijay Ramesh, Mao-Ning Tuanmu, and David A Coomes. 2023. Limits to the accurate and generalizable use of soundscapes to monitor biodiversity. Nature Ecology & Evolution 7, 9 (2023), 1373–1378.
- <span id="page-21-20"></span>[185] Abhinav Sharma, Arpit Jain, Prateek Gupta, and Vinay Chowdary. 2020. Machine learning applications for precision agriculture: A comprehensive review. IEEE Access 9 (2020), 4843–4873.
- <span id="page-21-22"></span>[186] Ben Shenglin, Felice Simonelli, Zhang Ruidong, Romain Bosc, and Li Wenwei. 2017. Digital infrastructure: Overcoming the digital divide in emerging economies. G20 Insights 3 (2017), 1–36.
- <span id="page-21-12"></span>[187] Ahmed AH Siddig. 2019. Why is biodiversity data-deficiency an ongoing conservation dilemma in Africa? Journal for Nature Conservation 50 (2019), 125719.
- <span id="page-21-11"></span>[188] Jessie J Smith, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. Real ML: Recognizing, exploring, and articulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 587–597.
- <span id="page-21-25"></span>[189] Irene Solaiman. 2023. The gradient of generative AI release: Methods and considerations. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 111–122.
- <span id="page-21-17"></span>[190] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and Apostol Vassilev. 2023. Evaluating the Social Impact of Generative AI Systems in Systems and Society. arXiv[:2306.05949](https://arxiv.org/abs/2306.05949) [cs.CY]
- <span id="page-21-9"></span>[191] Maddie Stone. 2024. Microsoft employees spent years fighting the tech giant's oil ties. Now, they're speaking out. <https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/>
- <span id="page-21-0"></span>[192] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).
- <span id="page-21-10"></span>[193] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1–9.
- <span id="page-21-19"></span>[194] Sy Taffel, Laura Bedford, and Monique Mann. 2019. Ecocide isn't ethical: Political ecology and capitalist AI ethics. ECONOMIES OF VIRTUE 20 (2019), 58.
- <span id="page-21-6"></span>[195] Petros Terzis. 2020. Onward for the Freedom of Others: Marching beyond the AI Ethics. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT\* '20). Association for Computing Machinery, New York, NY, USA, 220–229. <https://doi.org/10.1145/3351095.3373152>
- <span id="page-21-23"></span>[196] Julien Troudet, Philippe Grandcolas, Amandine Blin, Régine Vignes-Lebbe, and Frédéric Legendre. 2017. Taxonomic bias in biodiversity data and societal preferences. Scientific reports 7, 1 (2017), 9132.
- <span id="page-21-16"></span>[197] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakalembe, and Hannah Kerner. 2021. CropHarvest: A global dataset for croptype classification. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). <https://openreview.net/forum?id=JtjzUXPEaCu>
- <span id="page-21-14"></span>[198] Asaf Tzachor, Catherine E Richards, Masilin Gudoshava, Patricia Nying'uro, Herbert Misiani, Jemimah G Ongoma, Yoav Yair, Yacob Mulugetta, and Amadou T Gaye. 2023. How to reduce Africa's undue exposure to climate risks. Nature 620, 7974 (2023), 488–491.
- <span id="page-21-3"></span>[199] UN General Assembly. 2015. Transforming our world : the 2030 Agenda for Sustainable Development. <https://www.refworld.org/legal/resolution/unga/2015/en/111816>
- <span id="page-21-8"></span>[200] UNESCO. 2021. Recommendation on the ethics of artificial intelligence.
- <span id="page-21-2"></span>[201] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. 2020. Towards debiasing NLU models from unknown biases. arXiv preprint arXiv:2009.12303 (2020).
- <span id="page-21-24"></span>[202] I. van de Poel. 2020. Embedding Values in Artificial Intelligence (AI) Systems. Minds & Machines 30 (2020), 385–409. <https://doi.org/10.1007/s11023-020-09537-4>
- <span id="page-21-4"></span>[203] Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. AI and Ethics 1, 3 (2021), 213–218.
- <span id="page-21-18"></span>[204] GaÃG, l Varoquaux, Alexandra Sasha Luccioni, and Meredith Whittaker. 2024. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI. arXiv preprint arXiv:2409.14160 (2024).
- <span id="page-21-7"></span>[205] Lucia Vesnic-Alujevic, Susana Nascimento, and Alexandre Polvora. 2020. Societal and ethical impacts of artificial intelligence: Critical notes on European policy frameworks. Telecommunications Policy 44, 6 (2020), 101961.
- <span id="page-21-5"></span>[206] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. Nature communications 11, 1 (2020), 1–10.
- <span id="page-21-21"></span>[207] Johannes M Waldmueller. 2015. Agriculture, knowledge and the 'colonial matrix of power': approaching sustainabilities from the Global South. Journal of Global Ethics 11, 3 (2015), 294–302.

- <span id="page-22-7"></span>[208] J. Walmsley. 2021. Artificial Intelligence and the Value of Transparency. AI & Society 36 (2021), 585–595. <https://doi.org/10.1007/s00146-020-01066-z>
- <span id="page-22-0"></span>[209] Laura Weidinger, Kevin R. McKee, Richard Everett, Saffron Huang, Tina O. Zhu, Martin J. Chadwick, Christopher Summerfield, and Iason Gabriel. 2023. Using the Veil of Ignorance to align AI systems with principles of justice. Proceedings of the National Academy of Sciences 120, 18 (2023), e2213709120.<https://doi.org/10.1073/pnas.2213709120> arXiv[:https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120](https://arxiv.org/abs/https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120)
- <span id="page-22-5"></span>[210] Craig R White, Dustin J Marshall, Steven L Chown, Susana Clusella-Trullas, Steven J Portugal, Craig E Franklin, and Frank Seebacher. 2021. Geographical bias in physiological data limits predictions of global change impacts. Functional Ecology 35, 7 (2021), 1572–1578.
- <span id="page-22-11"></span>[211] Adrienne Williams, Milagros Miceli, and Timnit Gebru. 2022. The exploited labor behind artificial intelligence. Noema Magazine 13 (2022).
- <span id="page-22-8"></span>[212] T. Wischmeyer. 2020. Artificial Intelligence and Transparency: Opening the Black Box. In Regulating Artificial Intelligence. Springer, 75–101.
- <span id="page-22-4"></span>[213] Robert Wolfe and Aylin Caliskan. 2022. American== white in multimodal language-and-image ai. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. 800–812.
- <span id="page-22-9"></span>[214] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).
- <span id="page-22-1"></span>[215] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).
- <span id="page-22-3"></span>[216] D. Zeng. 2015. AI Ethics: Science Fiction Meets Technological Reality. IEEE Intelligent Systems 30, 03 (may 2015), 2–5. <https://doi.org/10.1109/MIS.2015.53>
- <span id="page-22-6"></span>[217] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. 2019. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning. PMLR, 7472–7482.
- <span id="page-22-10"></span>[218] Aram Ziai. 2016. Development discourse and global history: From colonialism to the sustainable development goals. Taylor & Francis.
- <span id="page-22-2"></span>[219] Ofer Zwikael and John Smyrk. 2015. Project governance: Balancing control and trust in dealing with risk. International Journal of Project Management 33, 4 (2015), 852–862.