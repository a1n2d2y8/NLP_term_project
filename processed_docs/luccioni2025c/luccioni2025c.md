# **Misinformation by Omission: The Need for More Environmental Transparency in AI**

**Sasha Luccioni**1,\***, Boris Gamazaychikov** 2 **, Theo Alves da Costa** 3 **, and Emma Strubell** 4

- <sup>1</sup>Hugging Face, Montreal, Canada
- <sup>2</sup>Salesforce, Paris, France
- <sup>3</sup>Ekimetrics, Paris, France
- <sup>4</sup>Carnegie Mellon University, School of Computer Science, Pittsburgh, USA
- \*sasha.luccioni@huggingface.co

## **ABSTRACT**

In recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational power and natural resources. In parallel to this trend, transparency around the costs and impacts of these models has decreased, meaning that the users of these technologies have little to no information about their resource demands and subsequent impacts on the environment. Despite this dearth of adequate data, escalating demand for figures quantifying AI's environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized best-effort estimates of greenhouse gas emissions. In this article, we explore pervasive myths and misconceptions shaping public understanding of AI's environmental impacts, tracing their origins and their spread in both the media and scientific publications. We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and conclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate negative impacts in the future.

## **Introduction**

AI-powered tools and systems are becoming increasingly ubiquitous, reshaping human behavior with corresponding impacts to our socioeconomic systems. While the companies that develop and deploy AI-driven technology strongly emphasize AI's positive impacts (real and speculative) [1](#page-7-0) – [3](#page-7-1) , the negative impacts are typically left unspoken. These are often uncovered by researchers or a concerned public who audit AI systems, driven by the desire to understand the impacts of these systems on society and the planet. Among the many impacts that have been analyzed in recent years are: algorithmic discrimination and bias [4](#page-7-2) , [5](#page-7-3) , the use of AI in military applications [6](#page-7-4) , the threat of AI to democracy [7](#page-7-5) , [8](#page-7-6) , and environmental impacts [9](#page-7-7) , [10](#page-7-8) .

Honing in on the latter, researchers and activists alike have been sounding the alarm on the increasingly unsustainable trends in energy and natural resource consumption arising from the data centers and devices used to train and deploy AI models which are growing in size and complexity[11](#page-7-9) [–15](#page-7-10). These resources range from the rare earth minerals necessary to manufacture computing hardware, the energy needed to power the very tangible "cloud" computation that underpins AI systems, the water needed for cooling and hardware manufacturing, and the greenhouse gases (GHGs) emitted at every stage of this process. Despite these rising costs, there exists minimal and often no data quantifying these impacts. When data does exist, it often lacks sufficient accessibility, detail and scope to enable effective decision-making or analysis, impeding impact assessment, mitigation, forecasting, and even basic understanding by the public at large.

As a result, researchers, investors, companies and policymakers are left to attempt best-effort approximations given limited data availability. In some cases, these estimates can be wildly flawed due to lack of critical prerequisite data, understanding or expertise. In other cases, estimates may be taken out of the careful, qualified contexts in which they were originally presented, leading to misinterpretation and in some cases severely inaccurate generalizations. The increasingly high stakes political and economic contexts surrounding climate change and AI severely compound this challenge; mistakes and misinterpretation devolve into misinformation as estimates are repeatedly shared and transformed through subsequent analyses, adopted as accurate measures and spread as trending posts through social media and the news, finally arriving on the desks of decisionmakers. The resulting misconceptions harm all stakeholders: policymakers and the public are unable to make informed decisions, and AI technology developers suffer from negative perceptions arising from overestimates of their social harms, further exacerbating their lack of disclosure. In this paper, we aim to elucidate some common myths surrounding AI's environmental impacts, explore the pitfalls that led to the emergence of those myths, and propose recommendations for remedying this challenge in the future.

# **Related Work**

Approaches to calculating the environmental impacts of AI systems have evolved significantly over the last several years, as these systems have grown more impactful and widely deployed in user-facing applications. Initial studies, such as that of Strubell et al., underscored the environmental cost of training Transformer-based language models[12](#page-7-11). Research done in the following years extended this analysis, for instance by calculating the energy use and GHG footprint for several notable AI models including GPT-3, T5, Meena, and Switch Transformer, providing new estimates[16](#page-7-12) and expanding the scope of analysis beyond model training to account for operational and embodied emissions[17](#page-7-13), improving methodology for software energy measurement[18](#page-7-14), and a lifecycle approach to assessing emissions from model training and deployment[11](#page-7-9). Wu et al. further advanced this analysis by explicitly mapping the environmental impacts across the entire AI development pipeline,[19](#page-7-15). Most recently, Luccioni, Jernite, and Strubell [20](#page-7-16) pioneered the AI inference impact methodology, revealing generative architectures as particularly energy-intensive compared to task-specific models and underscoring the critical importance of addressing inference impacts. These methodologies were then adapted into the AI Energy Score [21](#page-7-17), a project aiming to establish a unified approach for comparing the inference efficiency of AI models[22](#page-7-18) .

Above and beyond energy considerations, Li et al.[23](#page-7-19) expanded the scope of AI environmental impact measurement by estimating the water footprint of GPT-3 based on publicly available information, whereas Han et al.[24](#page-7-20) assessed the public health toll of AI training's air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. In another significant advancement, Google's recent TPU lifecycle assessment[25](#page-8-0) offered the most comprehensive cradle-to-grave environmental analysis of AI hardware to date, integrating embodied carbon data associated with manufacturing AI accelerators and data center infrastructure, significantly extending existing environmental impact models. Building on many of these approaches, Morrison et al.[26](#page-8-1) performed a holistic evaluation of the energy, carbon, and water impacts of AI hardware manufacturing, model development, and training, enhancing the accuracy of these metrics through the use of granular underlying data.

The breadth and diversity of the analyses described in this section illustrate the multitude of factors involved in estimating AI's environmental impacts, and the many different perspectives that exist in this space. Whereas several standardized approaches have been proposed to measure different aspects of AI's requirements in terms of energy and water, as well as the emissions associated with model training and inference, the field is still currently lacking a comprehensive methodology and standards that cover all dimensions. In the next section, we examine how this translates into decreased environmental transparency in the AI industry via an empirical analysis of AI models over time.

# **Environmental Transparency Trends**

While there has been progress in developing more robust methodologies for measuring AI's environmental impacts, the broader AI industry has paradoxically been trending in the opposite direction, disclosing less information over time. In order to quantify this trend, we analyze Epoch AI's Notable AI Models dataset[27](#page-8-2), which tracks information on *"models that were state of the art, highly cited, or otherwise historically notable"*, with respect to transparency about the environmental impacts of those models. We examine the level of environmental impact transparency for each model based on key information from the Epoch AI dataset (e.g., model accessibility, training compute estimation method) as well as from individual model release content (e.g., paper, model card, announcement). We select the time period starting in 2010 as this is the beginning of the modern "deep learning era" (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed, including all 754 models from 2010 to the first quarter of 2025. Our analysis, shown in Figure [1,](#page-2-0) reveals substantial variation in environmental impact transparency: some models disclose sufficient details to enable impact estimation, whereas others provide no information at all regarding their approach.

Overall, we find that models exhibit three transparency categories:

- *Direct Disclosure*: Developers explicitly reported energy or GHG emissions. Note that this category includes methodologies ranging from estimation (e.g., using hardware TDP, country average carbon intensity) to measurements (i.e., using tools like CodeCarbon).
- *Indirect Disclosure*: Developers provided training compute data or released their model weights, allowing external estimates of training or inference impacts.
- *No Disclosure*: Environmental impact data was not publicly released and estimation approaches (as noted in Indirect Disclosure) were not possible.

From 2010 to 2018, only 17% of the models shared data that could be used to indirectly estimate environmental impact of model training (ranging from 0 to 33% each year); no direct environmental impact data was released during this period. This is expected, given that AI models of that era required significantly less compute and resource usage transparency was not

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

**Figure Description:**
The image displays a bar chart titled "Environmental Transparency." It shows data for four years: 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, and 2023. Each year has three bars corresponding to different categories of environmental transparency: None, Indirect, and Direct.

The vertical axis is labeled with numbers ranging from 0 to 100, indicating some form of measurement or score related to environmental transparency. The horizontal axis lists the years mentioned above.

In terms of specific numerical values, there are several instances where the value for one category (None) is higher than that of another category (Indirect or Direct). For example, in 2013, the value for None is 80 compared to 50 for Indirect and 60 for Direct. Similarly, in 2015, the value for None is 70 while Indirect and Direct have lower scores at 50 and 60 respectively. However, without more context, it's unclear exactly what these numbers represent as they could be percentages, points on a scale, or any other unit of measure.

Overall, the chart provides a visual representation of how environmental transparency has changed over time across the three categories. The use of color coding helps distinguish between the categories, making it easier to compare their performance each year.



**Figure 1.** Environmental Impact Transparency of Notable AI Models by Release Year[27](#page-8-2)

<span id="page-2-1"></span>yet common practice, although many articles accompanying papers did provide related information about, e.g. the amount of training data used or number of epochs trained. From 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. This period includes the the work of Strubell et al.[12](#page-7-11), Luccioni[11](#page-7-9) and others. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the "no disclosure" category, as the line between research and commercial deployment became increasingly blurred.

![](_page_2_Figure_3.jpeg)

**Figure Description:**
The image displays a circular chart with three segments, each representing different categories: Direct, Indirect, and None. Each segment is filled to varying degrees, indicating the percentage of responses that fall into these categories.

The "Direct" segment has been filled to approximately 2.0% as indicated by the green coloring within it. Adjacent to this segment is the "Indirect" section, which appears to be filled up to around 13.9%. This indicates that slightly over one-third (approximately 40%) of respondents reported their actions as indirectly related to the subject at hand. Finally, the largest segment represents those who did not directly or indirectly relate to the subject, accounting for about 84.1%, as shown by the red filling.

On the left side of the circle, there are numerical percentages corresponding to each category. For Direct, it's 2.0%; for Indirect, it's 13.9%; and for None, it's 84.1%. These figures provide a clear visual representation of how many people fell into each category based on the survey data represented by the pie chart.



**Figure 2.** Environmental Impact Transparency of LLM Usage – OpenRouter[28](#page-8-3) (May 2025)

Beyond the long term trend, zooming in to examine recent AI model usage data helps illustrate today's environmental impact transparency conditions. OpenRouter[28](#page-8-3), a widely-used API platform for LLMs, publicly shares data on model traffic including top 20 models by month, and the number of tokens running through every model. May 2025 data (Figure [2\)](#page-2-1) indicates that of the top 20 used models, only one (Meta Llama 3.3 70B) directly released environmental data and three (DeepSeek R1, DeepSeek V3, Mistral Nemo) release it indirectly (by sharing compute data like GPU type and training length, as well as by releasing their model weights to enable efficiency analysis). In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure. This indicates that the majority of users who interact with LLMs have no information about their environmental impacts, and cannot make informed decisions based on model efficiency or carbon intensity.

From the limited data that is publicly available, we can observe significant disparities in energy use and emissions across models. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations). Inference workloads also show wide variation depending on model size, architecture and task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables [1](#page-10-0) and [2](#page-11-1) in the Appendix for more information). These ranges highlight not only the scale of potential impacts, but also the pressing need for more standardized and transparent reporting to enable meaningful comparisons.

## **Investigating the Urban Legends of AI's Environmental Impacts**

Making sustainably-minded decisions when using AI systems requires having the necessary information about different aspects of their development and deployment. While there are empirical studies focusing on AI's environmental impacts, such as those cited in previous sections, these numbers have often been taken out of context or used as proxies for conditions (e.g., model size, architecture, optimizations, hardware, location, setup, system) that they are not representative of. This fuels misinformation, undermines scientific research, and can result in decisions that are not grounded in facts[29](#page-8-4). In the paragraphs below, we address some of the common estimates for the environmental impacts of AI, in an effort to contextualize their provenance and to explore their potential for spreading environmental misinformation.

# **Training an AI model emits as much CO**<sup>2</sup> **as five cars in their lifetimes**

Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,[12](#page-7-11) which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original "base" (65M) and "big" (213M parameter) variants of the Transformer neural network architecture[30](#page-8-5) that forms the basis of LLMs to this day, as well as the cost to perform model *development*, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.[31](#page-8-6) In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.

The research article was written for a specialized audience of AI and NLP researchers, who would have the background knowledge to understand the appropriate scoping for the estimate. However, an author's tweet publicizing the paper and featuring a table containing the "five cars" estimate was widely shared on social media, leading to the publication being picked up by numerous media outlets (including MIT Technology Review[32](#page-8-7) and Forbes[33](#page-8-8)). The "five cars" number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training; the original article reports AI training workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity), and AI model training more broadly often requires even less energy and corresponding emissions.

Further, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs. In this way, the NAS training workload is similar to today's generative AI pretraining workloads, which are similarly performed less frequently than the average AI training. However, while the "five cars" estimate from Strubell et al. is not an accurate representation of the emissions arising from every AI training workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the "five cars" estimate: Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,[34](#page-8-9) over 4x the estimate that forms the basis for the "five cars" number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e [35](#page-8-10) or over 40x the "five cars" estimate.

#### **A request to ChatGPT consumes ten times more energy than a Google search**

Another often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours (Wh) of energy, which is "ten times more than a Google search". This figure is often quoted in the press[36,](#page-8-11) [37](#page-8-12) and in industry reports[38](#page-8-13). Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet's Chairman John Hennessy during a 2023 interview with Reuters, in which he said that *"having an exchange with AI known as a large language model likely cost 10 times more than a standard keyword search"*[39](#page-8-14). This remark was used was the basis of an estimate published in October 2023 of *"approximately 3 Wh per LLM interaction"*[40](#page-8-15), with the Google search number taken from a 2009 blog post from Google that stated that *"Queries vary in degree of difficulty, but for the average query [...] this amounts to 0.0003 kWh of energy per search"[41](#page-8-16)*. This number is misleading for several reasons. First, Hennessy has no relation to OpenAI or Microsoft (which provides the compute for OpenAI's services), so the comment he made was based on secondhand information. Second, even if Hennessy's comparison were accurate, basing the search estimate on a figure that is 16 years old — at a time when Web search was done using bag-of-words or vector-based search techniques as opposed to the current Transformer-based models — is also bound to amplify the inaccuracy of the estimate.

To understand the impact of the propagation of this estimate, we analyzed 100 news articles published as of April 11, 2025, that appear when searching for *"ChatGPT energy consumption"* on Google News. For each article, we noted whether it mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by acknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure [3,](#page-4-0) reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search[42](#page-8-17), 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs[43](#page-8-18) (likely using the same 3 Wh figure), 11% prefer to provide global figures on the energy impact of data centers[44](#page-8-19), 8% discuss other topics, particularly DeepSeek[45](#page-8-20) and optimizations with ternary neural network architectures to improve energy efficiency[46](#page-8-21) and only 5% explicitly call for transparency or necessary caution when addressing this subject[47](#page-8-22), stating that the true figures remain unknown. It is also noteworthy that among these articles, 9% also relay the claim that training a LLM produces emissions equivalent to 5 cars in their lifetime.

<span id="page-4-0"></span>![](_page_4_Figure_2.jpeg)

**Figure Description:**
The image is a pie chart that illustrates the distribution of search queries across various categories. At the center of the chart is an empty circle with concentric rings, each representing a different category or topic. From the innermost ring to the outermost one, the following topics are depicted:

1. **Call for transparency/caution**: This category has the largest share at 51%, indicating it receives the most search queries among all other topics.
2. **Other topics (e.g., optimization)**: This category occupies the second-largest slice of the pie chart, accounting for approximately 8% of the searches.
3. **Include global figures**: This category represents about 7% of the total search queries.
4. **Include other numbers**: This category accounts for around 6% of the search queries.
5. **Include "3 WH" or "10x Google Search":** This category seems to be the smallest, as indicated by its position near the edge of the pie chart, suggesting it might receive less than 1% of the search queries.

The colors used in the chart are red, yellow, green, blue, and orange, which correspond to the respective categories. Each color segment is labeled with the name of the corresponding category. Additionally, there's a legend in the top left corner of the chart that explains the meaning of the colored segments.

At the bottom of the chart, there's a note stating "include '3 WH' or '10x Google Search':," but without further context, it's unclear how this statement relates to the rest of the information presented in the chart.



**Figure 3.** Analysis of media articles discussing ChatGPT energy consumption.

#### **AI can reduce 10% of global emissions**

While the numbers around AI's negative environmental impacts can be misinterpreted and taken out of context, so, too, can the potential of AI to reduce emissions, especially by corporate actors that develop and deploy AI systems on a global scale. One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that *"Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union"*[48](#page-8-23). The same number appears in a more recent BCG report from 2023, which was commissioned by Google and published ahead of COP26[49](#page-8-24). The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale.

Applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding in fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous calculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence analysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T L.1480[50](#page-8-25) or WBCSD guidance on avoided emissions[51](#page-8-26). And yet, these numbers were picked up in research[52](#page-8-27) and the media, used as evidence that the potential of AI to stop climate change is overwhelmingly positive[53,](#page-8-28) [54](#page-8-29). While AI undoubtedly has potential positive applications in sectors ranging from transportation to agriculture to energy[55](#page-8-30), these global generalizations can be misleading because they overlook the myriad of problems that technology alone cannot solve, while giving credibility to the beliefs that the benefits of AI will outweigh its costs[56](#page-9-0) .

The lack of transparency around AI's environmental impacts can have far-reaching consequences, ranging from specific estimates taken out of context and blown out of proportion, to proxies becoming adopted by press and policymakers in the absence of more reliable figures. In the next section, we discuss a potential solution to this situation by proposing a set of metrics that different stakeholders can measure and report to bring more clarity to the extent of AI's environmental impacts.

## **How to improve environmental impact disclosures in AI**

Opacity in AI environmental reporting creates multiple interconnected challenges: organizations cannot make informed procurement or innovation decisions without access to reliable environmental performance data on AI, while policymakers lack the information necessary to develop evidence-based regulations. This opacity also generates cascading effects throughout value chains, as AI adoption creates unmeasured emissions that undermine corporate net zero commitments. Furthermore, the absence of standardized metrics prevents meaningful comparison between AI systems, limiting market mechanisms that could drive efficiency improvements. Perhaps most critically, this lack of transparency undermines accountability mechanisms, making it impossible to hold AI developers and deployers responsible for their environmental performance or to track progress toward sustainability goals.

This section explores how comprehensive environmental transparency can address these challenges through four interconnected pathways:

- 1. Carrying out comprehensive measurement and disclosure by AI developers at each stage of model development and deployment;
- 2. Integrating comprehensive AI environmental impacts into sustainability accounting frameworks and corporate sustainability disclosures by organizations across the entire AI value chain, from model providers and hyperscalers to end-user enterprises;
- 3. Developing standardized verification and assurance frameworks to ensure data reliability and enable meaningful comparisons; and
- 4. Implementing clear regulatory requirements by policymakers to ensure consistent, verifiable reporting across the industry.

**Measurement and Disclosure** As the starting point of AI development, AI researchers and developers are able to gather empirical measurements from the systems they create at different steps of the model lifecycle. When developing models from scratch, energy consumption and GHG emissions from training and inference can be estimated using programmatic tools like Code Carbon[57](#page-9-1) or no-code tools like Green Algorithms[58](#page-9-2). When using or adapting existing models, performance and efficiency testing can significantly reduce emissions by enabling the deployment of more energy-efficient models in production. For instance, the AI Energy Score project[21](#page-7-17) provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets. These metrics should be reported in model cards[59](#page-9-3) and scientific publications with complete methodological transparency, including hardware specifications, geographic locations, electricity sources, measurement uncertainties, and allocation methodologies. This empirical foundation enables downstream organizational GHG accounting while contributing to the broader scientific understanding of AI environmental impacts through peer-reviewed publication of methodologies and results. AI providers across the entire value chain, including cloud infrastructure providers, model hosting platforms, and API service providers, must implement comprehensive transparency with granular environmental data disclosure, enabling downstream organizations to accurately account for their AI-related environmental impacts. Government and public sector organizations should mandate transparency in all AI procurements, require open data for publicly funded research, and align AI deployments with existing net zero commitments.

**Organizational Implementation and Processes** As AI adoption accelerates, organizations should implement comprehensive frameworks to assess, measure, and integrate AI's environmental impacts into existing sustainability management systems using structured approaches tailored to their specific contexts and risk profiles. The materiality assessment framework should aim to establish quantitative thresholds across environmental intensity and usage scale dimensions – for example creating distinct tiers of analysis intensity. Organizations developing AI systems utilizing open-source models on their infrastructure should implement comprehensive measurement protocols at multiple levels of granularity: model-specific, service or process-level, and organization-wide aggregations. Similarly, entities utilizing third-party AI services (e.g., API-based integrations of commercial models or subscription-based access for internal teams like ChatGPT, Copilot or Claude) should demand transparency by incorporating environmental disclosure requirements into procurement processes and contractual agreements[60](#page-9-4). Specifically, organizations should request access to standardized metrics (such as the AI Energy Score or an equivalent) for all AI services under consideration. These environmental metrics should be systematically integrated into organizations' GHG accounting frameworks and non-financial performance disclosures, with explicit documentation of methodological assumptions and unmodeled factors.

**Standards, Verification and Assurance** Environmental AI disclosures require robust verification frameworks to ensure accuracy and prevent greenwashing, necessitating new assurance standards adapted to AI's rapid evolution, distributed compute, and complex value chains. While no unified standard yet exists for assessing AI sustainability, parallel efforts are underway across organizations such as the Green Software Foundation, ISO (International Organization for Standardization), and OECD (Organization for Economic Cooperation and Development). These bodies are well-positioned to develop standardized approaches for stakeholders ranging from developers to governments. Given AI's transnational nature, coordination and harmonization of these efforts is essential. Without alignment, implementation may diverge across jurisdictions, creating further confusion in the market. However, as formal standards may take years to materialize, interim ad hoc methods (such as those outlined above) can provide valuable insights and help shape the eventual development of formal methodologies. These AI environmental disclosure frameworks must also strengthen adherence to robust GHG accounting principles, particularly regarding the GHG Protocol's treatment of electricity emissions measurement. The current allowance for market-based accounting enables companies to significantly under-report their actual AI-related emissions through renewable energy certificates, creating the same problematic disconnect from reality that has undermined carbon offsetting credibility [56](#page-9-0). For AI services consuming substantial electricity across distributed data centers, mandatory location-based accounting would ensure environmental transparency frameworks capture the true systemic climate impacts rather than allowing them to be obscured through market mechanisms.

**Policy Frameworks and Reporting** Environmental transparency documentation is already commonplace for private organizations in existing legislation such as the Corporate Sustainability Reporting Directive (CSRD) in the EU, SEC climate disclosure requirements in the US, or local and state-level climate disclosure laws. However, policymakers should incorporate additional reporting requirements specifically addressing AI system utilization under standards such as European Sustainability Reporting Standards E1 (Climate Change) which mandates the disclosure of Scope 1, 2, and 3 GHG emissions, energy usage, and a transition plan aligned with the Paris Agreement[61](#page-9-5), particularly as this aligns with existing provisions in the EU AI Act. Non-governmental sustainability rating agencies such as CDP and EcoVadis should similarly expand their assessment criteria to incorporate AI-specific environmental impact metrics, creating market incentives for improved disclosure practices. For organizations directly participating in the AI value chain (service providers, data center operators, developers, IT integrators, semiconductor and GPU manufacturers) policymakers should implement more stringent transparency requirements. These could include mandatory detailed environmental reporting disaggregated by model, usage patterns, and physical infrastructure. Enforcement mechanisms might include annual comprehensive environmental reports or conditioning access to public markets and funding on compliance with disclosure standards.

## **Conclusion**

The current trend toward reduced transparency around AI's environmental impact contributes to misinformation and hinders informed decision-making across all levels, from individual researchers and developers to organizations and policymakers. This declining transparency is particularly troubling given AI's escalating environmental impacts amid global climate concerns and looming planetary boundaries. While competition is frequently cited to justify opacity, other competitive industries, such as food (with ingredient labeling) and healthcare (with side-effect and pricing transparency), demonstrate that a balance between transparency and competition is achievable. Reversing the trend toward opacity in AI environmental reporting is essential for informed decision-making, accountability, and sustainable technology advancement, particularly as new model paradigms emerge that may alter these impacts. As members of the AI community committed to addressing the climate crisis, we aim to ensure the sustainability of our field as it continues to expand – recognizing that increased transparency is fundamental to this goal.

# **References**

- <span id="page-7-0"></span>1. Luers, A. *et al.* Will ai accelerate or delay the race to net-zero emissions? *Nature* 628, 718–720 (2024).
- 2. Kshirsagar, M. *et al.* Becoming good at AI for good. In *Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society*, 664–673 (2021).
- <span id="page-7-1"></span>3. DeepMind. Using ai to fight climate change (2009).
- <span id="page-7-2"></span>4. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. In *Ethics of data and analytics*, 254–264 (Auerbach Publications, 2022).
- <span id="page-7-3"></span>5. Buolamwini, J. & Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In *Conference on fairness, accountability and transparency*, 77–91 (PMLR, 2018).
- <span id="page-7-4"></span>6. Rivera, J.-P. *et al.* Escalation risks from language models in military and diplomatic decision-making. In *Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency*, 836–898 (2024).
- <span id="page-7-5"></span>7. Manheim, K. & Kaplan, L. Artificial intelligence: Risks to privacy and democracy. *Yale JL & Tech.* 21, 106 (2019).
- <span id="page-7-6"></span>8. Summerfield, C. *et al.* How will advanced ai systems impact democracy? *arXiv preprint arXiv:2409.06729* (2024).
- <span id="page-7-7"></span>9. Crawford, K. *The atlas of AI: Power, politics, and the planetary costs of artificial intelligence* (Yale University Press, 2021).
- <span id="page-7-8"></span>10. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In *Proceedings of the 2021 ACM conference on fairness, accountability, and transparency*, 610–623 (2021).
- <span id="page-7-9"></span>11. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Estimating the carbon footprint of BLOOM, a 176b parameter language model. *arXiv preprint arXiv:2211.02001* (2022).
- <span id="page-7-11"></span>12. Strubell, E., Ganesh, A. & McCallum, A. Energy and policy considerations for deep learning in NLP. *arXiv preprint arXiv:1906.02243* (2019).
- 13. Luccioni, A. S. & Hernandez-Garcia, A. Counting carbon: A survey of factors influencing the emissions of machine learning. *arXiv preprint arXiv:2302.08476* (2023).
- 14. Dodge, J. *et al.* Measuring the carbon intensity of AI in cloud instances. In *Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency*, 1877–1894 (2022).
- <span id="page-7-10"></span>15. Ligozat, A.-L., Lefèvre, J., Bugeau, A. & Combaz, J. Unraveling the hidden environmental impacts of AI solutions for environment. *arXiv preprint arXiv:2110.11822* (2021).
- <span id="page-7-12"></span>16. Patterson, D. *et al.* Carbon emissions and large neural network training. *arXiv preprint arXiv:2104.10350* (2021).
- <span id="page-7-13"></span>17. Gupta, U. *et al.* Chasing Carbon: The Elusive Environmental Footprint of Computing. In *2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)*, 854–867 (IEEE, 2021).
- <span id="page-7-14"></span>18. Cao, Q., Lal, Y. K., Trivedi, H., Balasubramanian, A. & Balasubramanian, N. IrEne: Interpretable energy prediction for transformers. In Zong, C., Xia, F., Li, W. & Navigli, R. (eds.) *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, 2145–2157, DOI: <10.18653/v1/2021.acl-long.167> (Association for Computational Linguistics, Online, 2021).
- <span id="page-7-15"></span>19. Wu, C.-J. *et al.* Sustainable AI: Environmental Implications, Challenges and Opportunities. *arXiv preprint arXiv:2111.00364* (2021).
- <span id="page-7-16"></span>20. Luccioni, S., Jernite, Y. & Strubell, E. Power hungry processing: Watts driving the cost of ai deployment? In *The 2024 ACM Conference on Fairness, Accountability, and Transparency*, FAccT '24, 85–99, DOI: <10.1145/3630106.3658542> (ACM, 2024).
- <span id="page-7-17"></span>21. Luccioni, S. & Gamazaychikov, B. AI Energy Score Leaderboard. [https://huggingface.co/spaces/AIEnergyScore/](https://huggingface.co/spaces/AIEnergyScore/Leaderboard) [Leaderboard](https://huggingface.co/spaces/AIEnergyScore/Leaderboard) (2025). Hugging Face Spaces.
- <span id="page-7-18"></span>22. Luccioni, S. *et al.* Light bulbs have energy ratings—so why can't ai chatbots? *Nature* 632, 736–738 (2024).
- <span id="page-7-19"></span>23. Li, P., Yang, J., Islam, M. A. & Ren, S. Making ai less" thirsty": Uncovering and addressing the secret water footprint of ai models. *arXiv preprint arXiv:2304.03271* (2023).
- <span id="page-7-20"></span>24. Han, Y., Wu, Z., Li, P., Wierman, A. & Ren, S. The unpaid toll: Quantifying the public health impact of ai. *arXiv preprint arXiv:2412.06288* (2024).

- <span id="page-8-0"></span>25. Schneider, I. *et al.* Life-cycle emissions of ai hardware: A cradle-to-grave approach and generational trends (2025). [2502.01671.](2502.01671)
- <span id="page-8-1"></span>26. Morrison, J. *et al.* Holistically evaluating the environmental impact of creating language models. *arXiv preprint arXiv:2503.05804* (2025).
- <span id="page-8-2"></span>27. Epoch AI. Data on notable ai models (2024). Accessed: 2025-04-06.
- <span id="page-8-3"></span>28. OpenRouter. Openrouter leaderboard rankings. <https://openrouter.ai/rankings?view=month> (2025). Accessed: 2025-06-03.
- <span id="page-8-4"></span>29. Lovins, A. B. Artificial intelligence meets natural stupidity: Managing the risks (2025).
- <span id="page-8-5"></span>30. Vaswani, A. *et al.* Attention is all you need. *Adv. neural information processing systems* 30 (2017).
- <span id="page-8-6"></span>31. So, D., Le, Q. & Liang, C. The evolved transformer. In Chaudhuri, K. & Salakhutdinov, R. (eds.) *Proceedings of the 36th International Conference on Machine Learning*, vol. 97 of *Proceedings of Machine Learning Research*, 5877–5886 (PMLR, 2019).
- <span id="page-8-7"></span>32. Hao, K. Training a single ai model can emit as much carbon as five cars in their lifetimes. *MIT technology Rev.* 75, 103 (2019).
- <span id="page-8-8"></span>33. Toews, R. Deep learning's carbon emissions problem. *Forbes* (2020).
- <span id="page-8-9"></span>34. Gemma Team *et al.* Gemma 2: Improving open language models at a practical size (2024). [2408.00118.](2408.00118)
- <span id="page-8-10"></span>35. Meta. Llama 3.1 Model Card. <https://huggingface.co/meta-llama/Llama-3.1-8B> (2024). Hugging Face Model Card.
- <span id="page-8-11"></span>36. Kerr, D. & NPR. AI brings soaring emissions for Google and Microsoft, a major contributor to climate change. *NPR, July* (2024).
- <span id="page-8-12"></span>37. Chen, S. How much energy will ai really consume? the good, the bad and the unknown. *Nature* 639, 22–24 (2025).
- <span id="page-8-13"></span>38. Aljbour, J., Wilson, T. & Patel, P. Powering intelligence: Analyzing artificial intelligence and data center energy consumption. *EPRI White Pap. no. 3002028905* (2024).
- <span id="page-8-14"></span>39. Dastin, J. & Nellis, S. Focus: For tech giants, ai like bing and bard poses billion-dollar search problem. *Reuters* (2023).
- <span id="page-8-15"></span>40. De Vries, A. The growing energy footprint of artificial intelligence. *Joule* 7, 2191–2194 (2023).
- <span id="page-8-16"></span>41. Hölzle, U. Powering a google search (2009).
- <span id="page-8-17"></span>42. Inês Trindade Pereira. ChatGPT, Deepseek & Co: How much energy do AI-powered chatbots consume? Euronews (online). Accessed: 2025-06-01.
- <span id="page-8-18"></span>43. Hamish van der Ven. AI is bad for the environment, and the problem is bigger than energy consumption. The Conversation (online). Accessed: 2025-06-01.
- <span id="page-8-19"></span>44. Spencer Kimball. Data centers powering artificial intelligence could use more electricity than entire cities. CNBC (online). Accessed: 2025-06-01.
- <span id="page-8-20"></span>45. James O'Donnell. DeepSeek might not be such good news for energy after all. MIT Technology Review (online). Accessed: 2025-06-01.
- <span id="page-8-21"></span>46. Berry Zwets. Researchers claim to cut energy consumption AI 95 percent. Techzine (online). Accessed: 2025-06-01.
- <span id="page-8-22"></span>47. Adam Clark Estes. Should you feel guilty about using AI? Vox (online). Accessed: 2025-06-01.
- <span id="page-8-23"></span>48. Degot, C., Duranton, S., Frédeau, M. & Hutchinson, R. Reduce carbon and costs with the power of ai. *Boston Consult. Group* 26 (2021).
- <span id="page-8-24"></span>49. Dannouni, A. *et al.* Accelerating climate action with ai. *Boston Consult. Group Special Rep. Google* (2023).
- <span id="page-8-25"></span>50. ITU. Enabling the Net Zero transition: Assessing how the use of information and communication technology solutions impact greenhouse gas emissions of other sectors. Tech. Rep. ITU-T L.1480, ITU (2022). Accessed: 2025-06-01.
- <span id="page-8-26"></span>51. WBCSD. Guidance on Avoided Emissions. Tech. Rep., WBCSD (2023). Accessed: 2025-06-01.
- <span id="page-8-27"></span>52. Das, K. P. & Chandra, J. A survey on artificial intelligence for reducing the climate footprint in healthcare. *Energy Nexus* 9, 100167 (2023).
- <span id="page-8-28"></span>53. The Environment. Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022).
- <span id="page-8-29"></span>54. Kakkad, R. Google says AI could mitigate 5 to 10% of global emissions (2023).
- <span id="page-8-30"></span>55. Rolnick, D. *et al.* Tackling climate change with machine learning. *ACM Comput. Surv. (CSUR)* 55, 1–96 (2022).

- <span id="page-9-0"></span>56. Ambrose, J. & Hern, A. Ai will be help rather than hindrance in hitting climate targets, bill gates says (2024).
- <span id="page-9-1"></span>57. Schmidt, V. *et al.* Codecarbon: Estimate and track carbon emissions from machine learning computing (2021).
- <span id="page-9-2"></span>58. Lannelongue, L., Grealey, J. & Inouye, M. Green algorithms: Quantifying the carbon footprint of computation. *Adv. Sci.* 2100707 (2021).
- <span id="page-9-3"></span>59. Mitchell, M. *et al.* Model cards for model reporting. In *Proceedings of the conference on fairness, accountability, and transparency*, 220–229 (2019).
- <span id="page-9-4"></span>60. Luccioni, S. & Gamazaychikov, B. AI Models Hiding Their Energy Footprint? Here's What You Can Do (2025).
- <span id="page-9-5"></span>61. Leal Filho, W. *et al.* European sustainability reporting standards: An assessment of requirements and preparedness of eu companies. *J. environmental management* 380, 125008 (2025).
- <span id="page-9-6"></span>62. Gamazaychikov, B. Unveiling salesforce's blueprint for sustainable ai: Where responsibility meets innovation (2023).
- <span id="page-9-7"></span>63. Touvron, H. *et al.* Llama: Open and efficient foundation language models (2023). [2302.13971.](2302.13971)
- <span id="page-9-8"></span>64. Mesnard, T. *et al.* Gemma: Open models based on gemini research and technology (2024). [2403.08295.](2403.08295)
- <span id="page-9-9"></span>65. Meta. Llama 4 Model Card. <https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct> (2025). Hugging Face Model Card.
- <span id="page-9-10"></span>66. Meta. Llama 3 Model Card. [https://github.com/meta-llama/llama3/blob/main/MODEL\\_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) (2024). GitHub Repository.

## **Author contributions statement**

B.G. conducted the environmental impact transparency analysis, T.A.d.C carried out the media analysis. All authors wrote, edited and reviewed the manuscript.

# <span id="page-10-0"></span>**Appendix**

**Table 1.** Range of Pre-Training Environmental Impacts (Representative Models Displayed)

| Model            | Organization | Energy Consumption (MWh) | GHG Emissions (tCO2e) |
|------------------|--------------|--------------------------|-----------------------|
| OLMo 20M26       | Ai2          | 0.8                      | 0.3                   |
| CodeGen 350M62   | Salesforce   | 71                       | 6                     |
| Llama 7B63       | Meta         | 356                      | 14                    |
| BLOOM11          | Big Science  | 520                      | 30                    |
| T516             | Google       | 85.7                     | 47                    |
| OLMo 2 13B26     | Ai2          | 157                      | 101                   |
| Gemma 2B + 9B64  | Google       | ?                        | 131                   |
| GPT-316          | OpenAI       | 1,287                    | 552                   |
| Llama 4 Scout65  | Meta         | 3,500                    | 1,354                 |
| Llama 3 70B66    | Meta         | ?                        | 1,900                 |
| Llama 3.1 405B35 | Meta         | ?                        | 8,930                 |

Max/Min Variance: 4,375 29,767

**Table 2.** Range of Inference Energy Use[21](#page-7-17) (Representative Models Displayed)

<span id="page-11-1"></span><span id="page-11-0"></span>

| Model                       | Organization | GPU Energy for 1k Queries (Wh) | Task                 |
|-----------------------------|--------------|--------------------------------|----------------------|
| bert-tiny-finetuned-squadv2 | mrm8488      | 0.06                           | Extractive QA        |
| GIST-all-MiniLM-L6-v2       | avsolatorio  | 0.11                           | Sentence Similarity  |
| dynamic_tinybert            | Intel        | 0.21                           | Extractive QA        |
| distilbert-imdb             | lvwerra      | 0.22                           | Text Classification  |
| question_answering_v2       | Falconsai    | 0.23                           | Extractive QA        |
| Resnet 18                   | Microsoft    | 0.30                           | Image Classification |
| yolos-tiny                  | hustvl       | 1.00                           | Object Detection     |
| Vision Perceiver Conv       | Google       | 2.64                           | Image Classification |
| SFR-Embedding-Mistral       | Salesforce   | 5.22                           | Sentence Similarity  |
| yolos-base                  | hustvl       | 7.98                           | Object Detection     |
| Gemma 7B                    | Google       | 18.90                          | Text Generation      |
| T5 11b                      | Google       | 27.79                          | Text Classification  |
| phi-4                       | Microsoft    | 28.74                          | Text Generation      |
| T5 11b                      | Google       | 178.13                         | Extractive QA        |
| Mitsua Diffusion One        | Mitsua       | 186.81                         | Image Generation     |
| Mixtral 8x7B                | Mistral      | 615.39                         | Text Generation      |
| Stable Diffusion XL Base    | Stability AI | 1,639.85                       | Image Generation     |
| Llama 3 70B                 | Meta         | 1,719.66                       | Text Generation      |
| Qwen2.5 72B                 | Qwen         | 1,869.55                       | Text Generation      |
| Command-R Plus              | Cohere       | 3,426.38                       | Text Generation      |

Max/Min Variance: 57,106